{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "YzZdxqtuTg2o",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YzZdxqtuTg2o",
        "outputId": "0e32066c-1e01-4b5d-a503-53eaa0db9b06"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting yfscreen\n",
            "  Downloading yfscreen-0.1.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: requests>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from yfscreen) (2.32.4)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from yfscreen) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->yfscreen) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->yfscreen) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->yfscreen) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->yfscreen) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.2.0->yfscreen) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.2.0->yfscreen) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.2.0->yfscreen) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.2.0->yfscreen) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->yfscreen) (1.17.0)\n",
            "Downloading yfscreen-0.1.1-py3-none-any.whl (21 kB)\n",
            "Installing collected packages: yfscreen\n",
            "Successfully installed yfscreen-0.1.1\n",
            "Collecting ollama\n",
            "  Downloading ollama-0.5.3-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama) (0.28.1)\n",
            "Requirement already satisfied: pydantic>=2.9 in /usr/local/lib/python3.12/dist-packages (from ollama) (2.11.7)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.9->ollama) (0.4.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama) (1.3.1)\n",
            "Downloading ollama-0.5.3-py3-none-any.whl (13 kB)\n",
            "Installing collected packages: ollama\n",
            "Successfully installed ollama-0.5.3\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.12/dist-packages (0.2.65)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.0.2)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.32.4)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.0.12)\n",
            "Requirement already satisfied: platformdirs>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.3.8)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2025.2)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.12/dist-packages (from yfinance) (2.4.6)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.12/dist-packages (from yfinance) (3.18.2)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.12/dist-packages (from yfinance) (4.13.4)\n",
            "Requirement already satisfied: curl_cffi>=0.7 in /usr/local/lib/python3.12/dist-packages (from yfinance) (0.13.0)\n",
            "Requirement already satisfied: protobuf>=3.19.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (5.29.5)\n",
            "Requirement already satisfied: websockets>=13.0 in /usr/local/lib/python3.12/dist-packages (from yfinance) (15.0.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (4.14.1)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (1.17.1)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi>=0.7->yfinance) (2025.8.3)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.3.0->yfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.31->yfinance) (2.5.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi>=0.7->yfinance) (2.22)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas>=1.3.0->yfinance) (1.17.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (2.11.7)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (2.33.2)\n",
            "Requirement already satisfied: typing-extensions>=4.12.2 in /usr/local/lib/python3.12/dist-packages (from pydantic) (4.14.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic) (0.4.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting gnews\n",
            "  Downloading gnews-0.4.2-py3-none-any.whl.metadata (19 kB)\n",
            "Collecting feedparser~=6.0.2 (from gnews)\n",
            "  Downloading feedparser-6.0.11-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: beautifulsoup4<5,>=4.9.3 in /usr/local/lib/python3.12/dist-packages (from gnews) (4.13.4)\n",
            "Collecting dnspython (from gnews)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from gnews) (2.32.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9.3->gnews) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4<5,>=4.9.3->gnews) (4.14.1)\n",
            "Collecting sgmllib3k (from feedparser~=6.0.2->gnews)\n",
            "  Downloading sgmllib3k-1.0.0.tar.gz (5.8 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->gnews) (2025.8.3)\n",
            "Downloading gnews-0.4.2-py3-none-any.whl (18 kB)\n",
            "Downloading feedparser-6.0.11-py3-none-any.whl (81 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.3/81.3 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m313.6/313.6 kB\u001b[0m \u001b[31m15.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: sgmllib3k\n",
            "  Building wheel for sgmllib3k (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sgmllib3k: filename=sgmllib3k-1.0.0-py3-none-any.whl size=6046 sha256=b2290e91b01ccfa5774c7290cf4e408aaf5a72ec40da2a76560ba48fe7620fca\n",
            "  Stored in directory: /root/.cache/pip/wheels/03/f5/1a/23761066dac1d0e8e683e5fdb27e12de53209d05a4a37e6246\n",
            "Successfully built sgmllib3k\n",
            "Installing collected packages: sgmllib3k, feedparser, dnspython, gnews\n",
            "Successfully installed dnspython-2.7.0 feedparser-6.0.11 gnews-0.4.2 sgmllib3k-1.0.0\n",
            "Collecting finvizfinance\n",
            "  Downloading finvizfinance-1.1.1-py3-none-any.whl.metadata (5.0 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from finvizfinance) (2.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from finvizfinance) (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from finvizfinance) (4.13.4)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.12/dist-packages (from finvizfinance) (5.4.0)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->finvizfinance) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->finvizfinance) (4.14.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->finvizfinance) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->finvizfinance) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->finvizfinance) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->finvizfinance) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->finvizfinance) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->finvizfinance) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->finvizfinance) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->finvizfinance) (2025.8.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->finvizfinance) (1.17.0)\n",
            "Downloading finvizfinance-1.1.1-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.3/44.3 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: finvizfinance\n",
            "Successfully installed finvizfinance-1.1.1\n",
            "Collecting scikit-ollama\n",
            "  Downloading scikit_ollama-0.3.2-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting scikit-llm>=1.3.0 (from scikit-ollama)\n",
            "  Downloading scikit_llm-1.4.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: ollama>=0.4.7 in /usr/local/lib/python3.12/dist-packages (from scikit-ollama) (0.5.3)\n",
            "Requirement already satisfied: openai>=1.59.9 in /usr/local/lib/python3.12/dist-packages (from scikit-ollama) (1.100.0)\n",
            "Requirement already satisfied: pydantic in /usr/local/lib/python3.12/dist-packages (from scikit-ollama) (2.11.7)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama>=0.4.7->scikit-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (4.10.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (4.67.1)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.11 in /usr/local/lib/python3.12/dist-packages (from openai>=1.59.9->scikit-ollama) (4.14.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->scikit-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic->scikit-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic->scikit-ollama) (0.4.1)\n",
            "Requirement already satisfied: scikit-learn<2.0.0,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-llm>=1.3.0->scikit-ollama) (1.6.1)\n",
            "Requirement already satisfied: pandas<3.0.0,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from scikit-llm>=1.3.0->scikit-ollama) (2.2.2)\n",
            "Requirement already satisfied: google-cloud-aiplatform<2.0.0,>=1.27.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.109.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.12/dist-packages (from anyio<5,>=3.5.0->openai>=1.59.9->scikit-ollama) (3.10)\n",
            "Requirement already satisfied: google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.25.1)\n",
            "Requirement already satisfied: google-auth<3.0.0,>=2.14.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.38.0)\n",
            "Requirement already satisfied: proto-plus<2.0.0,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.26.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<7.0.0,>=3.20.2 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (5.29.5)\n",
            "Requirement already satisfied: packaging>=14.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (25.0)\n",
            "Requirement already satisfied: google-cloud-storage<3.0.0,>=1.32.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.19.0)\n",
            "Requirement already satisfied: google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (3.35.1)\n",
            "Requirement already satisfied: google-cloud-resource-manager<3.0.0,>=1.3.3 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.14.2)\n",
            "Requirement already satisfied: shapely<3.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.1.1)\n",
            "Requirement already satisfied: google-genai<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.30.0)\n",
            "Requirement already satisfied: docstring_parser<1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (0.17.0)\n",
            "Requirement already satisfied: pyyaml<7,>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (6.0.2)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama>=0.4.7->scikit-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama>=0.4.7->scikit-ollama) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama>=0.4.7->scikit-ollama) (0.16.0)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm>=1.3.0->scikit-ollama) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm>=1.3.0->scikit-ollama) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm>=1.3.0->scikit-ollama) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3.0.0,>=1.5.0->scikit-llm>=1.3.0->scikit-ollama) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm>=1.3.0->scikit-ollama) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm>=1.3.0->scikit-ollama) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn<2.0.0,>=1.1.0->scikit-llm>=1.3.0->scikit-ollama) (3.6.0)\n",
            "Requirement already satisfied: googleapis-common-protos<2.0.0,>=1.56.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.70.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.18.0 in /usr/local/lib/python3.12/dist-packages (from google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.32.4)\n",
            "Requirement already satisfied: grpcio<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.74.0)\n",
            "Requirement already satisfied: grpcio-status<2.0.0,>=1.33.2 in /usr/local/lib/python3.12/dist-packages (from google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.71.2)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (5.5.2)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (0.4.2)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.12/dist-packages (from google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (4.9.1)\n",
            "Requirement already satisfied: google-cloud-core<3.0.0,>=2.4.1 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.4.3)\n",
            "Requirement already satisfied: google-resumable-media<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-bigquery!=3.20.0,<4.0.0,>=1.15.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.7.2)\n",
            "Requirement already satisfied: grpc-google-iam-v1<1.0.0,>=0.14.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-resource-manager<3.0.0,>=1.3.3->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (0.14.2)\n",
            "Requirement already satisfied: google-crc32c<2.0dev,>=1.0 in /usr/local/lib/python3.12/dist-packages (from google-cloud-storage<3.0.0,>=1.32.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (1.7.1)\n",
            "Requirement already satisfied: tenacity<9.2.0,>=8.2.3 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (8.5.0)\n",
            "Requirement already satisfied: websockets<15.1.0,>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from google-genai<2.0.0,>=1.0.0->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (15.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3.0.0,>=1.5.0->scikit-llm>=1.3.0->scikit-ollama) (1.17.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.6.1 in /usr/local/lib/python3.12/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3.0.0,>=2.14.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (0.6.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.18.0->google-api-core!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-api-core[grpc]!=2.0.*,!=2.1.*,!=2.2.*,!=2.3.*,!=2.4.*,!=2.5.*,!=2.6.*,!=2.7.*,<3.0.0,>=1.34.1->google-cloud-aiplatform<2.0.0,>=1.27.0->google-cloud-aiplatform[pipelines]<2.0.0,>=1.27.0->scikit-llm>=1.3.0->scikit-ollama) (2.5.0)\n",
            "Downloading scikit_ollama-0.3.2-py3-none-any.whl (14 kB)\n",
            "Downloading scikit_llm-1.4.1-py3-none-any.whl (51 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.4/51.4 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: scikit-llm, scikit-ollama\n",
            "Successfully installed scikit-llm-1.4.1 scikit-ollama-0.3.2\n",
            "Requirement already satisfied: statsmodels in /usr/local/lib/python3.12/dist-packages (0.14.5)\n",
            "Requirement already satisfied: numpy<3,>=1.22.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.0.2)\n",
            "Requirement already satisfied: scipy!=1.9.2,>=1.8 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.16.1)\n",
            "Requirement already satisfied: pandas!=2.1.0,>=1.4 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (2.2.2)\n",
            "Requirement already satisfied: patsy>=0.5.6 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (1.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.12/dist-packages (from statsmodels) (25.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas!=2.1.0,>=1.4->statsmodels) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas!=2.1.0,>=1.4->statsmodels) (1.17.0)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Collecting langchain-ollama\n",
            "  Downloading langchain_ollama-0.3.6-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: ollama<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.5.3)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.70 in /usr/local/lib/python3.12/dist-packages (from langchain-ollama) (0.3.74)\n",
            "Requirement already satisfied: langsmith>=0.3.45 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.14)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.33)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (6.0.2)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (4.14.1)\n",
            "Requirement already satisfied: packaging>=23.2 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (25.0)\n",
            "Requirement already satisfied: pydantic>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.11.7)\n",
            "Requirement already satisfied: httpx>=0.27 in /usr/local/lib/python3.12/dist-packages (from ollama<1.0.0,>=0.5.1->langchain-ollama) (0.28.1)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (4.10.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (2025.8.3)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.12/dist-packages (from httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.0.0)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.11.2)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.32.4)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.24.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic>=2.7.4->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (0.4.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (3.4.3)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.0.0->langsmith>=0.3.45->langchain-core<1.0.0,>=0.3.70->langchain-ollama) (2.5.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx>=0.27->ollama<1.0.0,>=0.5.1->langchain-ollama) (1.3.1)\n",
            "Downloading langchain_ollama-0.3.6-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: langchain-ollama\n",
            "Successfully installed langchain-ollama-0.3.6\n",
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m127.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.48.1\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (5.24.1)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly) (8.5.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from plotly) (25.0)\n",
            "Collecting scikit-metrics\n",
            "  Downloading scikit-metrics-0.1.0.tar.gz (3.9 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from scikit-metrics) (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->scikit-metrics) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->scikit-metrics) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->scikit-metrics) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->scikit-metrics) (3.6.0)\n",
            "Building wheels for collected packages: scikit-metrics\n",
            "  Building wheel for scikit-metrics (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-metrics: filename=scikit_metrics-0.1.0-py3-none-any.whl size=4393 sha256=330b39969d05014fc35607488b302270fa70eaacb1eb11b897b917c62c77c335\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/b6/9d/6a185afb069eb36ef50dcadf4fdd81731a16fc7f9cf1f1fd68\n",
            "Successfully built scikit-metrics\n",
            "Installing collected packages: scikit-metrics\n",
            "Successfully installed scikit-metrics-0.1.0\n",
            "Collecting pandas-ta\n",
            "  Downloading pandas_ta-0.3.14b.tar.gz (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from pandas-ta) (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas-ta) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas-ta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas-ta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->pandas-ta) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas->pandas-ta) (1.17.0)\n",
            "Building wheels for collected packages: pandas-ta\n",
            "  Building wheel for pandas-ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pandas-ta: filename=pandas_ta-0.3.14b0-py3-none-any.whl size=218910 sha256=100518383ee2408f8ef2e26255e8c95dc0346dd61133c229e51354ec04e3661c\n",
            "  Stored in directory: /root/.cache/pip/wheels/fd/ed/18/2a12fd1b7906c63efca6accb351929f2c7f6bbc674e1c0ba5d\n",
            "Successfully built pandas-ta\n",
            "Installing collected packages: pandas-ta\n",
            "Successfully installed pandas-ta-0.3.14b0\n",
            "Collecting schedule\n",
            "  Downloading schedule-1.2.2-py3-none-any.whl.metadata (3.8 kB)\n",
            "Downloading schedule-1.2.2-py3-none-any.whl (12 kB)\n",
            "Installing collected packages: schedule\n",
            "Successfully installed schedule-1.2.2\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Collecting fake_useragent\n",
            "  Downloading fake_useragent-2.2.0-py3-none-any.whl.metadata (17 kB)\n",
            "Downloading fake_useragent-2.2.0-py3-none-any.whl (161 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m161.7/161.7 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: fake_useragent\n",
            "Successfully installed fake_useragent-2.2.0\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.12/dist-packages (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk) (4.67.1)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Collecting bs4\n",
            "  Downloading bs4-0.0.2-py2.py3-none-any.whl.metadata (411 bytes)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from bs4) (4.13.4)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->bs4) (4.14.1)\n",
            "Downloading bs4-0.0.2-py2.py3-none-any.whl (1.2 kB)\n",
            "Installing collected packages: bs4\n",
            "Successfully installed bs4-0.0.2\n",
            "Requirement already satisfied: fake_useragent in /usr/local/lib/python3.12/dist-packages (2.2.0)\n",
            "Requirement already satisfied: feedparser in /usr/local/lib/python3.12/dist-packages (6.0.11)\n",
            "Requirement already satisfied: sgmllib3k in /usr/local/lib/python3.12/dist-packages (from feedparser) (1.0.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (2.32.4)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (4.13.4)\n",
            "Requirement already satisfied: fake-useragent in /usr/local/lib/python3.12/dist-packages (2.2.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.12/dist-packages (0.9.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (2.7)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4) (4.14.1)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.12/dist-packages (0.13.2)\n",
            "Requirement already satisfied: numpy!=1.24.0,>=1.20 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.2 in /usr/local/lib/python3.12/dist-packages (from seaborn) (2.2.2)\n",
            "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in /usr/local/lib/python3.12/dist-packages (from seaborn) (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.2->seaborn) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
            "Requirement already satisfied: xgboost in /usr/local/lib/python3.12/dist-packages (3.0.4)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.0.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /usr/local/lib/python3.12/dist-packages (from xgboost) (2.27.3)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from xgboost) (1.16.1)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (4.6.0)\n",
            "Requirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from lightgbm) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from lightgbm) (1.16.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (1.5.1)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: textblob in /usr/local/lib/python3.12/dist-packages (0.19.0)\n",
            "Requirement already satisfied: nltk>=3.9 in /usr/local/lib/python3.12/dist-packages (from textblob) (3.9.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (8.2.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (1.5.1)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from nltk>=3.9->textblob) (4.67.1)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install yfscreen\n",
        "!pip install ollama\n",
        "!pip install yfinance\n",
        "!pip install pydantic\n",
        "!pip install pandas\n",
        "!pip install gnews\n",
        "!pip install finvizfinance\n",
        "!pip install scikit-ollama\n",
        "!pip install statsmodels\n",
        "!pip install plotly\n",
        "!pip install langchain-ollama\n",
        "!pip install streamlit\n",
        "!pip install numpy\n",
        "!pip install plotly\n",
        "!pip install scikit-metrics\n",
        "!pip install pandas-ta\n",
        "!pip install schedule\n",
        "!pip install matplotlib\n",
        "!pip install fake_useragent\n",
        "!pip install nltk\n",
        "!pip install feedparser\n",
        "!pip install tabulate\n",
        "!pip install bs4\n",
        "!pip install fake_useragent\n",
        "!pip install feedparser\n",
        "!pip install requests beautifulsoup4 fake-useragent tabulate\n",
        "!pip install seaborn\n",
        "!pip install xgboost\n",
        "!pip install lightgbm\n",
        "!pip install joblib\n",
        "!pip install scikit-learn\n",
        "!pip install textblob"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#analyser2.py three prog combo"
      ],
      "metadata": {
        "id": "jWeu6qMajR0F"
      },
      "id": "jWeu6qMajR0F"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import feedparser\n",
        "from tabulate import tabulate\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "# Add these imports at the top of your file\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
        "from sklearn.impute import SimpleImputer\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "import joblib\n",
        "\n",
        "# Add this class to your code\n",
        "class StockPredictionModel:\n",
        "    def __init__(self):\n",
        "        self.models = {\n",
        "            'random_forest': RandomForestClassifier(n_estimators=100, random_state=42),\n",
        "            'xgboost': xgb.XGBClassifier(n_estimators=100, random_state=42),\n",
        "            'lightgbm': lgb.LGBMClassifier(n_estimators=100, random_state=42),\n",
        "            'logistic_regression': LogisticRegression(random_state=42, max_iter=1000)\n",
        "        }\n",
        "        self.scaler = StandardScaler()\n",
        "        self.imputer = SimpleImputer(strategy='median')\n",
        "        self.selected_features = None\n",
        "        self.best_model = None\n",
        "\n",
        "    def prepare_training_data(self, fundamental_data, sentiment_data, price_data):\n",
        "        \"\"\"\n",
        "        Prepare training data from multiple sources\n",
        "        \"\"\"\n",
        "        # Merge fundamental and sentiment data\n",
        "        merged_data = pd.merge(fundamental_data, sentiment_data, on='ticker', how='inner')\n",
        "\n",
        "        # Add price movement target (1 if price increases next day, 0 otherwise)\n",
        "        merged_data['target'] = self._create_price_target(price_data)\n",
        "\n",
        "        # Select features for training\n",
        "        feature_columns = [\n",
        "            'operating_margin', 'debt_to_equity', 'price_to_book',\n",
        "            'pe_ratio', 'peg_ratio', 'beta', 'volume', 'composite_sentiment',\n",
        "            'price_sentiment', 'volume_sentiment', 'news_sentiment',\n",
        "            'social_sentiment', 'options_sentiment', 'rsi', 'insider_sentiment'\n",
        "        ]\n",
        "\n",
        "        # Filter to available columns\n",
        "        available_features = [col for col in feature_columns if col in merged_data.columns]\n",
        "        X = merged_data[available_features]\n",
        "        y = merged_data['target']\n",
        "\n",
        "        # Handle missing values\n",
        "        X_imputed = self.imputer.fit_transform(X)\n",
        "\n",
        "        # Scale features\n",
        "        X_scaled = self.scaler.fit_transform(X_imputed)\n",
        "\n",
        "        self.selected_features = available_features\n",
        "\n",
        "        return X_scaled, y, merged_data['ticker']\n",
        "\n",
        "    def _create_price_target(self, price_data, lookahead_days=1):\n",
        "        \"\"\"\n",
        "        Create target variable: 1 if price increases in next lookahead_days, 0 otherwise\n",
        "        \"\"\"\n",
        "        targets = []\n",
        "        for ticker in price_data['ticker'].unique():\n",
        "            ticker_data = price_data[price_data['ticker'] == ticker].sort_values('date')\n",
        "            if len(ticker_data) > lookahead_days:\n",
        "                current_price = ticker_data['close'].iloc[-lookahead_days-1]\n",
        "                future_price = ticker_data['close'].iloc[-1]\n",
        "                target = 1 if future_price > current_price else 0\n",
        "                targets.append((ticker, target))\n",
        "\n",
        "        return pd.DataFrame(targets, columns=['ticker', 'target']).set_index('ticker')['target']\n",
        "\n",
        "    def train_models(self, X, y, test_size=0.2):\n",
        "        \"\"\"\n",
        "        Train multiple models and select the best one\n",
        "        \"\"\"\n",
        "        # Use time series split for financial data\n",
        "        tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "        best_score = 0\n",
        "        best_model_name = None\n",
        "\n",
        "        for name, model in self.models.items():\n",
        "            print(f\"Training {name}...\")\n",
        "\n",
        "            # Cross-validation\n",
        "            scores = []\n",
        "            for train_index, test_index in tscv.split(X):\n",
        "                X_train, X_test = X[train_index], X[test_index]\n",
        "                y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
        "\n",
        "                model.fit(X_train, y_train)\n",
        "                y_pred = model.predict(X_test)\n",
        "                score = f1_score(y_test, y_pred)\n",
        "                scores.append(score)\n",
        "\n",
        "            avg_score = np.mean(scores)\n",
        "            print(f\"{name} average F1 score: {avg_score:.3f}\")\n",
        "\n",
        "            if avg_score > best_score:\n",
        "                best_score = avg_score\n",
        "                best_model_name = name\n",
        "                self.best_model = model\n",
        "\n",
        "        print(f\"Best model: {best_model_name} with F1 score: {best_score:.3f}\")\n",
        "\n",
        "        # Final training on all data\n",
        "        self.best_model.fit(X, y)\n",
        "\n",
        "        return self.best_model\n",
        "\n",
        "    def predict(self, X_new):\n",
        "        \"\"\"\n",
        "        Make predictions on new data\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model must be trained before making predictions\")\n",
        "\n",
        "        # Preprocess new data\n",
        "        X_imputed = self.imputer.transform(X_new)\n",
        "        X_scaled = self.scaler.transform(X_imputed)\n",
        "\n",
        "        predictions = self.best_model.predict(X_scaled)\n",
        "        probabilities = self.best_model.predict_proba(X_scaled)\n",
        "\n",
        "        return predictions, probabilities\n",
        "\n",
        "    def feature_importance(self):\n",
        "        \"\"\"\n",
        "        Get feature importance from the best model\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"Model must be trained first\")\n",
        "\n",
        "        if hasattr(self.best_model, 'feature_importances_'):\n",
        "            importance = self.best_model.feature_importances_\n",
        "        elif hasattr(self.best_model, 'coef_'):\n",
        "            importance = np.abs(self.best_model.coef_[0])\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "        # Create feature importance dataframe\n",
        "        feature_importance_df = pd.DataFrame({\n",
        "            'feature': self.selected_features,\n",
        "            'importance': importance\n",
        "        }).sort_values('importance', ascending=False)\n",
        "\n",
        "        return feature_importance_df\n",
        "\n",
        "    def save_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Save the trained model\n",
        "        \"\"\"\n",
        "        if self.best_model is None:\n",
        "            raise ValueError(\"No model trained to save\")\n",
        "\n",
        "        joblib.dump({\n",
        "            'model': self.best_model,\n",
        "            'scaler': self.scaler,\n",
        "            'imputer': self.imputer,\n",
        "            'features': self.selected_features\n",
        "        }, filepath)\n",
        "\n",
        "    def load_model(self, filepath):\n",
        "        \"\"\"\n",
        "        Load a trained model\n",
        "        \"\"\"\n",
        "        saved_data = joblib.load(filepath)\n",
        "        self.best_model = saved_data['model']\n",
        "        self.scaler = saved_data['scaler']\n",
        "        self.imputer = saved_data['imputer']\n",
        "        self.selected_features = saved_data['features']\n",
        "\n",
        "        return self\n",
        "\n",
        "# Add this function to collect historical data for training\n",
        "def collect_training_data(days=365):\n",
        "    \"\"\"\n",
        "    Collect historical data for model training\n",
        "    \"\"\"\n",
        "    print(\"Collecting historical data for training...\")\n",
        "\n",
        "    # Get all tickers\n",
        "    all_tickers = list(LSE_TICKERS.keys()) + list(PENNY_STOCKS.keys())\n",
        "\n",
        "    fundamental_data = []\n",
        "    sentiment_data = []\n",
        "    price_data = []\n",
        "\n",
        "    # Initialize sentiment analyzer\n",
        "    analyzer = ComprehensiveMarketSentimentAnalyzer()\n",
        "\n",
        "    for ticker in tqdm(all_tickers, desc=\"Collecting historical data\"):\n",
        "        try:\n",
        "            # Get historical fundamental data (simplified)\n",
        "            stock = yf.Ticker(ticker)\n",
        "            info = stock.info\n",
        "\n",
        "            fundamental_data.append({\n",
        "                'ticker': ticker,\n",
        "                'operating_margin': info.get('operatingMargins', np.nan),\n",
        "                'debt_to_equity': info.get('debtToEquity', np.nan),\n",
        "                'price_to_book': info.get('priceToBook', np.nan),\n",
        "                'pe_ratio': info.get('trailingPE', np.nan),\n",
        "                'peg_ratio': info.get('pegRatio', np.nan),\n",
        "                'beta': info.get('beta', np.nan),\n",
        "                'volume': info.get('averageVolume', np.nan)\n",
        "            })\n",
        "\n",
        "            # Get historical price data\n",
        "            hist = stock.history(period=f\"{days}d\")\n",
        "            for date, row in hist.iterrows():\n",
        "                price_data.append({\n",
        "                    'ticker': ticker,\n",
        "                    'date': date,\n",
        "                    'open': row['Open'],\n",
        "                    'high': row['High'],\n",
        "                    'low': row['Low'],\n",
        "                    'close': row['Close'],\n",
        "                    'volume': row['Volume']\n",
        "                })\n",
        "\n",
        "            # Simulate historical sentiment (in a real scenario, you'd need historical sentiment data)\n",
        "            sentiment_result = analyzer.calculate_composite_sentiment(ticker)\n",
        "            if sentiment_result:\n",
        "                sentiment_data.append({\n",
        "                    'ticker': ticker,\n",
        "                    'composite_sentiment': sentiment_result['composite_sentiment'],\n",
        "                    'price_sentiment': sentiment_result['components']['price'],\n",
        "                    'volume_sentiment': sentiment_result['components']['volume'],\n",
        "                    'news_sentiment': sentiment_result['components']['news'],\n",
        "                    'social_sentiment': sentiment_result['components']['social'],\n",
        "                    'options_sentiment': sentiment_result['components']['options'],\n",
        "                    'rsi': sentiment_result['rsi']\n",
        "                })\n",
        "\n",
        "            time.sleep(0.1)  # Rate limiting\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error collecting data for {ticker}: {e}\")\n",
        "            continue\n",
        "\n",
        "    return (pd.DataFrame(fundamental_data),\n",
        "            pd.DataFrame(sentiment_data),\n",
        "            pd.DataFrame(price_data))\n",
        "\n",
        "# Add this function to integrate ML predictions into your analysis\n",
        "def add_ml_predictions_to_analysis(top_stocks, model):\n",
        "    \"\"\"\n",
        "    Add machine learning predictions to stock analysis\n",
        "    \"\"\"\n",
        "    prediction_data = []\n",
        "\n",
        "    for stock in top_stocks:\n",
        "        # Prepare features for prediction\n",
        "        features = {\n",
        "            'operating_margin': stock.get('operating_margin', np.nan),\n",
        "            'debt_to_equity': stock.get('debt_to_equity', np.nan),\n",
        "            'price_to_book': stock.get('price_to_book', np.nan),\n",
        "            'pe_ratio': stock.get('pe_ratio', np.nan),\n",
        "            'peg_ratio': stock.get('peg_ratio', np.nan),\n",
        "            'beta': stock.get('beta', np.nan),\n",
        "            'volume': stock.get('volume', np.nan),\n",
        "            'composite_sentiment': stock.get('composite_sentiment', np.nan),\n",
        "            'price_sentiment': stock.get('components', {}).get('price', np.nan) if isinstance(stock.get('components'), dict) else np.nan,\n",
        "            'volume_sentiment': stock.get('components', {}).get('volume', np.nan) if isinstance(stock.get('components'), dict) else np.nan,\n",
        "            'news_sentiment': stock.get('components', {}).get('news', np.nan) if isinstance(stock.get('components'), dict) else np.nan,\n",
        "            'social_sentiment': stock.get('components', {}).get('social', np.nan) if isinstance(stock.get('components'), dict) else np.nan,\n",
        "            'options_sentiment': stock.get('components', {}).get('options', np.nan) if isinstance(stock.get('components'), dict) else np.nan,\n",
        "            'rsi': stock.get('rsi', np.nan),\n",
        "            'insider_sentiment': stock.get('insider_sentiment', 0)\n",
        "        }\n",
        "\n",
        "        prediction_data.append(features)\n",
        "\n",
        "    # Convert to DataFrame\n",
        "    prediction_df = pd.DataFrame(prediction_data)\n",
        "\n",
        "    # Make predictions\n",
        "    try:\n",
        "        predictions, probabilities = model.predict(prediction_df)\n",
        "\n",
        "        # Add predictions to stocks\n",
        "        for i, stock in enumerate(top_stocks):\n",
        "            stock['ml_prediction'] = 'UP' if predictions[i] == 1 else 'DOWN'\n",
        "            stock['ml_confidence'] = max(probabilities[i])\n",
        "            stock['ml_prob_up'] = probabilities[i][1] if len(probabilities[i]) > 1 else 0.5\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error making predictions: {e}\")\n",
        "        for stock in top_stocks:\n",
        "            stock['ml_prediction'] = 'N/A'\n",
        "            stock['ml_confidence'] = 0\n",
        "            stock['ml_prob_up'] = 0.5\n",
        "\n",
        "    return top_stocks\n",
        "\n",
        "# Modify your display_recommendations method to include ML predictions\n",
        "def display_recommendations_with_ml(self, top_stocks, all_results):\n",
        "    \"\"\"Display top stock recommendations with ML predictions\"\"\"\n",
        "    print(f\"\\n🎯 TOP {len(top_stocks)} STOCK RECOMMENDATIONS (WITH ML PREDICTIONS)\")\n",
        "    print(\"=\" * 140)\n",
        "    print(f\"{'Rank':<5} {'Symbol':<8} {'Market':<8} {'Price':<10} {'Change%':<8} {'Signal':<12} {'ML Pred':<8} {'ML Conf':<8} {'Sentiment':<10} {'RSI':<6}\")\n",
        "    print(\"-\" * 140)\n",
        "\n",
        "    for i, stock in enumerate(top_stocks, 1):\n",
        "        change_color = '\\033[92m' if stock['price_change'] >= 0 else '\\033[91m'\n",
        "        signal_color = '\\033[92m' if 'BUY' in stock['signal'] else '\\033[91m' if 'SELL' in stock['signal'] else '\\033[93m'\n",
        "        ml_color = '\\033[92m' if stock.get('ml_prediction') == 'UP' else '\\033[91m' if stock.get('ml_prediction') == 'DOWN' else '\\033[93m'\n",
        "        rsi_color = '\\033[91m' if stock['rsi'] > 70 else '\\033[92m' if stock['rsi'] < 30 else '\\033[93m'\n",
        "\n",
        "        print(f\"{i:<5} {stock['symbol']:<8} {stock['market']:<8} \"\n",
        "              f\"${stock['current_price']:<9.2f} \"\n",
        "              f\"{change_color}{stock['price_change']:>+6.1f}%\\033[0m \"\n",
        "              f\"{signal_color}{stock['signal']:<12}\\033[0m \"\n",
        "              f\"{ml_color}{stock.get('ml_prediction', 'N/A'):<8}\\033[0m \"\n",
        "              f\"{stock.get('ml_confidence', 0):<7.2f} \"\n",
        "              f\"{stock['composite_sentiment']:>+8.3f} \"\n",
        "              f\"{rsi_color}{stock['rsi']:>5.1f}\\033[0m\")\n",
        "\n",
        "    print(\"-\" * 140)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################################################################\n",
        "# Initialize sentiment analyzer\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize user agent generator\n",
        "ua = UserAgent()\n",
        "\n",
        "# Combined stock databases\n",
        "LSE_TICKERS = {\n",
        "    'FNXF.L': 'Fonix Mobile PLC',\n",
        "    'GGP.L': 'Greatland Resources Ltd',\n",
        "    'FSGF.L': 'Foresight Group Holdings Ltd',\n",
        "    'ACSO.L': 'Accesso Technology Group PLC',\n",
        "    'POLR.L': 'Polar Capital Holdings plc',\n",
        "    'BRMS.L': 'Braemar PLC',\n",
        "    'GFM.L': 'Griffin Mining',\n",
        "    'FRP.L': 'Frp Advisory Group Plc',\n",
        "    'EZJ.L': 'EasyJet PLC',\n",
        "    'W7L.L': 'Warpaint London PLC',\n",
        "    'BEG.L': 'Begbies Traynor Group PLC',\n",
        "    'SRAD.L': 'Stelrad Group PLC',\n",
        "    'LSL.L': 'LSL Property Services Plc',\n",
        "    'MEGPM.L': 'ME Group International PLC',\n",
        "    'VTYV.L': 'Vistry Group PLC',\n",
        "    'MUT.L': 'Murray Income Trust',\n",
        "    'BKSB.L': 'Beeks Financial Cloud Group PLC',\n",
        "    'FDEV.L': 'Frontier Developments plc',\n",
        "    'ASOS.L': 'ASOS PLC',\n",
        "    'LGEN.L': 'Legal & General Group PLC',\n",
        "    'MNG.L': 'M&G Plc',\n",
        "    'BP.L': 'BP PLC',\n",
        "    'PHNX.L': 'Phoenix Group Holdings PLC',\n",
        "    'VOD.L': 'Vodafone Group PLC',\n",
        "    'CNA.L': 'Centrica PLC',\n",
        "    'GLEN.L': 'Glencore PLC'\n",
        "}\n",
        "\n",
        "PENNY_STOCKS = {\n",
        "    'NTVO.L': 'Nativo Resources PLC',\n",
        "    'SOU.L': 'Sound Energy PLC',\n",
        "    'GENL.L': 'Genel Energy Plc',\n",
        "    'IQE.L': 'IQE PLC',\n",
        "    'LDG.L': 'Logistics Development Group PLC',\n",
        "    'VAST.L': 'Vast Resources PLC',\n",
        "    'SCEU.L': 'Surface Transforms Plc',\n",
        "    'PDL.L': 'Petra Diamonds Ltd',\n",
        "    'ITM.L': 'ITM Power',\n",
        "    'TLW.L': 'Tullow Oil PLC',\n",
        "    'ARB.L': 'Argo Blockchain PLC',\n",
        "    'AVCT.L': 'Avacta Group PLC',\n",
        "    'TBLD.L': 'Tinybuild Inc',\n",
        "    'EOG.L': 'Europa Oil & Gas Holdings',\n",
        "    'PHEG.L': 'PowerHouse Energy Group plc',\n",
        "    'SPRSP.L': 'Springfield Properties PLC',\n",
        "    'AML.L': 'Aston Martin Lagonda Global Holdings PLC',\n",
        "    'VANL.L': 'Van Elle Holdings PLC',\n",
        "    'GMS.L': 'Gulf Marine Services PLC',\n",
        "    'RNO.L': 'Renold',\n",
        "    'SOSS.L': 'Sosandar PLC',\n",
        "    'BELLB.L': 'Belluscura PLC',\n",
        "    'THW.ASE': 'Daniel Thwaites PLC',\n",
        "    'KOD.L': 'Kodal Minerals',\n",
        "    'CSSG.L': 'Croma Security Solutions Group Plc',\n",
        "    'VOD.L': 'Vodafone Group PLC',\n",
        "    'SDIS.L': 'SDI Group PLC',\n",
        "    'RWS.L': 'RWS Holdings PLC',\n",
        "    'JD.L': 'JD Sports Fashion PLC',\n",
        "    'STAF.L': 'Staffline Group Plc',\n",
        "    'BEG.L': 'Begbies Traynor Group PLC',\n",
        "    'ENQ.L': 'Enquest Plc',\n",
        "    'BIGB.L': 'Big Technologies PLC',\n",
        "    'DEBS.L': 'Boohoo Group PLC',\n",
        "    'HAYS.L': 'Hays Plc',\n",
        "    'ULTP.L': 'Ultimate Products PLC',\n",
        "    'RMR.L': 'Rome Resources PLC',\n",
        "    'RKH.L': 'Rockhopper Exploration',\n",
        "    'CAV.L': 'Cavendish Financial PLC',\n",
        "    'BHL.L': 'Bradda Head Holdings Ltd',\n",
        "    'EUA.L': 'Eurasia Mining',\n",
        "    'PATP.L': 'Panthera Resources PLC',\n",
        "    'JOG.L': 'Jersey Oil and Gas PLC',\n",
        "    'MSMN.L': 'Mosman Oil and Gas Ltd',\n",
        "    'TERNT.L': 'Tern PLC',\n",
        "    'ALLA.L': 'Atlantic Lithium Ltd',\n",
        "    'EMEL.L': 'Empyrean Energy'\n",
        "}\n",
        "\n",
        "# Combined financial metrics thresholds\n",
        "METRIC_THRESHOLDS = {\n",
        "    'regular': {\n",
        "        'price': {'max': float('inf')},\n",
        "        'volume': {'min': 0},\n",
        "        'operating_margin': {'ideal': 0.1, 'acceptable': 0.05},\n",
        "        'debt_to_equity': {'ideal': 0.5, 'acceptable': 1.0},\n",
        "        'price_to_book': {'ideal': 2.0, 'acceptable': 3.0},\n",
        "        'pe_ratio': {'ideal': 15.0, 'acceptable': 20.0},\n",
        "        'peg_ratio': {'ideal': 1.5, 'acceptable': 2.0},\n",
        "    },\n",
        "    'penny': {\n",
        "        'price': {'max': 5.00},\n",
        "        'volume': {'min': 500000},\n",
        "        'operating_margin': {'ideal': 0.05, 'acceptable': 0.01},\n",
        "        'debt_to_equity': {'ideal': 1.0, 'acceptable': 2.0},\n",
        "        'price_to_book': {'ideal': 1.5, 'acceptable': 3.0},\n",
        "        'pe_ratio': {'ideal': 15.0, 'acceptable': 30.0},\n",
        "        'peg_ratio': {'ideal': 1.5, 'acceptable': 2.5},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Combined news sources\n",
        "NEWS_SOURCES = [\n",
        "    {\"name\": \"Vox Markets\", \"url\": \"https://www.voxmarkets.co.uk/latest-news/\", \"parser\": \"vox\"},\n",
        "    {\"name\": \"Yahoo Finance UK\", \"url\": \"https://uk.finance.yahoo.com/topic/latest-news\", \"parser\": \"yahoo\"},\n",
        "    {\"name\": \"MarketWatch UK\", \"url\": \"https://www.marketwatch.com/latest-news\", \"parser\": \"marketwatch\"},\n",
        "    {\"name\": \"Financial Times\", \"url\": \"https://www.ft.com/markets\", \"parser\": \"ft\"},\n",
        "    {\"name\": \"Reuters UK\", \"url\": \"https://www.reuters.com/business/finance\", \"parser\": \"reuters\"},\n",
        "    {\"name\": \"London Stock Exchange\", \"url\": \"https://www.londonstockexchange.com/news\", \"parser\": \"lse\"},\n",
        "    {\"name\": \"Google News\", \"url\": \"https://news.google.com/rss/search?q={query}+stock\", \"parser\": \"google\"},\n",
        "    {\"name\": 'PennyStocks.com', \"url\": 'https://pennystocks.com/news/', \"parser\": 'pennystocks'},\n",
        "    {\"name\": 'InvestorPlace Penny Stocks', \"url\": 'https://investorplace.com/category/penny-stocks/', \"parser\": 'investorplace'}\n",
        "]\n",
        "\n",
        "def get_full_lse_tickers():\n",
        "    \"\"\"Get comprehensive list of LSE tickers including AIM\"\"\"\n",
        "    # This combines FTSE 350 + popular AIM stocks\n",
        "    return [\n",
        "        'III', 'AAL', 'ABDN', 'ADM', 'AHT', 'ANTO', 'AUTO', 'AV', 'AVV', 'AZN',\n",
        "        'BA', 'BARC', 'BDEV', 'BKG', 'BLND', 'BNZL', 'BP', 'BRBY', 'BT-A', 'CCH',\n",
        "        'CCL', 'CNA', 'CPG', 'CRDA', 'CRH', 'CTEC', 'DCC', 'DGE', 'DLG', 'EXPN',\n",
        "        'FERG', 'FLTR', 'FRES', 'GLEN', 'GSK', 'HLMA', 'HSBA', 'IAG', 'IHG', 'IMB',\n",
        "        'INF', 'ITRK', 'JD', 'JET', 'KGF', 'LAND', 'LGEN', 'LLOY', 'LSE', 'MKS',\n",
        "        'MNDI', 'MRO', 'NG', 'NXT', 'OCDO', 'PRU', 'PSN', 'PSON', 'REL', 'RIO',\n",
        "        'RMV', 'RR', 'RTO', 'SBRY', 'SDR', 'SGE', 'SGRO', 'SHP', 'SKG', 'SMDS',\n",
        "        'SMIN', 'SN', 'SPX', 'SSE', 'STAN', 'STJ', 'SVT', 'TSCO', 'ULVR', 'UU',\n",
        "        'VOD', 'WEIR', 'WPP', 'WTB', 'BME', 'CWR', 'FDM', 'GAW', 'HAS', 'LRE',\n",
        "        'MCG', 'PAGE', 'RM', 'SMT', 'TEP', 'TRST', 'VCT', 'BOO', 'CINE', 'DARK',\n",
        "        'FOUR', 'GNS', 'HWDN', 'IQE', 'LXI', 'OXIG', 'PFC', 'QLT', 'RR.', 'SVS',\n",
        "        'TPT', 'VANL', 'WJG', 'APAX', 'ATST', 'BBGI', 'CEY', 'DNLM', 'ECOR',\n",
        "        'FDM', 'GFTU', 'HMSO', 'ITV', 'JMAT', 'KWS', 'LWDB', 'MTO', 'NCC', 'PDL',\n",
        "        'QUIZ', 'RBD', 'SOHO', 'TND', 'UKW', 'VNET', 'WBI', 'XPS', 'AML', 'BOWL',\n",
        "        'CAML', 'DSCV', 'ECEL', 'FIPP', 'GAMA', 'HRI', 'INSP', 'JET2', 'KBT',\n",
        "        'LIO', 'MERC', 'NIOX', 'PALM', 'QRT', 'RKH', 'SIS', 'TSTG', 'UTG', 'VOG',\n",
        "        'WPS', 'XAR', 'YOOM'\n",
        "    ]\n",
        "\n",
        "def get_all_lse_metrics(tickers):\n",
        "    \"\"\"\n",
        "    Get fundamental metrics for all LSE stocks with minimal filtering\n",
        "    Only excludes stocks with completely missing data\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for ticker in tqdm(tickers, desc=\"Processing stocks\"):\n",
        "        yf_ticker = f\"{ticker}.L\"\n",
        "        try:\n",
        "            stock = yf.Ticker(yf_ticker)\n",
        "            time.sleep(0.1)  # Very light rate limiting\n",
        "\n",
        "            info = stock.info\n",
        "            if not info:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Get basic metrics (all optional)\n",
        "                metrics = {\n",
        "                    'Ticker': ticker,\n",
        "                    'Name': info.get('shortName', ticker),\n",
        "                    'Sector': info.get('sector', 'N/A'),\n",
        "                    'Price (GBp)': info.get('currentPrice', None),\n",
        "                    'Market Cap (£M)': info.get('marketCap', None),\n",
        "                    'P/E': info.get('trailingPE', None) or info.get('forwardPE', None),\n",
        "                    'P/B': info.get('priceToBook', None),\n",
        "                    'Debt/Equity': info.get('debtToEquity', None),\n",
        "                    'Operating Margin %': info.get('operatingMargins', None),\n",
        "                    'Dividend Yield %': info.get('dividendYield', None),\n",
        "                    'Beta': info.get('beta', None)\n",
        "                }\n",
        "\n",
        "                # Convert price to pence if available\n",
        "                if metrics['Price (GBp)'] is not None:\n",
        "                    metrics['Price (GBp)'] *= 1\n",
        "\n",
        "                # Convert market cap to millions if available\n",
        "                if metrics['Market Cap (£M)'] is not None:\n",
        "                    metrics['Market Cap (£M)'] /= 1e6\n",
        "\n",
        "                # Convert percentages if available\n",
        "                if metrics['Operating Margin %'] is not None:\n",
        "                    metrics['Operating Margin %'] *= 100\n",
        "                if metrics['Dividend Yield %'] is not None:\n",
        "                    metrics['Dividend Yield %'] *= 100\n",
        "\n",
        "                results.append(metrics)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def run_lse_fundamental_collection():\n",
        "    \"\"\"Run the LSE fundamental data collection\"\"\"\n",
        "    print(f\"LSE Fundamental Data Collector - {datetime.now().strftime('%d %b %Y')}\\n\")\n",
        "\n",
        "    # Get all possible tickers\n",
        "    lse_tickers = get_full_lse_tickers()\n",
        "    print(f\"Loaded {len(lse_tickers)} LSE tickers\\n\")\n",
        "\n",
        "    # Get all available metrics with no filtering\n",
        "    df = get_all_lse_metrics(lse_tickers)\n",
        "\n",
        "    if not df.empty:\n",
        "        # Clean and format the data\n",
        "        numeric_cols = ['Price (GBp)', 'Market Cap (£M)', 'P/E', 'P/B',\n",
        "                       'Debt/Equity', 'Operating Margin %', 'Dividend Yield %', 'Beta']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Sort by market cap descending\n",
        "        df = df.sort_values('Market Cap (£M)', ascending=False)\n",
        "\n",
        "        print(f\"Found {len(df)} stocks with available data:\")\n",
        "        pd.set_option('display.max_rows', None)\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.width', 1000)\n",
        "        pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "        print(df)\n",
        "\n",
        "        # Save full dataset\n",
        "        filename = f\"lse_full_metrics_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"\\nFull results saved to {filename}\")\n",
        "\n",
        "        # Generate potential value candidates\n",
        "        print(\"\\nPotential value candidates (manual review recommended):\")\n",
        "        value_candidates = df[\n",
        "            (df['P/E'].notna()) &\n",
        "            (df['P/B'].notna()) &\n",
        "            (df['P/E'] < 30) &\n",
        "            (df['P/B'] < 3)\n",
        "        ].sort_values(['P/E', 'P/B'])\n",
        "\n",
        "        print(value_candidates.head(50))\n",
        "\n",
        "        return df, value_candidates\n",
        "    else:\n",
        "        print(\"No data retrieved - please check your connection or try again later.\")\n",
        "        return None, None\n",
        "\n",
        "class ComprehensiveMarketSentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.sentiment_data = []\n",
        "        self.last_update = None\n",
        "        self.setup_market_data()\n",
        "\n",
        "    def setup_market_data(self):\n",
        "        \"\"\"Define stocks from multiple markets including LSE\"\"\"\n",
        "        self.market_stocks = {\n",
        "            # UK Stocks (LSE - London Stock Exchange)\n",
        "            'LSE': list(LSE_TICKERS.keys()),\n",
        "            'LSE Penny': list(PENNY_STOCKS.keys())\n",
        "        }\n",
        "\n",
        "        # All stocks combined for analysis\n",
        "        self.all_stocks = []\n",
        "        for market in self.market_stocks.values():\n",
        "            self.all_stocks.extend(market)\n",
        "\n",
        "    def get_stock_data(self, symbol, period='1d', interval='5m'):\n",
        "        \"\"\"Get real-time stock data with enhanced error handling\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(symbol)\n",
        "            data = stock.history(period=period, interval=interval)\n",
        "\n",
        "            if data.empty or len(data) < 2:\n",
        "                # Try with longer period for less liquid stocks\n",
        "                data = stock.history(period='5d', interval='15m')\n",
        "\n",
        "            return data if not data.empty else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def calculate_technical_indicators(self, data):\n",
        "        \"\"\"Calculate additional technical indicators\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return {}\n",
        "\n",
        "        # RSI (Relative Strength Index)\n",
        "        delta = data['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # Moving Averages\n",
        "        sma_20 = data['Close'].rolling(window=20).mean()\n",
        "        sma_50 = data['Close'].rolling(window=50).mean()\n",
        "\n",
        "        # MACD\n",
        "        exp12 = data['Close'].ewm(span=12).mean()\n",
        "        exp26 = data['Close'].ewm(span=26).mean()\n",
        "        macd = exp12 - exp26\n",
        "        signal = macd.ewm(span=9).mean()\n",
        "\n",
        "        return {\n",
        "            'rsi': rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50,\n",
        "            'sma_20': sma_20.iloc[-1],\n",
        "            'sma_50': sma_50.iloc[-1],\n",
        "            'macd': macd.iloc[-1],\n",
        "            'signal': signal.iloc[-1]\n",
        "        }\n",
        "\n",
        "    def calculate_volume_sentiment(self, data):\n",
        "        \"\"\"Calculate sentiment based on volume patterns\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return 0\n",
        "\n",
        "        current_volume = data['Volume'].iloc[-1]\n",
        "        avg_volume_5d = data['Volume'].rolling(window=5).mean().iloc[-1]\n",
        "        avg_volume_20d = data['Volume'].rolling(window=20).mean().iloc[-1]\n",
        "\n",
        "        if avg_volume_20d == 0:  # Avoid division by zero\n",
        "            return 0\n",
        "\n",
        "        volume_ratio = current_volume / avg_volume_20d\n",
        "\n",
        "        if volume_ratio > 2.0:\n",
        "            return 1.0  # Very high volume\n",
        "        elif volume_ratio > 1.5:\n",
        "            return 0.7  # High volume\n",
        "        elif volume_ratio > 1.2:\n",
        "            return 0.4  # Moderate volume\n",
        "        elif volume_ratio > 0.8:\n",
        "            return 0.1  # Normal volume\n",
        "        else:\n",
        "            return -0.2  # Low volume\n",
        "\n",
        "    def calculate_price_sentiment(self, data, technicals):\n",
        "        \"\"\"Calculate sentiment based on price movement and technicals\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return 0\n",
        "\n",
        "        current_price = data['Close'].iloc[-1]\n",
        "        open_price = data['Open'].iloc[-1]\n",
        "        prev_close = data['Close'].iloc[-2]\n",
        "\n",
        "        # Price changes\n",
        "        daily_change_pct = ((current_price - open_price) / open_price) * 100\n",
        "        prev_change_pct = ((current_price - prev_close) / prev_close) * 100\n",
        "\n",
        "        # Technical analysis sentiment\n",
        "        rsi_sentiment = 0\n",
        "        if technicals['rsi'] > 70:\n",
        "            rsi_sentiment = -0.3  # Overbought\n",
        "        elif technicals['rsi'] < 30:\n",
        "            rsi_sentiment = 0.3   # Oversold\n",
        "\n",
        "        ma_sentiment = 0\n",
        "        if current_price > technicals['sma_20'] > technicals['sma_50']:\n",
        "            ma_sentiment = 0.4  # Strong uptrend\n",
        "        elif current_price < technicals['sma_20'] < technicals['sma_50']:\n",
        "            ma_sentiment = -0.4  # Strong downtrend\n",
        "\n",
        "        macd_sentiment = 0.2 if technicals['macd'] > technicals['signal'] else -0.2\n",
        "\n",
        "        # Combined sentiment\n",
        "        price_sentiment = (\n",
        "            (min(1, daily_change_pct / 5) * 0.4) +\n",
        "            (min(1, prev_change_pct / 3) * 0.3) +\n",
        "            rsi_sentiment +\n",
        "            ma_sentiment +\n",
        "            macd_sentiment\n",
        "        )\n",
        "\n",
        "        return max(-1, min(1, price_sentiment))\n",
        "\n",
        "    def get_market_news(self, symbol):\n",
        "        \"\"\"Fetch market news with multiple sources\"\"\"\n",
        "        try:\n",
        "            # Remove .L suffix for LSE stocks for news search\n",
        "            search_symbol = symbol.replace('.L', '') if '.L' in symbol else symbol\n",
        "\n",
        "            # Get company name\n",
        "            if symbol in LSE_TICKERS:\n",
        "                company_name = LSE_TICKERS[symbol]\n",
        "            elif symbol in PENNY_STOCKS:\n",
        "                company_name = PENNY_STOCKS[symbol]\n",
        "            else:\n",
        "                company_name = search_symbol\n",
        "\n",
        "            # Fetch actual news using the existing function\n",
        "            articles = fetch_news_for_ticker(symbol, company_name, \"regular\")\n",
        "\n",
        "            if not articles:\n",
        "                # Fallback to simulated news\n",
        "                news_sources = [\n",
        "                    f\"Financial Times: {search_symbol} shows strong momentum\",\n",
        "                    f\"Bloomberg: Analysts positive on {search_symbol} outlook\",\n",
        "                    f\"Reuters: {search_symbol} earnings exceed expectations\",\n",
        "                    f\"MarketWatch: {search_symbol} trading volume spikes\",\n",
        "                    f\"CNBC: Institutional investors accumulating {search_symbol}\",\n",
        "                    f\"WSJ: {search_symbol} announces strategic initiatives\",\n",
        "                    f\"Investing.com: Technical breakout for {search_symbol}\",\n",
        "                    f\"SeekingAlpha: {search_symbol} undervalued relative to peers\"\n",
        "                ]\n",
        "\n",
        "                articles = [{'title': np.random.choice(news_sources),\n",
        "                            'source': 'Financial News',\n",
        "                            'publishedAt': datetime.now().isoformat()}]\n",
        "\n",
        "            return articles\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"News error for {symbol}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_news_sentiment(self, articles):\n",
        "        \"\"\"Analyze sentiment from news headlines\"\"\"\n",
        "        if not articles:\n",
        "            return 0\n",
        "\n",
        "        sentiments = []\n",
        "        for article in articles:\n",
        "            title = article.get('title', '')\n",
        "            if title:\n",
        "                try:\n",
        "                    analysis = TextBlob(title)\n",
        "                    sentiment = analysis.sentiment.polarity\n",
        "                    # Weight by source credibility\n",
        "                    source = article.get('source', '').lower()\n",
        "                    if 'bloomberg' in source or 'financial times' in source:\n",
        "                        sentiment *= 1.2\n",
        "                    elif 'reuters' in source:\n",
        "                        sentiment *= 1.1\n",
        "                    sentiments.append(sentiment)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return np.mean(sentiments) if sentiments else 0\n",
        "\n",
        "    def get_social_sentiment(self, symbol):\n",
        "        \"\"\"Enhanced social media sentiment analysis\"\"\"\n",
        "        # Simulate social media data from multiple platforms\n",
        "        platforms = {\n",
        "            'twitter': np.random.uniform(-0.4, 0.4),\n",
        "            'reddit': np.random.uniform(-0.3, 0.3),\n",
        "            'stocktwits': np.random.uniform(-0.5, 0.5),\n",
        "            'forums': np.random.uniform(-0.2, 0.2)\n",
        "        }\n",
        "\n",
        "        # Volume simulation\n",
        "        volumes = {\n",
        "            'twitter': np.random.randint(100, 1000),\n",
        "            'reddit': np.random.randint(50, 500),\n",
        "            'stocktwits': np.random.randint(200, 800),\n",
        "            'forums': np.random.randint(30, 300)\n",
        "        }\n",
        "\n",
        "        # Weighted average by platform volume\n",
        "        total_volume = sum(volumes.values())\n",
        "        weighted_sentiment = sum(platforms[p] * (volumes[p] / total_volume) for p in platforms)\n",
        "\n",
        "        return weighted_sentiment\n",
        "\n",
        "    def get_options_flow(self, symbol):\n",
        "        \"\"\"Simulate options flow data\"\"\"\n",
        "        # More realistic options simulation\n",
        "        if '.L' in symbol:  # LSE stocks have different options activity\n",
        "            call_volume = np.random.randint(50, 300)\n",
        "            put_volume = np.random.randint(40, 250)\n",
        "        else:  # US stocks\n",
        "            call_volume = np.random.randint(100, 2000)\n",
        "            put_volume = np.random.randint(80, 1500)\n",
        "\n",
        "        put_call_ratio = put_volume / (call_volume + 1e-6)\n",
        "\n",
        "        if put_call_ratio < 0.7:\n",
        "            return 0.4  # Very bullish\n",
        "        elif put_call_ratio < 0.9:\n",
        "            return 0.2  # Bullish\n",
        "        elif put_call_ratio > 1.3:\n",
        "            return -0.4  # Very bearish\n",
        "        elif put_call_ratio > 1.1:\n",
        "            return -0.2  # Bearish\n",
        "        else:\n",
        "            return 0  # Neutral\n",
        "\n",
        "    def calculate_market_cap_sentiment(self, symbol, current_price):\n",
        "        \"\"\"Consider market cap in sentiment analysis\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(symbol)\n",
        "            info = stock.info\n",
        "            market_cap = info.get('marketCap', 0)\n",
        "\n",
        "            if market_cap > 200e9:  # Large cap\n",
        "                return -0.1  # More stable, less volatile sentiment\n",
        "            elif market_cap > 10e9:  # Mid cap\n",
        "                return 0.0\n",
        "            else:  # Small cap\n",
        "                return 0.2  # More volatile, higher potential sentiment\n",
        "\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_composite_sentiment(self, symbol):\n",
        "        \"\"\"Calculate comprehensive market sentiment\"\"\"\n",
        "        stock_data = self.get_stock_data(symbol)\n",
        "\n",
        "        if stock_data is None or len(stock_data) < 5:\n",
        "            return None\n",
        "\n",
        "        technicals = self.calculate_technical_indicators(stock_data)\n",
        "        current_price = stock_data['Close'].iloc[-1]\n",
        "\n",
        "        # Get all sentiment components\n",
        "        volume_sentiment = self.calculate_volume_sentiment(stock_data)\n",
        "        price_sentiment = self.calculate_price_sentiment(stock_data, technicals)\n",
        "\n",
        "        news = self.get_market_news(symbol)\n",
        "        news_sentiment = self.analyze_news_sentiment(news)\n",
        "\n",
        "        social_sentiment = self.get_social_sentiment(symbol)\n",
        "        options_sentiment = self.get_options_flow(symbol)\n",
        "        market_cap_sentiment = self.calculate_market_cap_sentiment(symbol, current_price)\n",
        "\n",
        "        # Weighted composite sentiment (adjusted for different markets)\n",
        "        weights = {\n",
        "            'price': 0.30,\n",
        "            'volume': 0.20,\n",
        "            'news': 0.15,\n",
        "            'social': 0.10,\n",
        "            'options': 0.10,\n",
        "            'market_c': 0.05,\n",
        "            'technicals': 0.10\n",
        "        }\n",
        "\n",
        "        # Add technical sentiment from RSI and moving averages\n",
        "        technical_sentiment = (\n",
        "            (0.3 if technicals['rsi'] < 40 else -0.3 if technicals['rsi'] > 60 else 0) +\n",
        "            (0.2 if current_price > technicals['sma_20'] else -0.2)\n",
        "        )\n",
        "\n",
        "        composite = (\n",
        "            price_sentiment * weights['price'] +\n",
        "            volume_sentiment * weights['volume'] +\n",
        "            news_sentiment * weights['news'] +\n",
        "            social_sentiment * weights['social'] +\n",
        "            options_sentiment * weights['options'] +\n",
        "            market_cap_sentiment * weights['market_c'] +\n",
        "            # market_cap_sentiment * weights['market_cap'] +\n",
        "            technical_sentiment * weights['technicals']\n",
        "        )\n",
        "\n",
        "        # Determine signal strength\n",
        "        if composite > 0.3:\n",
        "            signal = \"STRONG BUY\"\n",
        "            confidence = \"HIGH\"\n",
        "        elif composite > 0.15:\n",
        "            signal = \"BUY\"\n",
        "            confidence = \"MEDIUM\"\n",
        "        elif composite > 0.05:\n",
        "            signal = \"MILD BUY\"\n",
        "            confidence = \"LOW\"\n",
        "        elif composite < -0.3:\n",
        "            signal = \"STRONG SELL\"\n",
        "            confidence = \"HIGH\"\n",
        "        elif composite < -0.15:\n",
        "            signal = \"SELL\"\n",
        "            confidence = \"MEDIUM\"\n",
        "        elif composite < -0.05:\n",
        "            signal = \"MILD SELL\"\n",
        "            confidence = \"LOW\"\n",
        "        else:\n",
        "            signal = \"NEUTRAL\"\n",
        "            confidence = \"LOW\"\n",
        "\n",
        "        result = {\n",
        "            'symbol': symbol,\n",
        "            'market': 'LSE' if '.L' in symbol else 'LSE Penny',\n",
        "            'timestamp': datetime.now(),\n",
        "            'composite_sentiment': composite,\n",
        "            'signal': signal,\n",
        "            'confidence': confidence,\n",
        "            'current_price': current_price,\n",
        "            'price_change': ((current_price - stock_data['Open'].iloc[-1]) / stock_data['Open'].iloc[-1]) * 100,\n",
        "            'volume': stock_data['Volume'].iloc[-1],\n",
        "            'rsi': technicals['rsi'],\n",
        "            'components': {\n",
        "                'price': price_sentiment,\n",
        "                'volume': volume_sentiment,\n",
        "                'news': news_sentiment,\n",
        "                'social': social_sentiment,\n",
        "                'options': options_sentiment\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.sentiment_data.append(result)\n",
        "        return result\n",
        "\n",
        "    def analyze_top_stocks(self, top_n=15):\n",
        "        \"\"\"Analyze and recommend top stocks\"\"\"\n",
        "        print(\"🔍 Analyzing market sentiment across all stocks...\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        results = []\n",
        "        total_stocks = len(self.all_stocks)\n",
        "\n",
        "        for i, symbol in enumerate(self.all_stocks, 1):\n",
        "            print(f\"Processing {i}/{total_stocks}: {symbol}\", end='\\r')\n",
        "            result = self.calculate_composite_sentiment(symbol)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "            time.sleep(0.1)  # Be polite to API\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "\n",
        "        # Sort by composite sentiment (highest first)\n",
        "        results.sort(key=lambda x: x['composite_sentiment'], reverse=True)\n",
        "\n",
        "        # Get top recommendations\n",
        "        top_stocks = results[:top_n]\n",
        "\n",
        "        return top_stocks, results\n",
        "\n",
        "    def display_recommendations(self, top_stocks, all_results):\n",
        "        \"\"\"Display top stock recommendations\"\"\"\n",
        "        print(f\"\\n🎯 TOP {len(top_stocks)} STOCK RECOMMENDATIONS\")\n",
        "        print(\"=\" * 120)\n",
        "        print(f\"{'Rank':<5} {'Symbol':<8} {'Market':<8} {'Price':<10} {'Change%':<8} {'Signal':<12} {'Confidence':<10} {'Sentiment':<10} {'RSI':<6}\")\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "        for i, stock in enumerate(top_stocks, 1):\n",
        "            change_color = '\\033[92m' if stock['price_change'] >= 0 else '\\033[91m'\n",
        "            signal_color = '\\033[92m' if 'BUY' in stock['signal'] else '\\033[91m' if 'SELL' in stock['signal'] else '\\033[93m'\n",
        "            rsi_color = '\\033[91m' if stock['rsi'] > 70 else '\\033[92m' if stock['rsi'] < 30 else '\\033[93m'\n",
        "\n",
        "            print(f\"{i:<5} {stock['symbol']:<8} {stock['market']:<8} \"\n",
        "                  f\"${stock['current_price']:<9.2f} \"\n",
        "                  f\"{change_color}{stock['price_change']:>+6.1f}%\\033[0m \"\n",
        "                  f\"{signal_color}{stock['signal']:<12}\\033[0m \"\n",
        "                  f\"{stock['confidence']:<10} \"\n",
        "                  f\"{stock['composite_sentiment']:>+8.3f} \"\n",
        "                  f\"{rsi_color}{stock['rsi']:>5.1f}\\033[0m\")\n",
        "\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "        # Market distribution\n",
        "        market_counts = pd.Series([s['market'] for s in top_stocks]).value_counts()\n",
        "        print(f\"\\n📊 Market Distribution in Top {len(top_stocks)}:\")\n",
        "        for market, count in market_counts.items():\n",
        "            print(f\"  {market}: {count} stocks\")\n",
        "\n",
        "        # Signal distribution\n",
        "        signal_counts = pd.Series([s['signal'] for s in top_stocks]).value_counts()\n",
        "        print(f\"\\n🎯 Signal Distribution:\")\n",
        "        for signal, count in signal_counts.items():\n",
        "            print(f\"  {signal}: {count} stocks\")\n",
        "\n",
        "        return top_stocks\n",
        "\n",
        "    def generate_detailed_report(self, top_stocks):\n",
        "        \"\"\"Generate detailed analysis report\"\"\"\n",
        "        print(f\"\\n📈 DETAILED ANALYSIS REPORT\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for i, stock in enumerate(top_stocks[:5], 1):  # Top 5 detailed analysis\n",
        "            print(f\"\\n{i}. {stock['symbol']} ({stock['market']}) - {stock['signal']}\")\n",
        "            print(f\"   Current Price: ${stock['current_price']:.2f}\")\n",
        "            print(f\"   Today's Change: {stock['price_change']:+.2f}%\")\n",
        "            print(f\"   Composite Sentiment: {stock['composite_sentiment']:+.3f}\")\n",
        "            print(f\"   RSI: {stock['rsi']:.1f}\")\n",
        "            print(f\"   Confidence: {stock['confidence']}\")\n",
        "            print(\"   Component Breakdown:\")\n",
        "            for comp, value in stock['components'].items():\n",
        "                print(f\"     - {comp.capitalize()}: {value:+.3f}\")\n",
        "\n",
        "    def create_visualizations(self, all_results):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        if not all_results:\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results)\n",
        "\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Sentiment Distribution by Market\n",
        "        plt.subplot(2, 3, 1)\n",
        "        market_data = df.groupby('market')['composite_sentiment'].mean()\n",
        "        market_data.plot(kind='bar', color=['blue', 'green', 'orange', 'red'])\n",
        "        plt.title('Average Sentiment by Market')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # 2. Top 10 Stocks by Sentiment\n",
        "        plt.subplot(2, 3, 2)\n",
        "        top_10 = df.nlargest(10, 'composite_sentiment')\n",
        "        plt.barh(top_10['symbol'], top_10['composite_sentiment'],\n",
        "                color=['green' if x > 0 else 'red' for x in top_10['composite_sentiment']])\n",
        "        plt.title('Top 10 Stocks by Sentiment')\n",
        "        plt.xlabel('Sentiment Score')\n",
        "\n",
        "        # 3. RSI Distribution\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.hist(df['rsi'], bins=20, alpha=0.7, color='purple')\n",
        "        plt.axvline(30, color='green', linestyle='--', label='Oversold (30)')\n",
        "        plt.axvline(70, color='red', linestyle='--', label='Overbought (70)')\n",
        "        plt.title('RSI Distribution')\n",
        "        plt.legend()\n",
        "\n",
        "        # 4. Signal Distribution\n",
        "        plt.subplot(2, 3, 4)\n",
        "        signal_counts = df['signal'].value_counts()\n",
        "        colors = ['green' if 'BUY' in s else 'red' if 'SELL' in s else 'gray' for s in signal_counts.index]\n",
        "        signal_counts.plot(kind='pie', autopct='%1.1f%%', colors=colors)\n",
        "        plt.title('Overall Signal Distribution')\n",
        "\n",
        "        # 5. Price vs Sentiment Scatter\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.scatter(df['current_price'], df['composite_sentiment'], alpha=0.6)\n",
        "        plt.xlabel('Price ($)')\n",
        "        plt.ylabel('Sentiment Score')\n",
        "        plt.title('Price vs Sentiment')\n",
        "\n",
        "        # 6. Volume vs Sentiment\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.scatter(np.log(df['volume'] + 1), df['composite_sentiment'], alpha=0.6)\n",
        "        plt.xlabel('Log Volume')\n",
        "        plt.ylabel('Sentiment Score')\n",
        "        plt.title('Trading Volume vs Sentiment')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_recommendations(self, top_stocks, filename='stock_recommendations.csv'):\n",
        "        \"\"\"Save recommendations to CSV file\"\"\"\n",
        "        df = pd.DataFrame(top_stocks)\n",
        "\n",
        "        # Flatten components dictionary\n",
        "        components_df = pd.json_normalize(df['components'])\n",
        "        df = pd.concat([df.drop(['components', 'timestamp'], axis=1), components_df], axis=1)\n",
        "\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"\\n💾 Recommendations saved to {filename}\")\n",
        "\n",
        "def safe_divide(a, b):\n",
        "    \"\"\"Safe division function to handle division by zero\"\"\"\n",
        "    try:\n",
        "        return float(a) / float(b) if float(b) != 0 else 0\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def convert_to_float(value):\n",
        "    \"\"\"Convert value to float safely\"\"\"\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "def get_price_data(ticker, days=90):\n",
        "    \"\"\"Get historical price data for technical analysis\"\"\"\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=days)\n",
        "    try:\n",
        "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading price data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def add_technical_indicators(data, indicators, stock_type=\"regular\"):\n",
        "    \"\"\"Add selected technical indicators to price data\"\"\"\n",
        "    if data.empty:\n",
        "        return data\n",
        "\n",
        "    # Set window sizes based on stock type\n",
        "    sma_window = 10 if stock_type == \"penny\" else 20\n",
        "    ema_window = 10 if stock_type == \"penny\" else 20\n",
        "\n",
        "    for indicator in indicators:\n",
        "        indicator = indicator.strip().lower()\n",
        "        if indicator == \"sma\":\n",
        "            data[f'SMA_{sma_window}'] = data['Close'].rolling(window=sma_window).mean()\n",
        "        elif indicator == \"ema\":\n",
        "            data[f'EMA_{ema_window}'] = data['Close'].ewm(span=ema_window).mean()\n",
        "        elif indicator == \"bollinger\":\n",
        "            window = 10 if stock_type == \"penny\" else 20\n",
        "            data[f'SMA_{window}'] = data['Close'].rolling(window=window).mean()\n",
        "            data[f'STD_{window}'] = data['Close'].rolling(window=window).std()\n",
        "            data['BB_Upper'] = data[f'SMA_{window}'] + 2 * data[f'STD_{window}']\n",
        "            data['BB_Lower'] = data[f'SMA_{window}'] - 2 * data[f'STD_{window}']\n",
        "        elif indicator == \"vwap\":\n",
        "            if 'Volume' in data and data['Volume'].sum() > 0:\n",
        "                data['VWAP'] = (data['Close'] * data['Volume']).cumsum() / data['Volume'].cumsum()\n",
        "        elif indicator == \"rsi\":\n",
        "            delta = data['Close'].diff()\n",
        "            gain = delta.where(delta > 0, 0)\n",
        "            loss = -delta.where(delta < 0, 0)\n",
        "            avg_gain = gain.rolling(window=14).mean()\n",
        "            avg_loss = loss.rolling(window=14).mean()\n",
        "            rs = avg_gain / avg_loss\n",
        "            data['RSI'] = 100 - (100 / (1 + rs))\n",
        "    return data\n",
        "\n",
        "def plot_price_chart(ticker, company, indicators, stock_type=\"regular\"):\n",
        "    \"\"\"Create interactive price chart with selected indicators\"\"\"\n",
        "    days = 90 if stock_type == \"penny\" else 180\n",
        "    data = get_price_data(ticker, days)\n",
        "    if data.empty:\n",
        "        print(f\"\\nNo price data available for {ticker}\")\n",
        "        return\n",
        "\n",
        "    data = add_technical_indicators(data, indicators, stock_type)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Candlestick chart\n",
        "    fig.add_trace(go.Candlestick(\n",
        "        x=data.index,\n",
        "        open=data['Open'],\n",
        "        high=data['High'],\n",
        "        low=data['Low'],\n",
        "        close=data['Close'],\n",
        "        name='Price'\n",
        "    ))\n",
        "\n",
        "    # Add selected indicators\n",
        "    for indicator in indicators:\n",
        "        indicator = indicator.strip().lower()\n",
        "        window = 10 if stock_type == \"penny\" else 20\n",
        "\n",
        "        if indicator == \"sma\" and f'SMA_{window}' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data[f'SMA_{window}'],\n",
        "                mode='lines',\n",
        "                name=f'SMA ({window})',\n",
        "                line=dict(color='blue', width=2)\n",
        "            ))\n",
        "        elif indicator == \"ema\" and f'EMA_{window}' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data[f'EMA_{window}'],\n",
        "                mode='lines',\n",
        "                name=f'EMA ({window})',\n",
        "                line=dict(color='green', width=2)\n",
        "            ))\n",
        "        elif indicator == \"bollinger\" and all(col in data for col in ['BB_Upper', 'BB_Lower']):\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['BB_Upper'],\n",
        "                mode='lines',\n",
        "                name='BB Upper',\n",
        "                line=dict(color='red', width=1)\n",
        "            ))\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['BB_Lower'],\n",
        "                mode='lines',\n",
        "                name='BB Lower',\n",
        "                line=dict(color='blue', width=2),\n",
        "                fill='tonexty',\n",
        "                fillcolor='rgba(255,0,0,0.1)'\n",
        "            ))\n",
        "        elif indicator == \"vwap\" and 'VWAP' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['VWAP'],\n",
        "                mode='lines',\n",
        "                name='VWAP',\n",
        "                line=dict(color='purple', width=2)\n",
        "            ))\n",
        "        elif indicator == \"rsi\" and 'RSI' in data:\n",
        "            # Create RSI subplot\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['RSI'],\n",
        "                mode='lines',\n",
        "                name='RSI',\n",
        "                line=dict(color='orange', width=2),\n",
        "                secondary_y=True\n",
        "            ))\n",
        "            # Add RSI reference lines\n",
        "            fig.add_hline(y=70, line_dash=\"dot\", line_color=\"red\",\n",
        "                         annotation_text=\"Overbought\", annotation_position=\"top right\",\n",
        "                         secondary_y=True)\n",
        "            fig.add_hline(y=30, line_dash=\"dot\", line_color=\"green\",\n",
        "                         annotation_text=\"Oversold\", annotation_position=\"bottom right\",\n",
        "                         secondary_y=True)\n",
        "\n",
        "    title_suffix = \"Penny Stock\" if stock_type == \"penny\" else \"Stock\"\n",
        "    fig.update_layout(\n",
        "        title=f\"{company} ({ticker}) - {title_suffix} Price Chart\",\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=\"Price (GBp)\" if \".L\" in ticker else \"Price ($)\",\n",
        "        xaxis_rangeslider_visible=False,\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    if 'rsi' in [i.strip().lower() for i in indicators]:\n",
        "        fig.update_layout(\n",
        "            yaxis2=dict(\n",
        "                title=\"RSI\",\n",
        "                overlaying=\"y\",\n",
        "                side=\"right\",\n",
        "                range=[0, 100]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def get_insider_transactions(ticker):\n",
        "    \"\"\"Scrape insider transactions data from MarketWatch\"\"\"\n",
        "    try:\n",
        "        base_ticker = ticker.replace('.L', '') if '.L' in ticker else ticker\n",
        "        url = f\"https://www.marketwatch.com/investing/stock/{base_ticker}/insideractions\"\n",
        "        headers = {'User-Agent': ua.random}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        transactions = []\n",
        "        for row in soup.select('table.insider-actions tr')[1:6]:  # Get last 5 transactions\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) >= 5:\n",
        "                transactions.append({\n",
        "                    'date': cells[0].text.strip(),\n",
        "                    'insider': cells[1].text.strip(),\n",
        "                    'position': cells[2].text.strip(),\n",
        "                    'transaction': cells[3].text.strip(),\n",
        "                    'shares': cells[4].text.strip()\n",
        "                })\n",
        "\n",
        "        buy_count = sum(1 for t in transactions if 'Buy' in t['transaction'])\n",
        "        sell_count = sum(1 for t in transactions if 'Sell' in t['transaction'])\n",
        "        sentiment = (buy_count - sell_count) / len(transactions) if transactions else 0\n",
        "\n",
        "        return {\n",
        "            'insider_transactions': transactions[:3],  # Return latest 3\n",
        "            'insider_sentiment': sentiment\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting insider transactions for {ticker}: {e}\")\n",
        "        return {\n",
        "            'insider_transactions': 'N/A',\n",
        "            'insider_sentiment': 0\n",
        "        }\n",
        "\n",
        "def calculate_metric_score(value, metric, stock_type=\"regular\"):\n",
        "    \"\"\"Calculate score for each metric (0-1) with safer division\"\"\"\n",
        "    try:\n",
        "        float_value = convert_to_float(value)\n",
        "        if pd.isna(float_value):\n",
        "            return 0\n",
        "\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        ideal = thresholds[metric]['ideal']\n",
        "        acceptable = thresholds[metric]['acceptable']\n",
        "\n",
        "        if metric in ['debt_to_equity', 'price_to_book', 'pe_ratio', 'peg_ratio']:\n",
        "            # For metrics where lower is better\n",
        "            if float_value <= ideal:\n",
        "                return 1\n",
        "            elif float_value <= acceptable:\n",
        "                return 0.5 * (1 + safe_divide((acceptable - float_value), (acceptable - ideal)))\n",
        "            else:\n",
        "                return max(0, 0.5 * safe_divide(acceptable, float_value))\n",
        "        else:\n",
        "            # For metrics where higher is better (operating margin)\n",
        "            if float_value >= ideal:\n",
        "                return 1\n",
        "            elif float_value >= acceptable:\n",
        "                return 0.5 * (1 + safe_divide((float_value - acceptable), (ideal - acceptable)))\n",
        "            else:\n",
        "                return max(0, 0.5 * safe_divide(float_value, acceptable))\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating score for {metric}: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_fundamentals(ticker, stock_type=\"regular\"):\n",
        "    \"\"\"Get fundamental data with improved error handling\"\"\"\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "\n",
        "        # Get current price with fallbacks\n",
        "        current_price = convert_to_float(\n",
        "            info.get('currentPrice',\n",
        "                   info.get('regularMarketPrice',\n",
        "                           info.get('ask', np.nan)))\n",
        "        )\n",
        "\n",
        "        if pd.isna(current_price):\n",
        "            print(f\"{ticker}: No price data available\")\n",
        "            return None\n",
        "\n",
        "        # Get volume data with error handling\n",
        "        try:\n",
        "            hist = stock.history(period=\"1mo\")\n",
        "            avg_volume = hist['Volume'].mean() if not hist.empty else 0\n",
        "        except Exception as e:\n",
        "            print(f\"{ticker}: Error getting volume data - {e}\")\n",
        "            avg_volume = 0\n",
        "\n",
        "        # Skip if not matching stock type criteria\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        if (current_price > thresholds['price']['max'] or\n",
        "            avg_volume < thresholds['volume']['min']):\n",
        "            return None\n",
        "\n",
        "        # Get company name from appropriate database\n",
        "        if stock_type == \"penny\":\n",
        "            company_name = PENNY_STOCKS.get(ticker, info.get('shortName', ticker))\n",
        "        else:\n",
        "            company_name = LSE_TICKERS.get(ticker, info.get('shortName', ticker))\n",
        "\n",
        "        fundamentals = {\n",
        "            'ticker': ticker,\n",
        "            'company': company_name,\n",
        "            'price': current_price,\n",
        "            'volume': avg_volume,\n",
        "            'operating_margin': convert_to_float(info.get('operatingMargins', np.nan)),\n",
        "            'debt_to_equity': convert_to_float(info.get('debtToEquity', np.nan)),\n",
        "            'price_to_book': convert_to_float(info.get('priceToBook', np.nan)),\n",
        "            'pe_ratio': convert_to_float(info.get('trailingPE', np.nan)),\n",
        "            'peg_ratio': convert_to_float(info.get('pegRatio', np.nan)),\n",
        "            'market_cap': convert_to_float(info.get('marketCap', np.nan)),\n",
        "            'sector': info.get('sector', 'N/A'),\n",
        "            'industry': info.get('industry', 'N/A'),\n",
        "            'beta': convert_to_float(info.get('beta', np.nan)),\n",
        "            'stock_type': stock_type\n",
        "        }\n",
        "\n",
        "        # Get insider transactions with error handling\n",
        "        try:\n",
        "            insider_data = get_insider_transactions(ticker)\n",
        "            fundamentals.update(insider_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting insider data for {ticker}: {e}\")\n",
        "            fundamentals.update({\n",
        "                'insider_transactions': 'N/A',\n",
        "                'insider_sentiment': 0\n",
        "            })\n",
        "\n",
        "        return fundamentals\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting fundamentals for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_news_for_ticker(ticker, company_name, stock_type=\"regular\"):\n",
        "    \"\"\"Fetch news articles for a specific ticker from all sources\"\"\"\n",
        "    all_articles = []\n",
        "\n",
        "    for source in NEWS_SOURCES:\n",
        "        try:\n",
        "            if source[\"parser\"] == \"google\":\n",
        "                query = f\"{ticker.replace('.L','')}+{company_name.replace(' ','+')}\"\n",
        "                if stock_type == \"penny\":\n",
        "                    query += \"+penny+stock\"\n",
        "                url = source[\"url\"].format(query=query)\n",
        "            else:\n",
        "                url = source[\"url\"]\n",
        "\n",
        "            headers = {'User-Agent': ua.random}\n",
        "\n",
        "            if source[\"parser\"] == \"google\":\n",
        "                feed = feedparser.parse(url)\n",
        "                for entry in feed.entries[:10]:\n",
        "                    all_articles.append({\n",
        "                        'title': entry.title,\n",
        "                        'link': entry.link,\n",
        "                        'source': 'Google News',\n",
        "                        'time': entry.published\n",
        "                    })\n",
        "                continue\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            if source[\"parser\"] == \"vox\":\n",
        "                for item in soup.select('div.post-content-wrap')[:10]:\n",
        "                    title_elem = item.select_one('h2.entry-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.entry-date').text if item.select_one('time.entry-date') else \"\"\n",
        "                        })\n",
        "\n",
        "            elif source[\"parser\"] == \"yahoo\":\n",
        "                for item in soup.select('h3[class*=\"Mb(5px)\"]')[:10]:\n",
        "                    link = item.find('a')['href']\n",
        "                    if not link.startswith('http'):\n",
        "                        link = f\"https://uk.finance.yahoo.com{link}\"\n",
        "                    all_articles.append({\n",
        "                        'title': item.text.strip(),\n",
        "                        'link': link,\n",
        "                        'source': source['name']\n",
        "                    })\n",
        "\n",
        "            elif source[\"parser\"] == \"pennystocks\":\n",
        "                for item in soup.select('div.td-module-container')[:10]:\n",
        "                    title_elem = item.select_one('h3.entry-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.entry-date')['datetime'] if item.select_one('time.entry-date') else \"\"\n",
        "                        })\n",
        "\n",
        "            elif source[\"parser\"] == \"investorplace\":\n",
        "                for item in soup.select('div.article-content')[:10]:\n",
        "                    title_elem = item.select_one('h4.article-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.article-time')['datetime'] if item.select_one('time.article-time') else \"\"\n",
        "                        })\n",
        "\n",
        "            time.sleep(1)  # Be polite with requests\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching from {source['name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Filter to only articles mentioning the ticker or company\n",
        "    filtered_articles = []\n",
        "    for article in all_articles:\n",
        "        if (ticker.replace('.L','') in article['title'] or\n",
        "            company_name.lower() in article['title'].lower()):\n",
        "            filtered_articles.append(article)\n",
        "\n",
        "    return filtered_articles\n",
        "\n",
        "def analyze_news_sentiment(articles):\n",
        "    \"\"\"Analyze sentiment for news articles\"\"\"\n",
        "    if not articles:\n",
        "        return 0, []\n",
        "\n",
        "    sentiments = []\n",
        "    analyzed_articles = []\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            sentiment = sia.polarity_scores(article['title'])\n",
        "            compound = sentiment['compound']\n",
        "            sentiments.append(compound)\n",
        "            analyzed_articles.append({\n",
        "                **article,\n",
        "                'sentiment': compound,\n",
        "                'sentiment_label': 'Positive' if compound >= 0.05 else 'Negative' if compound <= -0.05 else 'Neutral'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing article: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_sentiment = sum(sentiments)/len(sentiments) if sentiments else 0\n",
        "    return avg_sentiment, analyzed_articles\n",
        "\n",
        "def create_interactive_visualizations(top_stocks, stock_type=\"regular\"):\n",
        "    \"\"\"Create Plotly visualizations for the top stocks\"\"\"\n",
        "    # Prepare data for visualizations\n",
        "    tickers = [stock['ticker'] for stock in top_stocks]\n",
        "    companies = [stock['company'] for stock in top_stocks]\n",
        "    prices = [stock['price'] for stock in top_stocks]\n",
        "    volumes = [stock['volume']/1000000 for stock in top_stocks]  # In millions\n",
        "\n",
        "    if stock_type == \"regular\":\n",
        "        op_margins = [stock['operating_margin']*100 if not pd.isna(stock['operating_margin']) else 0 for stock in top_stocks]\n",
        "        debt_equity = [stock['debt_to_equity'] if not pd.isna(stock['debt_to_equity']) else 0 for stock in top_stocks]\n",
        "        pb_ratios = [stock['price_to_book'] if not pd.isna(stock['price_to_book']) else 0 for stock in top_stocks]\n",
        "        pe_ratios = [stock['pe_ratio'] if not pd.isna(stock['pe_ratio']) else 0 for stock in top_stocks]\n",
        "        peg_ratios = [stock['peg_ratio'] if not pd.isna(stock['peg_ratio']) else 0 for stock in top_stocks]\n",
        "\n",
        "    comp_scores = [stock['composite_score'] for stock in top_stocks]\n",
        "\n",
        "    # Get news sentiment for each stock\n",
        "    news_sentiments = []\n",
        "    for stock in top_stocks:\n",
        "        articles = fetch_news_for_ticker(stock['ticker'], stock['company'], stock_type)\n",
        "        avg_sentiment, _ = analyze_news_sentiment(articles)\n",
        "        news_sentiments.append(avg_sentiment)\n",
        "\n",
        "    # Create subplots based on stock type\n",
        "    if stock_type == \"regular\":\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Fundamental Metrics Comparison\",\n",
        "                \"Composite Scores vs News Sentiment\",\n",
        "                \"Valuation Ratios\",\n",
        "                \"Profitability & Financial Health\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"bar\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.15,\n",
        "            horizontal_spacing=0.15\n",
        "        )\n",
        "\n",
        "        # Fundamental Metrics Comparison (Bar Chart)\n",
        "        metrics = ['Operating Margin', 'Debt-to-Equity', 'Price-to-Book', 'P/E Ratio', 'PEG Ratio']\n",
        "        for i, company in enumerate(companies[:2]):  # Compare top 2 companies\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=metrics,\n",
        "                    y=[op_margins[i], debt_equity[i], pb_ratios[i], pe_ratios[i], peg_ratios[i]],\n",
        "                    name=company,\n",
        "                    marker_color='blue' if i == 0 else 'green'\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "        # Valuation Ratios (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=pb_ratios,\n",
        "                name='Price-to-Book',\n",
        "                marker_color='indianred'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=pe_ratios,\n",
        "                name='P/E Ratio',\n",
        "                marker_color='lightsalmon'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=peg_ratios,\n",
        "                name='PEG Ratio',\n",
        "                marker_color='crimson'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Profitability & Financial Health (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=op_margins,\n",
        "                name='Operating Margin (%)',\n",
        "                marker_color='darkgreen'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=debt_equity,\n",
        "                name='Debt-to-Equity',\n",
        "                marker_color='darkblue'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update axes for regular stocks\n",
        "        fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Ratio Value\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Percentage/Value\", row=2, col=2)\n",
        "\n",
        "    else:  # Penny stocks\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Price & Volume\",\n",
        "                \"Composite Scores vs News Sentiment\",\n",
        "                \"Financial Health\",\n",
        "                \"Operating Performance\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"bar\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.15,\n",
        "            horizontal_spacing=0.15\n",
        "        )\n",
        "\n",
        "        # Price & Volume (Dual Axis Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=prices,\n",
        "                name='Price ($)',\n",
        "                marker_color='blue'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=volumes,\n",
        "                name='Volume (M)',\n",
        "                marker_color='lightblue',\n",
        "                opacity=0.6\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Financial Health (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=debt_equity,\n",
        "                name='Debt-to-Equity',\n",
        "                marker_color='darkred'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Operating Performance (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=op_margins,\n",
        "                name='Operating Margin (%)',\n",
        "                marker_color='darkgreen'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update axes for penny stocks\n",
        "        fig.update_yaxes(title_text=\"Price / Volume\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Debt-to-Equity\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Operating Margin (%)\", row=2, col=2)\n",
        "\n",
        "    # Composite Scores vs News Sentiment (Scatter Plot) - common for both\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=comp_scores,\n",
        "            y=news_sentiments,\n",
        "            text=companies,\n",
        "            mode='markers+text',\n",
        "            marker=dict(\n",
        "                size=12,\n",
        "                color=comp_scores,\n",
        "                colorscale='Viridis',\n",
        "                showscale=True,\n",
        "                colorbar=dict(title=\"Composite Score\")\n",
        "            ),\n",
        "            name='Score vs Sentiment',\n",
        "            textposition='top center'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    title_suffix = \"Penny Stocks\" if stock_type == \"penny\" else \"LSE Stocks\"\n",
        "    fig.update_layout(\n",
        "        title_text=f\"Top {title_suffix} Analysis Dashboard\",\n",
        "        height=900,\n",
        "        showlegend=True,\n",
        "        hovermode=\"closest\",\n",
        "        template=\"plotly_white\",\n",
        "        barmode='group'\n",
        "    )\n",
        "\n",
        "    # Update subplot titles\n",
        "    fig.update_annotations(font_size=12)\n",
        "\n",
        "    # Update common axes\n",
        "    fig.update_yaxes(title_text=\"News Sentiment\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Composite Score\", row=1, col=2)\n",
        "\n",
        "    # Show the figure\n",
        "    fig.show()\n",
        "\n",
        "def screen_stocks(stock_type=\"regular\"):\n",
        "    \"\"\"Screen stocks based on fundamental metrics\"\"\"\n",
        "    screened_stocks = []\n",
        "\n",
        "    # Select appropriate stock database\n",
        "    stock_db = PENNY_STOCKS if stock_type == \"penny\" else LSE_TICKERS\n",
        "\n",
        "    for i, ticker in enumerate(stock_db.keys(), 1):\n",
        "        print(f\"Processing {i}/{len(stock_db)}: {ticker}\", end='\\r')\n",
        "\n",
        "        fundamentals = get_fundamentals(ticker, stock_type)\n",
        "        if not fundamentals:\n",
        "            continue\n",
        "\n",
        "        # Calculate scores for each metric\n",
        "        metric_scores = {\n",
        "            'operating_margin': calculate_metric_score(\n",
        "                fundamentals['operating_margin'], 'operating_margin', stock_type),\n",
        "            'debt_to_equity': calculate_metric_score(\n",
        "                fundamentals['debt_to_equity'], 'debt_to_equity', stock_type),\n",
        "            'price_to_book': calculate_metric_score(\n",
        "                fundamentals['price_to_book'], 'price_to_book', stock_type),\n",
        "            'pe_ratio': calculate_metric_score(\n",
        "                fundamentals['pe_ratio'], 'pe_ratio', stock_type),\n",
        "            'peg_ratio': calculate_metric_score(\n",
        "                fundamentals['peg_ratio'], 'peg_ratio', stock_type),\n",
        "            'insider_sentiment': max(0, fundamentals['insider_sentiment']),\n",
        "        }\n",
        "\n",
        "        # Add volume score for penny stocks\n",
        "        if stock_type == \"penny\":\n",
        "            metric_scores['volume'] = min(1, fundamentals['volume'] / 1000000)  # Normalize volume\n",
        "\n",
        "        # Calculate composite score with appropriate weights\n",
        "        if stock_type == \"penny\":\n",
        "            weights = {\n",
        "                'operating_margin': 0.15,\n",
        "                'debt_to_equity': 0.15,\n",
        "                'price_to_book': 0.15,\n",
        "                'pe_ratio': 0.1,\n",
        "                'peg_ratio': 0.1,\n",
        "                'insider_sentiment': 0.2,\n",
        "                'volume': 0.15\n",
        "            }\n",
        "        else:\n",
        "            weights = {\n",
        "                'operating_margin': 0.25,\n",
        "                'debt_to_equity': 0.2,\n",
        "                'price_to_book': 0.15,\n",
        "                'pe_ratio': 0.15,\n",
        "                'peg_ratio': 0.15,\n",
        "                'insider_sentiment': 0.1\n",
        "            }\n",
        "\n",
        "        composite_score = sum(\n",
        "            metric_scores[metric] * weights[metric]\n",
        "            for metric in metric_scores\n",
        "        )\n",
        "\n",
        "        # Count how many \"ideal\" criteria are met\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        ideal_met = sum(\n",
        "            1 for metric in ['operating_margin', 'debt_to_equity',\n",
        "                           'price_to_book', 'pe_ratio', 'peg_ratio']\n",
        "            if (not pd.isna(fundamentals[metric])) and\n",
        "               ((metric == 'operating_margin' and fundamentals[metric] >= thresholds[metric]['ideal']) or\n",
        "                (metric != 'operating_margin' and fundamentals[metric] <= thresholds[metric]['ideal']))\n",
        "        )\n",
        "\n",
        "        screened_stocks.append({\n",
        "            **fundamentals,\n",
        "            **metric_scores,\n",
        "            'composite_score': composite_score,\n",
        "            'ideal_met': ideal_met,\n",
        "            'data_quality': sum(0 if pd.isna(fundamentals[m]) else 1\n",
        "                          for m in ['operating_margin', 'debt_to_equity',\n",
        "                                  'price_to_book', 'pe_ratio', 'peg_ratio'])\n",
        "        })\n",
        "\n",
        "    # Filter stocks with at least 3 metrics worth of data\n",
        "    screened_stocks = [s for s in screened_stocks if s['data_quality'] >= 3]\n",
        "\n",
        "    # Sort by composite score then by number of ideal criteria met\n",
        "    screened_stocks.sort(key=lambda x: (-x['composite_score'], -x['ideal_met']))\n",
        "\n",
        "    return screened_stocks\n",
        "\n",
        "def display_top_candidates_with_news(stocks, stock_type=\"regular\"):\n",
        "    \"\"\"Display the top stocks with news sentiment analysis and interactive visualizations\"\"\"\n",
        "    if len(stocks) == 0:\n",
        "        print(f\"No {stock_type} stocks found matching the criteria\")\n",
        "        print(\"Possible reasons:\")\n",
        "        print(\"- Market may be closed now\")\n",
        "        print(\"- Some data sources may be temporarily unavailable\")\n",
        "        print(\"- The screening criteria may be too strict\")\n",
        "        return\n",
        "\n",
        "    top_5 = stocks[:5]\n",
        "\n",
        "    # Create main summary table\n",
        "    summary_data = []\n",
        "    for stock in top_5:\n",
        "        articles = fetch_news_for_ticker(stock['ticker'], stock['company'], stock_type)\n",
        "        avg_sentiment, _ = analyze_news_sentiment(articles)\n",
        "\n",
        "        if stock_type == \"penny\":\n",
        "            summary_data.append([\n",
        "                stock['ticker'],\n",
        "                stock['company'][:15] + '...' if len(stock['company']) > 15 else stock['company'],\n",
        "                f\"${stock['price']:.2f}\",\n",
        "                f\"{stock['volume']/1000:.1f}K\",\n",
        "                f\"{stock['composite_score']:.2f}\",\n",
        "                f\"{avg_sentiment:.2f}\",\n",
        "                f\"{stock['debt_to_equity']:.2f}\" if not pd.isna(stock['debt_to_equity']) else 'N/A',\n",
        "                len(articles)\n",
        "            ])\n",
        "        else:\n",
        "            summary_data.append([\n",
        "                stock['ticker'],\n",
        "                stock['company'][:15] + '...' if len(stock['company']) > 15 else stock['company'],\n",
        "                f\"{stock['composite_score']:.2f}\",\n",
        "                f\"{avg_sentiment:.2f}\",\n",
        "                f\"{stock['operating_margin']*100:.1f}%\" if not pd.isna(stock['operating_margin']) else 'N/A',\n",
        "                f\"{stock['debt_to_equity']:.2f}\" if not pd.isna(stock['debt_to_equity']) else 'N/A',\n",
        "                len(articles)\n",
        "            ])\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        headers = [\n",
        "            \"Ticker\", \"Company\", \"Price\", \"Volume\", \"Score\",\n",
        "            \"News Sent\", \"D/E\", \"News Count\"\n",
        "        ]\n",
        "    else:\n",
        "        headers = [\n",
        "            \"Ticker\", \"Company\", \"Fundamental\", \"News\", \"OpM\",\n",
        "            \"D/E\", \"News Count\"\n",
        "        ]\n",
        "\n",
        "    print(tabulate(summary_data, headers=headers, tablefmt='grid'))\n",
        "\n",
        "    # Detailed analysis of top candidate\n",
        "    best = top_5[0]\n",
        "    articles = fetch_news_for_ticker(best['ticker'], best['company'], stock_type)\n",
        "    avg_sentiment, analyzed_articles = analyze_news_sentiment(articles)\n",
        "\n",
        "    stock_type_label = \"Penny Stock\" if stock_type == \"penny\" else \"Stock\"\n",
        "    print(f\"\\nTop {stock_type_label}: {best['company']} ({best['ticker']})\")\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        print(f\"Current Price: ${best['price']:.2f}\")\n",
        "        print(f\"Average Volume: {best['volume']/1000:.1f}K\")\n",
        "\n",
        "    print(f\"Composite Score: {best['composite_score']:.2f}\")\n",
        "    print(f\"Average News Sentiment: {avg_sentiment:.2f}\")\n",
        "\n",
        "    print(\"\\nRecent News Headlines:\")\n",
        "    for article in analyzed_articles[:5]:\n",
        "        print(f\"- [{article['sentiment_label']}] {article['title']} ({article['source']})\")\n",
        "\n",
        "    print(\"\\nKey Metrics:\")\n",
        "    print(f\"- Operating Margin: {best['operating_margin']*100:.1f}%\" if not pd.isna(best['operating_margin']) else \"- Operating Margin: N/A\")\n",
        "    print(f\"- Debt-to-Equity: {best['debt_to_equity']:.2f}\" if not pd.isna(best['debt_to_equity']) else \"- Debt-to-Equity: N/A\")\n",
        "    print(f\"- Price-to-Book: {best['price_to_book']:.2f}\" if not pd.isna(best['price_to_book']) else \"- Price-to-Book: N/A\")\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        print(f\"- Beta (Volatility): {best['beta']:.2f}\" if not pd.isna(best['beta']) else \"- Beta: N/A\")\n",
        "\n",
        "    print(f\"- P/E Ratio: {best['pe_ratio']:.1f}\" if not pd.isna(best['pe_ratio']) else \"- P/E Ratio: N/A\")\n",
        "    print(f\"- PEG Ratio: {best['peg_ratio']:.1f}\" if not pd.isna(best['peg_ratio']) else \"- PEG Ratio: N/A\")\n",
        "    print(f\"- Insider Sentiment: {'Positive' if best['insider_sentiment'] > 0 else 'Neutral'}\")\n",
        "\n",
        "    # Create interactive visualizations\n",
        "    create_interactive_visualizations(top_5, stock_type)\n",
        "\n",
        "    # Ask user for technical indicators and plot price chart\n",
        "    print(f\"\\nTechnical Analysis for Top {stock_type_label}:\")\n",
        "    indicator_options = \"SMA, EMA, Bollinger, VWAP\" + (\", RSI\" if stock_type == \"penny\" else \"\")\n",
        "    indicators = input(f\"Select Indicators (comma-separated: {indicator_options}): \").split(\",\")\n",
        "    plot_price_chart(best['ticker'], best['company'], indicators, stock_type)\n",
        "\n",
        "def run_sentiment_analysis():\n",
        "    \"\"\"Run the comprehensive market sentiment analysis\"\"\"\n",
        "    print(\"🚀 Comprehensive Market Sentiment Analyzer\")\n",
        "    print(\"📊 Analyzing LSE and LSE Penny Stocks\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    analyzer = ComprehensiveMarketSentimentAnalyzer()\n",
        "\n",
        "    # Analyze all stocks and get top 15 recommendations\n",
        "    top_stocks, all_results = analyzer.analyze_top_stocks(top_n=15)\n",
        "\n",
        "    # Display recommendations\n",
        "    analyzer.display_recommendations(top_stocks, all_results)\n",
        "\n",
        "    # Generate detailed report\n",
        "    analyzer.generate_detailed_report(top_stocks)\n",
        "\n",
        "    # Create visualizations\n",
        "    analyzer.create_visualizations(all_results)\n",
        "\n",
        "    # Save to CSV\n",
        "    analyzer.save_recommendations(top_stocks)\n",
        "\n",
        "    print(f\"\\n✅ Sentiment analysis complete! {len(top_stocks)} top recommendations generated.\")\n",
        "    print(\"💡 Remember: This is sentiment analysis - always do your own research before investing!\")\n",
        "\n",
        "def main():\n",
        "    print(\"Comprehensive Stock Screener with Technical & Sentiment Analysis\\n\")\n",
        "\n",
        "    # Let user choose analysis type\n",
        "    print(\"Select analysis type:\")\n",
        "    print(\"1. Fundamental Stock Screening\")\n",
        "    print(\"2. Comprehensive Market Sentiment Analysis\")\n",
        "    print(\"3. LSE Fundamental Data Collection\")\n",
        "    print(\"4. All Analyses (Full Suite)\")\n",
        "    print(\"5. Train Machine Learning Model\")\n",
        "    print(\"6. Run Analysis with ML Predictions\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1-6): \").strip()\n",
        "\n",
        "\n",
        "\n",
        "    # choice = input(\"Enter your choice (1, 2, 3, or 4): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Let user choose stock type\n",
        "        print(\"\\nSelect stock type to screen:\")\n",
        "        print(\"1. Regular LSE Stocks\")\n",
        "        print(\"2. Penny Stocks\")\n",
        "\n",
        "        stock_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "        if stock_choice == \"1\":\n",
        "            stock_type = \"regular\"\n",
        "            print(\"\\nScreening regular LSE stocks based on:\")\n",
        "            print(\"- Fundamental metrics (Profitability, Valuation, etc.)\")\n",
        "            print(\"- News sentiment analysis\")\n",
        "            print(\"- Technical indicators\")\n",
        "        elif stock_choice == \"2\":\n",
        "            stock_type = \"penny\"\n",
        "            print(\"\\nScreening penny stocks based on:\")\n",
        "            print(\"- Price (<$5) and Volume (>500K)\")\n",
        "            print(\"- Fundamental metrics (adjusted for penny stocks)\")\n",
        "            print(\"- News sentiment from penny stock sources\")\n",
        "            print(\"- Technical indicators\")\n",
        "        else:\n",
        "            print(\"Invalid choice. Defaulting to regular LSE stocks.\")\n",
        "            stock_type = \"regular\"\n",
        "\n",
        "        try:\n",
        "            # Screen stocks based on fundamentals\n",
        "            screened_stocks = screen_stocks(stock_type)\n",
        "\n",
        "            # Display results with news sentiment and technical analysis\n",
        "            display_top_candidates_with_news(screened_stocks, stock_type)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during screening: {e}\")\n",
        "            print(\"This might be due to temporary Yahoo Finance API issues or data limitations.\")\n",
        "\n",
        "        print(\"\\nAnalysis Complete\")\n",
        "\n",
        "        if stock_type == \"penny\":\n",
        "            print(\"Note: Penny stocks are highly volatile - always conduct thorough research before investing\")\n",
        "        else:\n",
        "            print(\"Note: Combined fundamental, sentiment, and technical analysis provides comprehensive insights\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        run_sentiment_analysis()\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Run LSE fundamental data collection\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LSE FUNDAMENTAL DATA COLLECTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        run_lse_fundamental_collection()\n",
        "\n",
        "    elif choice == \"4\":\n",
        "        # Run all analyses\n",
        "        print(\"\\nRunning comprehensive analysis suite...\")\n",
        "\n",
        "        # First run LSE fundamental data collection\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LSE FUNDAMENTAL DATA COLLECTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        lse_data, value_candidates = run_lse_fundamental_collection()\n",
        "\n",
        "        # Then run fundamental screening\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FUNDAMENTAL STOCK SCREENING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for stock_type in [\"regular\", \"penny\"]:\n",
        "            print(f\"\\nScreening {stock_type} stocks...\")\n",
        "            try:\n",
        "                screened_stocks = screen_stocks(stock_type)\n",
        "                if screened_stocks:\n",
        "                    display_top_candidates_with_news(screened_stocks[:3], stock_type)  # Show top 3 only\n",
        "                else:\n",
        "                    print(f\"No {stock_type} stocks found matching criteria.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error screening {stock_type} stocks: {e}\")\n",
        "\n",
        "        # Finally run sentiment analysis\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE MARKET SENTIMENT ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        run_sentiment_analysis()\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice. Defaulting to fundamental stock screening.\")\n",
        "        # Run fundamental screening with regular stocks\n",
        "        stock_type = \"regular\"\n",
        "        try:\n",
        "            screened_stocks = screen_stocks(stock_type)\n",
        "            display_top_candidates_with_news(screened_stocks, stock_type)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during screening: {e}\")\n",
        "    # Add ML training option\n",
        "    if choice == \"5\":\n",
        "        print(\"\\nTraining Machine Learning Model...\")\n",
        "\n",
        "        # Collect training data\n",
        "        fundamental_data, sentiment_data, price_data = collect_training_data(days=180)\n",
        "\n",
        "        # Initialize and train model\n",
        "        model = StockPredictionModel()\n",
        "        X, y, tickers = model.prepare_training_data(fundamental_data, sentiment_data, price_data)\n",
        "        trained_model = model.train_models(X, y)\n",
        "\n",
        "        # Show feature importance\n",
        "        importance = model.feature_importance()\n",
        "        if importance is not None:\n",
        "            print(\"\\nFeature Importance:\")\n",
        "            print(importance.to_string(index=False))\n",
        "\n",
        "        # Save model\n",
        "        model.save_model(\"stock_prediction_model.pkl\")\n",
        "        print(\"Model saved as stock_prediction_model.pkl\")\n",
        "\n",
        "    # Add ML prediction option\n",
        "    elif choice == \"6\":\n",
        "        print(\"\\nRunning Analysis with ML Predictions...\")\n",
        "\n",
        "        # Load trained model\n",
        "        try:\n",
        "            model = StockPredictionModel().load_model(\"stock_prediction_model.pkl\")\n",
        "            print(\"ML model loaded successfully\")\n",
        "        except:\n",
        "            print(\"No trained model found. Please train a model first (option 5).\")\n",
        "            return\n",
        "\n",
        "        # Run sentiment analysis\n",
        "        analyzer = ComprehensiveMarketSentimentAnalyzer()\n",
        "        top_stocks, all_results = analyzer.analyze_top_stocks(top_n=15)\n",
        "\n",
        "        # Add ML predictions\n",
        "        top_stocks_with_ml = add_ml_predictions_to_analysis(top_stocks, model)\n",
        "\n",
        "        # Display with ML predictions\n",
        "        analyzer.display_recommendations = lambda x, y: display_recommendations_with_ml(analyzer, x, y)\n",
        "        analyzer.display_recommendations(top_stocks_with_ml, all_results)\n",
        "\n",
        "        # Generate detailed report\n",
        "        analyzer.generate_detailed_report(top_stocks_with_ml)\n",
        "\n",
        "        # Save to CSV\n",
        "        # analyzer.save_recommendations(top_stocks_with_ml, \"stock_recommendations_with_ml.csv\")\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages if not already installed\n",
        "    required_packages = {\n",
        "        'yfinance': 'yfinance',\n",
        "        'feedparser': 'feedparser',\n",
        "        'nltk': 'nltk',\n",
        "        'fake_useragent': 'fake-useragent',\n",
        "        'plotly': 'plotly',\n",
        "        'textblob': 'textblob',\n",
        "        'seaborn': 'seaborn',\n",
        "        'tqdm': 'tqdm'\n",
        "    }\n",
        "\n",
        "    for package, install_name in required_packages.items():\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            import subprocess\n",
        "            subprocess.run(['pip', 'install', install_name], check=True)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "U3vXOQCTF3JI",
        "outputId": "f428b445-b074-4605-d253-beee3e372e6e"
      },
      "id": "U3vXOQCTF3JI",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Comprehensive Stock Screener with Technical & Sentiment Analysis\n",
            "\n",
            "Select analysis type:\n",
            "1. Fundamental Stock Screening\n",
            "2. Comprehensive Market Sentiment Analysis\n",
            "3. LSE Fundamental Data Collection\n",
            "4. All Analyses (Full Suite)\n",
            "5. Train Machine Learning Model\n",
            "6. Run Analysis with ML Predictions\n",
            "Enter your choice (1-6): 5\n",
            "Invalid choice. Defaulting to fundamental stock screening.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FNXF.L: No price data available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FSGF.L: No price data available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BRMS.L: No price data available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MEGPM.L: No price data available\n",
            "Processing 15/26: VTYV.L\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "VTYV.L: No price data available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "BKSB.L: No price data available\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ERROR:yfinance:HTTP Error 404: \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ASOS.L: No price data available\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "| Ticker   | Company            |   Fundamental |   News | OpM    |   D/E |   News Count |\n",
            "+==========+====================+===============+========+========+=======+==============+\n",
            "| W7L.L    | Warpaint London... |          0.42 |   0.16 | 100.0% |  0.09 |            5 |\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "| FRP.L    | Frp Advisory Gr... |          0.4  |   0.28 | 100.0% |  0.03 |            9 |\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "| POLR.L   | Polar Capital H... |          0.4  |   0.31 | 100.0% |  0    |            4 |\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "| FDEV.L   | Frontier Develo... |          0.4  |   0.2  | 96.1%  |  0.02 |            6 |\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "| LSL.L    | LSL Property Se... |          0.39 |   0.33 | 100.0% |  0.01 |           10 |\n",
            "+----------+--------------------+---------------+--------+--------+-------+--------------+\n",
            "\n",
            "Top Stock: Warpaint London PLC (W7L.L)\n",
            "Composite Score: 0.42\n",
            "Average News Sentiment: 0.16\n",
            "\n",
            "Recent News Headlines:\n",
            "- [Negative] Warpaint London (LON:W7L) Reaches New 52-Week Low - Time to Sell? - MarketBeat (Google News)\n",
            "- [Neutral] Warpaint London PLC's (LON:W7L) Share Price Could Signal Some Risk - simplywall.st (Google News)\n",
            "- [Positive] Warpaint London PLC Successfully Passes All AGM Resolutions - TipRanks (Google News)\n",
            "- [Neutral] Warpaint London (LON:W7L) Trading Down 2.3% - Should You Sell? - MarketBeat (Google News)\n",
            "- [Positive] Warpaint London PLC's (LON:W7L) Stock's On An Uptrend: Are Strong Financials Guiding The Market? - simplywall.st (Google News)\n",
            "\n",
            "Key Metrics:\n",
            "- Operating Margin: 100.0%\n",
            "- Debt-to-Equity: 0.09\n",
            "- Price-to-Book: 0.00\n",
            "- P/E Ratio: 1.0\n",
            "- PEG Ratio: 0.0\n",
            "- Insider Sentiment: Neutral\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<html>\n",
              "<head><meta charset=\"utf-8\" /></head>\n",
              "<body>\n",
              "    <div>            <script src=\"https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_SVG\"></script><script type=\"text/javascript\">if (window.MathJax && window.MathJax.Hub && window.MathJax.Hub.Config) {window.MathJax.Hub.Config({SVG: {font: \"STIX-Web\"}});}</script>                <script type=\"text/javascript\">window.PlotlyConfig = {MathJaxConfig: 'local'};</script>\n",
              "        <script charset=\"utf-8\" src=\"https://cdn.plot.ly/plotly-2.35.2.min.js\"></script>                <div id=\"dfe639f5-723b-4910-8b47-25b6b8e68009\" class=\"plotly-graph-div\" style=\"height:900px; width:100%;\"></div>            <script type=\"text/javascript\">                                    window.PLOTLYENV=window.PLOTLYENV || {};                                    if (document.getElementById(\"dfe639f5-723b-4910-8b47-25b6b8e68009\")) {                    Plotly.newPlot(                        \"dfe639f5-723b-4910-8b47-25b6b8e68009\",                        [{\"marker\":{\"color\":\"blue\"},\"name\":\"Warpaint London PLC\",\"x\":[\"Operating Margin\",\"Debt-to-Equity\",\"Price-to-Book\",\"P\\u002fE Ratio\",\"PEG Ratio\"],\"y\":[100,0.08632596685082873,0.004270931250364097,1,0],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"green\"},\"name\":\"Frp Advisory Group Plc\",\"x\":[\"Operating Margin\",\"Debt-to-Equity\",\"Price-to-Book\",\"P\\u002fE Ratio\",\"PEG Ratio\"],\"y\":[100,0.03139126067302863,0.004144307780830106,0.9677779,0],\"type\":\"bar\",\"xaxis\":\"x\",\"yaxis\":\"y\"},{\"marker\":{\"color\":\"indianred\"},\"name\":\"Price-to-Book\",\"x\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"y\":[0.004270931250364097,0.004144307780830106,0.004401899014450026,0.008445178937295841,0.004411111472168754],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"lightsalmon\"},\"name\":\"P\\u002fE Ratio\",\"x\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"y\":[1,0.9677779,1,1,0.9117647],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"crimson\"},\"name\":\"PEG Ratio\",\"x\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"y\":[0,0,0,0,0],\"type\":\"bar\",\"xaxis\":\"x3\",\"yaxis\":\"y3\"},{\"marker\":{\"color\":\"darkgreen\"},\"name\":\"Operating Margin (%)\",\"x\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"y\":[100,100,100,96.06,100],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":\"darkblue\"},\"name\":\"Debt-to-Equity\",\"x\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"y\":[0.08632596685082873,0.03139126067302863,0,0.02018000565040158,0.010961066292528937],\"type\":\"bar\",\"xaxis\":\"x4\",\"yaxis\":\"y4\"},{\"marker\":{\"color\":[0.41790583305772033,0.40206658330173023,0.4006602848521675,0.3954527779706747,0.3896185849793311],\"colorbar\":{\"title\":{\"text\":\"Composite Score\"}},\"colorscale\":[[0.0,\"#440154\"],[0.1111111111111111,\"#482878\"],[0.2222222222222222,\"#3e4989\"],[0.3333333333333333,\"#31688e\"],[0.4444444444444444,\"#26828e\"],[0.5555555555555556,\"#1f9e89\"],[0.6666666666666666,\"#35b779\"],[0.7777777777777778,\"#6ece58\"],[0.8888888888888888,\"#b5de2b\"],[1.0,\"#fde725\"]],\"showscale\":true,\"size\":12},\"mode\":\"markers+text\",\"name\":\"Score vs Sentiment\",\"text\":[\"Warpaint London PLC\",\"Frp Advisory Group Plc\",\"Polar Capital Holdings plc\",\"Frontier Developments plc\",\"LSL Property Services Plc\"],\"textposition\":\"top center\",\"x\":[0.41790583305772033,0.40206658330173023,0.4006602848521675,0.3954527779706747,0.3896185849793311],\"y\":[0.1608,0.2775888888888889,0.30622499999999997,0.19733333333333333,0.33313000000000004],\"type\":\"scatter\",\"xaxis\":\"x2\",\"yaxis\":\"y2\"}],                        {\"template\":{\"data\":{\"barpolar\":[{\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"barpolar\"}],\"bar\":[{\"error_x\":{\"color\":\"#2a3f5f\"},\"error_y\":{\"color\":\"#2a3f5f\"},\"marker\":{\"line\":{\"color\":\"white\",\"width\":0.5},\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"bar\"}],\"carpet\":[{\"aaxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"baxis\":{\"endlinecolor\":\"#2a3f5f\",\"gridcolor\":\"#C8D4E3\",\"linecolor\":\"#C8D4E3\",\"minorgridcolor\":\"#C8D4E3\",\"startlinecolor\":\"#2a3f5f\"},\"type\":\"carpet\"}],\"choropleth\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"choropleth\"}],\"contourcarpet\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"contourcarpet\"}],\"contour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"contour\"}],\"heatmapgl\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmapgl\"}],\"heatmap\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"heatmap\"}],\"histogram2dcontour\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2dcontour\"}],\"histogram2d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"histogram2d\"}],\"histogram\":[{\"marker\":{\"pattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2}},\"type\":\"histogram\"}],\"mesh3d\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"type\":\"mesh3d\"}],\"parcoords\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"parcoords\"}],\"pie\":[{\"automargin\":true,\"type\":\"pie\"}],\"scatter3d\":[{\"line\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatter3d\"}],\"scattercarpet\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattercarpet\"}],\"scattergeo\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergeo\"}],\"scattergl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattergl\"}],\"scattermapbox\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scattermapbox\"}],\"scatterpolargl\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolargl\"}],\"scatterpolar\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterpolar\"}],\"scatter\":[{\"fillpattern\":{\"fillmode\":\"overlay\",\"size\":10,\"solidity\":0.2},\"type\":\"scatter\"}],\"scatterternary\":[{\"marker\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"type\":\"scatterternary\"}],\"surface\":[{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"},\"colorscale\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"type\":\"surface\"}],\"table\":[{\"cells\":{\"fill\":{\"color\":\"#EBF0F8\"},\"line\":{\"color\":\"white\"}},\"header\":{\"fill\":{\"color\":\"#C8D4E3\"},\"line\":{\"color\":\"white\"}},\"type\":\"table\"}]},\"layout\":{\"annotationdefaults\":{\"arrowcolor\":\"#2a3f5f\",\"arrowhead\":0,\"arrowwidth\":1},\"autotypenumbers\":\"strict\",\"coloraxis\":{\"colorbar\":{\"outlinewidth\":0,\"ticks\":\"\"}},\"colorscale\":{\"diverging\":[[0,\"#8e0152\"],[0.1,\"#c51b7d\"],[0.2,\"#de77ae\"],[0.3,\"#f1b6da\"],[0.4,\"#fde0ef\"],[0.5,\"#f7f7f7\"],[0.6,\"#e6f5d0\"],[0.7,\"#b8e186\"],[0.8,\"#7fbc41\"],[0.9,\"#4d9221\"],[1,\"#276419\"]],\"sequential\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]],\"sequentialminus\":[[0.0,\"#0d0887\"],[0.1111111111111111,\"#46039f\"],[0.2222222222222222,\"#7201a8\"],[0.3333333333333333,\"#9c179e\"],[0.4444444444444444,\"#bd3786\"],[0.5555555555555556,\"#d8576b\"],[0.6666666666666666,\"#ed7953\"],[0.7777777777777778,\"#fb9f3a\"],[0.8888888888888888,\"#fdca26\"],[1.0,\"#f0f921\"]]},\"colorway\":[\"#636efa\",\"#EF553B\",\"#00cc96\",\"#ab63fa\",\"#FFA15A\",\"#19d3f3\",\"#FF6692\",\"#B6E880\",\"#FF97FF\",\"#FECB52\"],\"font\":{\"color\":\"#2a3f5f\"},\"geo\":{\"bgcolor\":\"white\",\"lakecolor\":\"white\",\"landcolor\":\"white\",\"showlakes\":true,\"showland\":true,\"subunitcolor\":\"#C8D4E3\"},\"hoverlabel\":{\"align\":\"left\"},\"hovermode\":\"closest\",\"mapbox\":{\"style\":\"light\"},\"paper_bgcolor\":\"white\",\"plot_bgcolor\":\"white\",\"polar\":{\"angularaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"radialaxis\":{\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\"}},\"scene\":{\"xaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"yaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"},\"zaxis\":{\"backgroundcolor\":\"white\",\"gridcolor\":\"#DFE8F3\",\"gridwidth\":2,\"linecolor\":\"#EBF0F8\",\"showbackground\":true,\"ticks\":\"\",\"zerolinecolor\":\"#EBF0F8\"}},\"shapedefaults\":{\"line\":{\"color\":\"#2a3f5f\"}},\"ternary\":{\"aaxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"baxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"},\"bgcolor\":\"white\",\"caxis\":{\"gridcolor\":\"#DFE8F3\",\"linecolor\":\"#A2B1C6\",\"ticks\":\"\"}},\"title\":{\"x\":0.05},\"xaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2},\"yaxis\":{\"automargin\":true,\"gridcolor\":\"#EBF0F8\",\"linecolor\":\"#EBF0F8\",\"ticks\":\"\",\"title\":{\"standoff\":15},\"zerolinecolor\":\"#EBF0F8\",\"zerolinewidth\":2}}},\"xaxis\":{\"anchor\":\"y\",\"domain\":[0.0,0.425]},\"yaxis\":{\"anchor\":\"x\",\"domain\":[0.575,1.0],\"title\":{\"text\":\"Value\"}},\"xaxis2\":{\"anchor\":\"y2\",\"domain\":[0.575,1.0],\"title\":{\"text\":\"Composite Score\"}},\"yaxis2\":{\"anchor\":\"x2\",\"domain\":[0.575,1.0],\"title\":{\"text\":\"News Sentiment\"}},\"xaxis3\":{\"anchor\":\"y3\",\"domain\":[0.0,0.425]},\"yaxis3\":{\"anchor\":\"x3\",\"domain\":[0.0,0.425],\"title\":{\"text\":\"Ratio Value\"}},\"xaxis4\":{\"anchor\":\"y4\",\"domain\":[0.575,1.0]},\"yaxis4\":{\"anchor\":\"x4\",\"domain\":[0.0,0.425],\"title\":{\"text\":\"Percentage\\u002fValue\"}},\"annotations\":[{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Fundamental Metrics Comparison\",\"x\":0.2125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Composite Scores vs News Sentiment\",\"x\":0.7875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":1.0,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Valuation Ratios\",\"x\":0.2125,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.425,\"yanchor\":\"bottom\",\"yref\":\"paper\"},{\"font\":{\"size\":12},\"showarrow\":false,\"text\":\"Profitability & Financial Health\",\"x\":0.7875,\"xanchor\":\"center\",\"xref\":\"paper\",\"y\":0.425,\"yanchor\":\"bottom\",\"yref\":\"paper\"}],\"title\":{\"text\":\"Top LSE Stocks Analysis Dashboard\"},\"height\":900,\"showlegend\":true,\"hovermode\":\"closest\",\"barmode\":\"group\"},                        {\"responsive\": true}                    ).then(function(){\n",
              "                            \n",
              "var gd = document.getElementById('dfe639f5-723b-4910-8b47-25b6b8e68009');\n",
              "var x = new MutationObserver(function (mutations, observer) {{\n",
              "        var display = window.getComputedStyle(gd).display;\n",
              "        if (!display || display === 'none') {{\n",
              "            console.log([gd, 'removed!']);\n",
              "            Plotly.purge(gd);\n",
              "            observer.disconnect();\n",
              "        }}\n",
              "}});\n",
              "\n",
              "// Listen for the removal of the full notebook cells\n",
              "var notebookContainer = gd.closest('#notebook-container');\n",
              "if (notebookContainer) {{\n",
              "    x.observe(notebookContainer, {childList: true});\n",
              "}}\n",
              "\n",
              "// Listen for the clearing of the current output cell\n",
              "var outputEl = gd.closest('.output');\n",
              "if (outputEl) {{\n",
              "    x.observe(outputEl, {childList: true});\n",
              "}}\n",
              "\n",
              "                        })                };                            </script>        </div>\n",
              "</body>\n",
              "</html>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Technical Analysis for Top Stock:\n",
            "Select Indicators (comma-separated: SMA, EMA, Bollinger, VWAP): vwap\n",
            "\n",
            "An error occurred during screening: The truth value of a Series is ambiguous. Use a.empty, a.bool(), a.item(), a.any() or a.all().\n",
            "\n",
            "Training Machine Learning Model...\n",
            "Collecting historical data for training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\rCollecting historical data:   0%|          | 0/73 [00:00<?, ?it/s]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$FNXF.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$FNXF.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$FNXF.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:   3%|▎         | 2/73 [00:14<09:46,  8.26s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$FSGF.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$FSGF.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$FSGF.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:   7%|▋         | 5/73 [00:45<12:41, 11.20s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$BRMS.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BRMS.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BRMS.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  10%|▉         | 7/73 [01:00<10:36,  9.65s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error fetching from London Stock Exchange: HTTPSConnectionPool(host='www.londonstockexchange.com', port=443): Read timed out. (read timeout=15)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting historical data:  18%|█▊        | 13/73 [02:39<13:45, 13.76s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$MEGPM.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$MEGPM.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$MEGPM.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  19%|█▉        | 14/73 [02:40<09:47,  9.96s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$VTYV.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$VTYV.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$VTYV.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  22%|██▏       | 16/73 [02:54<08:29,  8.94s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$BKSB.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BKSB.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BKSB.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  25%|██▍       | 18/73 [03:08<07:37,  8.32s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$ASOS.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$ASOS.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$ASOS.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  44%|████▍     | 32/73 [05:44<08:09, 11.94s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$SCEU.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SCEU.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SCEU.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  55%|█████▍    | 40/73 [07:18<06:58, 12.68s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$PHEG.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$PHEG.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$PHEG.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  56%|█████▌    | 41/73 [07:19<04:57,  9.31s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$SPRSP.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SPRSP.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SPRSP.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  63%|██████▎   | 46/73 [08:11<05:00, 11.15s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$SOSS.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SOSS.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$SOSS.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  64%|██████▍   | 47/73 [08:12<03:32,  8.18s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$BELLB.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BELLB.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BELLB.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  66%|██████▌   | 48/73 [08:13<02:31,  6.08s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$THW.ASE: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$THW.ASE: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$THW.ASE: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  71%|███████   | 52/73 [08:57<03:48, 10.87s/it]ERROR:yfinance:$SDIS.L: possibly delisted; no price data found  (period=1d)\n",
            "Collecting historical data:  79%|███████▉  | 58/73 [10:00<02:56, 11.75s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$BIGB.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BIGB.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$BIGB.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  82%|████████▏ | 60/73 [10:14<02:08,  9.86s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$HAYS.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$HAYS.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$HAYS.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  89%|████████▉ | 65/73 [11:06<01:29, 11.24s/it]ERROR:yfinance:$BHL.L: possibly delisted; no price data found  (period=1d)\n",
            "Collecting historical data:  92%|█████████▏| 67/73 [11:31<01:11, 11.98s/it]ERROR:yfinance:$PATP.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$PATP.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$PATP.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  96%|█████████▌| 70/73 [12:01<00:34, 11.46s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$TERNT.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$TERNT.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$TERNT.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data:  97%|█████████▋| 71/73 [12:02<00:16,  8.40s/it]ERROR:yfinance:$ALLA.L: possibly delisted; no price data found  (period=180d)\n",
            "ERROR:yfinance:$ALLA.L: possibly delisted; no price data found  (period=1d)\n",
            "ERROR:yfinance:$ALLA.L: possibly delisted; no price data found  (period=5d)\n",
            "Collecting historical data:  99%|█████████▊| 72/73 [12:03<00:06,  6.05s/it]ERROR:yfinance:HTTP Error 404: \n",
            "ERROR:yfinance:$EMEL.L: possibly delisted; no price data found  (period=180d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$EMEL.L: possibly delisted; no price data found  (period=1d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "ERROR:yfinance:$EMEL.L: possibly delisted; no price data found  (period=5d) (Yahoo error = \"No data found, symbol may be delisted\")\n",
            "Collecting historical data: 100%|██████████| 73/73 [12:04<00:00,  9.93s/it]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "Input X contains infinity or a value too large for dtype('float64').",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1950788615.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2126\u001b[0m             \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'pip'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'install'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minstall_name\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2128\u001b[0;31m     \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/tmp/ipython-input-1950788615.py\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m   2064\u001b[0m         \u001b[0;31m# Initialize and train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2065\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStockPredictionModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2066\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtickers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_training_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfundamental_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentiment_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprice_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2067\u001b[0m         \u001b[0mtrained_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_models\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2068\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-1950788615.py\u001b[0m in \u001b[0;36mprepare_training_data\u001b[0;34m(self, fundamental_data, sentiment_data, price_data)\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;31m# Handle missing values\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mX_imputed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimputer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Scale features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/_set_output.py\u001b[0m in \u001b[0;36mwrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    317\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mwraps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 319\u001b[0;31m         \u001b[0mdata_to_wrap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    320\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_to_wrap\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    321\u001b[0m             \u001b[0;31m# only wrap the first output for cross decomposition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mfit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    916\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m             \u001b[0;31m# fit method of arity 1 (unsupervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 918\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    919\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m             \u001b[0;31m# fit method of arity 2 (supervised transformation)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/base.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1387\u001b[0m                 )\n\u001b[1;32m   1388\u001b[0m             ):\n\u001b[0;32m-> 1389\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfit_method\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m    432\u001b[0m             \u001b[0mFitted\u001b[0m \u001b[0mestimator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    433\u001b[0m         \"\"\"\n\u001b[0;32m--> 434\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    436\u001b[0m         \u001b[0;31m# default fill_value is 0 for numerical input and \"missing_value\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mnew_ve\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mve\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0min_fit\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/impute/_base.py\u001b[0m in \u001b[0;36m_validate_input\u001b[0;34m(self, X, in_fit)\u001b[0m\n\u001b[1;32m    342\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    343\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 344\u001b[0;31m             X = validate_data(\n\u001b[0m\u001b[1;32m    345\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m                 \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mvalidate_data\u001b[0;34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[0m\n\u001b[1;32m   2942\u001b[0m             \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2943\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2944\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"X\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2945\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mno_val_X\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mno_val_y\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2946\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_y\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mcheck_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m   1105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1106\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mensure_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1107\u001b[0;31m             _assert_all_finite(\n\u001b[0m\u001b[1;32m   1108\u001b[0m                 \u001b[0marray\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m                 \u001b[0minput_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    118\u001b[0m         \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m     _assert_all_finite_element_wise(\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mxp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mxp\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite_element_wise\u001b[0;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[0m\n\u001b[1;32m    167\u001b[0m                 \u001b[0;34m\"#estimators-that-handle-nan-values\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    168\u001b[0m             )\n\u001b[0;32m--> 169\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: Input X contains infinity or a value too large for dtype('float64')."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import yfinance as yf\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from fake_useragent import UserAgent\n",
        "import nltk\n",
        "from nltk.sentiment import SentimentIntensityAnalyzer\n",
        "import time\n",
        "from datetime import datetime, timedelta\n",
        "import feedparser\n",
        "from tabulate import tabulate\n",
        "import plotly.graph_objects as go\n",
        "from plotly.subplots import make_subplots\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from textblob import TextBlob\n",
        "import warnings\n",
        "from tqdm import tqdm\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Initialize sentiment analyzer\n",
        "nltk.download('vader_lexicon', quiet=True)\n",
        "sia = SentimentIntensityAnalyzer()\n",
        "\n",
        "# Initialize user agent generator\n",
        "ua = UserAgent()\n",
        "\n",
        "# Combined stock databases\n",
        "LSE_TICKERS = {\n",
        "    'FNXF.L': 'Fonix Mobile PLC',\n",
        "    'GGP.L': 'Greatland Resources Ltd',\n",
        "    'FSGF.L': 'Foresight Group Holdings Ltd',\n",
        "    'ACSO.L': 'Accesso Technology Group PLC',\n",
        "    'POLR.L': 'Polar Capital Holdings plc',\n",
        "    'BRMS.L': 'Braemar PLC',\n",
        "    'GFM.L': 'Griffin Mining',\n",
        "    'FRP.L': 'Frp Advisory Group Plc',\n",
        "    'EZJ.L': 'EasyJet PLC',\n",
        "    'W7L.L': 'Warpaint London PLC',\n",
        "    'BEG.L': 'Begbies Traynor Group PLC',\n",
        "    'SRAD.L': 'Stelrad Group PLC',\n",
        "    'LSL.L': 'LSL Property Services Plc',\n",
        "    'MEGPM.L': 'ME Group International PLC',\n",
        "    'VTYV.L': 'Vistry Group PLC',\n",
        "    'MUT.L': 'Murray Income Trust',\n",
        "    'BKSB.L': 'Beeks Financial Cloud Group PLC',\n",
        "    'FDEV.L': 'Frontier Developments plc',\n",
        "    'ASOS.L': 'ASOS PLC',\n",
        "    'LGEN.L': 'Legal & General Group PLC',\n",
        "    'MNG.L': 'M&G Plc',\n",
        "    'BP.L': 'BP PLC',\n",
        "    'PHNX.L': 'Phoenix Group Holdings PLC',\n",
        "    'VOD.L': 'Vodafone Group PLC',\n",
        "    'CNA.L': 'Centrica PLC',\n",
        "    'GLEN.L': 'Glencore PLC'\n",
        "}\n",
        "\n",
        "PENNY_STOCKS = {\n",
        "    'NTVO.L': 'Nativo Resources PLC',\n",
        "    'SOU.L': 'Sound Energy PLC',\n",
        "    'GENL.L': 'Genel Energy Plc',\n",
        "    'IQE.L': 'IQE PLC',\n",
        "    'LDG.L': 'Logistics Development Group PLC',\n",
        "    'VAST.L': 'Vast Resources PLC',\n",
        "    'SCEU.L': 'Surface Transforms Plc',\n",
        "    'PDL.L': 'Petra Diamonds Ltd',\n",
        "    'ITM.L': 'ITM Power',\n",
        "    'TLW.L': 'Tullow Oil PLC',\n",
        "    'ARB.L': 'Argo Blockchain PLC',\n",
        "    'AVCT.L': 'Avacta Group PLC',\n",
        "    'TBLD.L': 'Tinybuild Inc',\n",
        "    'EOG.L': 'Europa Oil & Gas Holdings',\n",
        "    'PHEG.L': 'PowerHouse Energy Group plc',\n",
        "    'SPRSP.L': 'Springfield Properties PLC',\n",
        "    'AML.L': 'Aston Martin Lagonda Global Holdings PLC',\n",
        "    'VANL.L': 'Van Elle Holdings PLC',\n",
        "    'GMS.L': 'Gulf Marine Services PLC',\n",
        "    'RNO.L': 'Renold',\n",
        "    'SOSS.L': 'Sosandar PLC',\n",
        "    'BELLB.L': 'Belluscura PLC',\n",
        "    'THW.ASE': 'Daniel Thwaites PLC',\n",
        "    'KOD.L': 'Kodal Minerals',\n",
        "    'CSSG.L': 'Croma Security Solutions Group Plc',\n",
        "    'VOD.L': 'Vodafone Group PLC',\n",
        "    'SDIS.L': 'SDI Group PLC',\n",
        "    'RWS.L': 'RWS Holdings PLC',\n",
        "    'JD.L': 'JD Sports Fashion PLC',\n",
        "    'STAF.L': 'Staffline Group Plc',\n",
        "    'BEG.L': 'Begbies Traynor Group PLC',\n",
        "    'ENQ.L': 'Enquest Plc',\n",
        "    'BIGB.L': 'Big Technologies PLC',\n",
        "    'DEBS.L': 'Boohoo Group PLC',\n",
        "    'HAYS.L': 'Hays Plc',\n",
        "    'ULTP.L': 'Ultimate Products PLC',\n",
        "    'RMR.L': 'Rome Resources PLC',\n",
        "    'RKH.L': 'Rockhopper Exploration',\n",
        "    'CAV.L': 'Cavendish Financial PLC',\n",
        "    'BHL.L': 'Bradda Head Holdings Ltd',\n",
        "    'EUA.L': 'Eurasia Mining',\n",
        "    'PATP.L': 'Panthera Resources PLC',\n",
        "    'JOG.L': 'Jersey Oil and Gas PLC',\n",
        "    'MSMN.L': 'Mosman Oil and Gas Ltd',\n",
        "    'TERNT.L': 'Tern PLC',\n",
        "    'ALLA.L': 'Atlantic Lithium Ltd',\n",
        "    'EMEL.L': 'Empyrean Energy'\n",
        "}\n",
        "\n",
        "# Combined financial metrics thresholds\n",
        "METRIC_THRESHOLDS = {\n",
        "    'regular': {\n",
        "        'price': {'max': float('inf')},\n",
        "        'volume': {'min': 0},\n",
        "        'operating_margin': {'ideal': 0.1, 'acceptable': 0.05},\n",
        "        'debt_to_equity': {'ideal': 0.5, 'acceptable': 1.0},\n",
        "        'price_to_book': {'ideal': 2.0, 'acceptable': 3.0},\n",
        "        'pe_ratio': {'ideal': 15.0, 'acceptable': 20.0},\n",
        "        'peg_ratio': {'ideal': 1.5, 'acceptable': 2.0},\n",
        "    },\n",
        "    'penny': {\n",
        "        'price': {'max': 5.00},\n",
        "        'volume': {'min': 500000},\n",
        "        'operating_margin': {'ideal': 0.05, 'acceptable': 0.01},\n",
        "        'debt_to_equity': {'ideal': 1.0, 'acceptable': 2.0},\n",
        "        'price_to_book': {'ideal': 1.5, 'acceptable': 3.0},\n",
        "        'pe_ratio': {'ideal': 15.0, 'acceptable': 30.0},\n",
        "        'peg_ratio': {'ideal': 1.5, 'acceptable': 2.5},\n",
        "    }\n",
        "}\n",
        "\n",
        "# Combined news sources\n",
        "NEWS_SOURCES = [\n",
        "    {\"name\": \"Vox Markets\", \"url\": \"https://www.voxmarkets.co.uk/latest-news/\", \"parser\": \"vox\"},\n",
        "    {\"name\": \"Yahoo Finance UK\", \"url\": \"https://uk.finance.yahoo.com/topic/latest-news\", \"parser\": \"yahoo\"},\n",
        "    {\"name\": \"MarketWatch UK\", \"url\": \"https://www.marketwatch.com/latest-news\", \"parser\": \"marketwatch\"},\n",
        "    {\"name\": \"Financial Times\", \"url\": \"https://www.ft.com/markets\", \"parser\": \"ft\"},\n",
        "    {\"name\": \"Reuters UK\", \"url\": \"https://www.reuters.com/business/finance\", \"parser\": \"reuters\"},\n",
        "    {\"name\": \"London Stock Exchange\", \"url\": \"https://www.londonstockexchange.com/news\", \"parser\": \"lse\"},\n",
        "    {\"name\": \"Google News\", \"url\": \"https://news.google.com/rss/search?q={query}+stock\", \"parser\": \"google\"},\n",
        "    {\"name\": 'PennyStocks.com', \"url\": 'https://pennystocks.com/news/', \"parser\": 'pennystocks'},\n",
        "    {\"name\": 'InvestorPlace Penny Stocks', \"url\": 'https://investorplace.com/category/penny-stocks/', \"parser\": 'investorplace'}\n",
        "]\n",
        "\n",
        "def get_full_lse_tickers():\n",
        "    \"\"\"Get comprehensive list of LSE tickers including AIM\"\"\"\n",
        "    # This combines FTSE 350 + popular AIM stocks\n",
        "    return [\n",
        "        'III', 'AAL', 'ABDN', 'ADM', 'AHT', 'ANTO', 'AUTO', 'AV', 'AVV', 'AZN',\n",
        "        'BA', 'BARC', 'BDEV', 'BKG', 'BLND', 'BNZL', 'BP', 'BRBY', 'BT-A', 'CCH',\n",
        "        'CCL', 'CNA', 'CPG', 'CRDA', 'CRH', 'CTEC', 'DCC', 'DGE', 'DLG', 'EXPN',\n",
        "        'FERG', 'FLTR', 'FRES', 'GLEN', 'GSK', 'HLMA', 'HSBA', 'IAG', 'IHG', 'IMB',\n",
        "        'INF', 'ITRK', 'JD', 'JET', 'KGF', 'LAND', 'LGEN', 'LLOY', 'LSE', 'MKS',\n",
        "        'MNDI', 'MRO', 'NG', 'NXT', 'OCDO', 'PRU', 'PSN', 'PSON', 'REL', 'RIO',\n",
        "        'RMV', 'RR', 'RTO', 'SBRY', 'SDR', 'SGE', 'SGRO', 'SHP', 'SKG', 'SMDS',\n",
        "        'SMIN', 'SN', 'SPX', 'SSE', 'STAN', 'STJ', 'SVT', 'TSCO', 'ULVR', 'UU',\n",
        "        'VOD', 'WEIR', 'WPP', 'WTB', 'BME', 'CWR', 'FDM', 'GAW', 'HAS', 'LRE',\n",
        "        'MCG', 'PAGE', 'RM', 'SMT', 'TEP', 'TRST', 'VCT', 'BOO', 'CINE', 'DARK',\n",
        "        'FOUR', 'GNS', 'HWDN', 'IQE', 'LXI', 'OXIG', 'PFC', 'QLT', 'RR.', 'SVS',\n",
        "        'TPT', 'VANL', 'WJG', 'APAX', 'ATST', 'BBGI', 'CEY', 'DNLM', 'ECOR',\n",
        "        'FDM', 'GFTU', 'HMSO', 'ITV', 'JMAT', 'KWS', 'LWDB', 'MTO', 'NCC', 'PDL',\n",
        "        'QUIZ', 'RBD', 'SOHO', 'TND', 'UKW', 'VNET', 'WBI', 'XPS', 'AML', 'BOWL',\n",
        "        'CAML', 'DSCV', 'ECEL', 'FIPP', 'GAMA', 'HRI', 'INSP', 'JET2', 'KBT',\n",
        "        'LIO', 'MERC', 'NIOX', 'PALM', 'QRT', 'RKH', 'SIS', 'TSTG', 'UTG', 'VOG',\n",
        "        'WPS', 'XAR', 'YOOM'\n",
        "    ]\n",
        "\n",
        "def get_all_lse_metrics(tickers):\n",
        "    \"\"\"\n",
        "    Get fundamental metrics for all LSE stocks with minimal filtering\n",
        "    Only excludes stocks with completely missing data\n",
        "    \"\"\"\n",
        "    results = []\n",
        "\n",
        "    for ticker in tqdm(tickers, desc=\"Processing stocks\"):\n",
        "        yf_ticker = f\"{ticker}.L\"\n",
        "        try:\n",
        "            stock = yf.Ticker(yf_ticker)\n",
        "            time.sleep(0.1)  # Very light rate limiting\n",
        "\n",
        "            info = stock.info\n",
        "            if not info:\n",
        "                continue\n",
        "\n",
        "            try:\n",
        "                # Get basic metrics (all optional)\n",
        "                metrics = {\n",
        "                    'Ticker': ticker,\n",
        "                    'Name': info.get('shortName', ticker),\n",
        "                    'Sector': info.get('sector', 'N/A'),\n",
        "                    'Price (GBp)': info.get('currentPrice', None),\n",
        "                    'Market Cap (£M)': info.get('marketCap', None),\n",
        "                    'P/E': info.get('trailingPE', None) or info.get('forwardPE', None),\n",
        "                    'P/B': info.get('priceToBook', None),\n",
        "                    'Debt/Equity': info.get('debtToEquity', None),\n",
        "                    'Operating Margin %': info.get('operatingMargins', None),\n",
        "                    'Dividend Yield %': info.get('dividendYield', None),\n",
        "                    'Beta': info.get('beta', None)\n",
        "                }\n",
        "\n",
        "                # Convert price to pence if available\n",
        "                if metrics['Price (GBp)'] is not None:\n",
        "                    metrics['Price (GBp)'] *= 1\n",
        "\n",
        "                # Convert market cap to millions if available\n",
        "                if metrics['Market Cap (£M)'] is not None:\n",
        "                    metrics['Market Cap (£M)'] /= 1e6\n",
        "\n",
        "                # Convert percentages if available\n",
        "                if metrics['Operating Margin %'] is not None:\n",
        "                    metrics['Operating Margin %'] *= 100\n",
        "                if metrics['Dividend Yield %'] is not None:\n",
        "                    metrics['Dividend Yield %'] *= 100\n",
        "\n",
        "                results.append(metrics)\n",
        "\n",
        "            except Exception as e:\n",
        "                continue\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    return pd.DataFrame(results)\n",
        "\n",
        "def run_lse_fundamental_collection():\n",
        "    \"\"\"Run the LSE fundamental data collection\"\"\"\n",
        "    print(f\"LSE Fundamental Data Collector - {datetime.now().strftime('%d %b %Y')}\\n\")\n",
        "\n",
        "    # Get all possible tickers\n",
        "    lse_tickers = get_full_lse_tickers()\n",
        "    print(f\"Loaded {len(lse_tickers)} LSE tickers\\n\")\n",
        "\n",
        "    # Get all available metrics with no filtering\n",
        "    df = get_all_lse_metrics(lse_tickers)\n",
        "\n",
        "    if not df.empty:\n",
        "        # Clean and format the data\n",
        "        numeric_cols = ['Price (GBp)', 'Market Cap (£M)', 'P/E', 'P/B',\n",
        "                       'Debt/Equity', 'Operating Margin %', 'Dividend Yield %', 'Beta']\n",
        "\n",
        "        for col in numeric_cols:\n",
        "            if col in df.columns:\n",
        "                df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Sort by market cap descending\n",
        "        df = df.sort_values('Market Cap (£M)', ascending=False)\n",
        "\n",
        "        print(f\"Found {len(df)} stocks with available data:\")\n",
        "        pd.set_option('display.max_rows', None)\n",
        "        pd.set_option('display.max_columns', None)\n",
        "        pd.set_option('display.width', 1000)\n",
        "        pd.set_option('display.float_format', '{:.2f}'.format)\n",
        "        print(df)\n",
        "\n",
        "        # Save full dataset\n",
        "        filename = f\"lse_full_metrics_{datetime.now().strftime('%Y%m%d')}.csv\"\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"\\nFull results saved to {filename}\")\n",
        "\n",
        "        # Generate potential value candidates\n",
        "        print(\"\\nPotential value candidates (manual review recommended):\")\n",
        "        value_candidates = df[\n",
        "            (df['P/E'].notna()) &\n",
        "            (df['P/B'].notna()) &\n",
        "            (df['P/E'] < 30) &\n",
        "            (df['P/B'] < 3)\n",
        "        ].sort_values(['P/E', 'P/B'])\n",
        "\n",
        "        print(value_candidates.head(50))\n",
        "\n",
        "        return df, value_candidates\n",
        "    else:\n",
        "        print(\"No data retrieved - please check your connection or try again later.\")\n",
        "        return None, None\n",
        "\n",
        "class ComprehensiveMarketSentimentAnalyzer:\n",
        "    def __init__(self):\n",
        "        self.sentiment_data = []\n",
        "        self.last_update = None\n",
        "        self.setup_market_data()\n",
        "\n",
        "    def setup_market_data(self):\n",
        "        \"\"\"Define stocks from multiple markets including LSE\"\"\"\n",
        "        self.market_stocks = {\n",
        "            # UK Stocks (LSE - London Stock Exchange)\n",
        "            'LSE': list(LSE_TICKERS.keys()),\n",
        "            'LSE Penny': list(PENNY_STOCKS.keys())\n",
        "        }\n",
        "\n",
        "        # All stocks combined for analysis\n",
        "        self.all_stocks = []\n",
        "        for market in self.market_stocks.values():\n",
        "            self.all_stocks.extend(market)\n",
        "\n",
        "    def get_stock_data(self, symbol, period='1d', interval='5m'):\n",
        "        \"\"\"Get real-time stock data with enhanced error handling\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(symbol)\n",
        "            data = stock.history(period=period, interval=interval)\n",
        "\n",
        "            if data.empty or len(data) < 2:\n",
        "                # Try with longer period for less liquid stocks\n",
        "                data = stock.history(period='5d', interval='15m')\n",
        "\n",
        "            return data if not data.empty else None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching data for {symbol}: {e}\")\n",
        "            return None\n",
        "\n",
        "    def calculate_technical_indicators(self, data):\n",
        "        \"\"\"Calculate additional technical indicators\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return {}\n",
        "\n",
        "        # RSI (Relative Strength Index)\n",
        "        delta = data['Close'].diff()\n",
        "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
        "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
        "        rs = gain / loss\n",
        "        rsi = 100 - (100 / (1 + rs))\n",
        "\n",
        "        # Moving Averages\n",
        "        sma_20 = data['Close'].rolling(window=20).mean()\n",
        "        sma_50 = data['Close'].rolling(window=50).mean()\n",
        "\n",
        "        # MACD\n",
        "        exp12 = data['Close'].ewm(span=12).mean()\n",
        "        exp26 = data['Close'].ewm(span=26).mean()\n",
        "        macd = exp12 - exp26\n",
        "        signal = macd.ewm(span=9).mean()\n",
        "\n",
        "        return {\n",
        "            'rsi': rsi.iloc[-1] if not pd.isna(rsi.iloc[-1]) else 50,\n",
        "            'sma_20': sma_20.iloc[-1],\n",
        "            'sma_50': sma_50.iloc[-1],\n",
        "            'macd': macd.iloc[-1],\n",
        "            'signal': signal.iloc[-1]\n",
        "        }\n",
        "\n",
        "    def calculate_volume_sentiment(self, data):\n",
        "        \"\"\"Calculate sentiment based on volume patterns\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return 0\n",
        "\n",
        "        current_volume = data['Volume'].iloc[-1]\n",
        "        avg_volume_5d = data['Volume'].rolling(window=5).mean().iloc[-1]\n",
        "        avg_volume_20d = data['Volume'].rolling(window=20).mean().iloc[-1]\n",
        "\n",
        "        if avg_volume_20d == 0:  # Avoid division by zero\n",
        "            return 0\n",
        "\n",
        "        volume_ratio = current_volume / avg_volume_20d\n",
        "\n",
        "        if volume_ratio > 2.0:\n",
        "            return 1.0  # Very high volume\n",
        "        elif volume_ratio > 1.5:\n",
        "            return 0.7  # High volume\n",
        "        elif volume_ratio > 1.2:\n",
        "            return 0.4  # Moderate volume\n",
        "        elif volume_ratio > 0.8:\n",
        "            return 0.1  # Normal volume\n",
        "        else:\n",
        "            return -0.2  # Low volume\n",
        "\n",
        "    def calculate_price_sentiment(self, data, technicals):\n",
        "        \"\"\"Calculate sentiment based on price movement and technicals\"\"\"\n",
        "        if data is None or len(data) < 5:\n",
        "            return 0\n",
        "\n",
        "        current_price = data['Close'].iloc[-1]\n",
        "        open_price = data['Open'].iloc[-1]\n",
        "        prev_close = data['Close'].iloc[-2]\n",
        "\n",
        "        # Price changes\n",
        "        daily_change_pct = ((current_price - open_price) / open_price) * 100\n",
        "        prev_change_pct = ((current_price - prev_close) / prev_close) * 100\n",
        "\n",
        "        # Technical analysis sentiment\n",
        "        rsi_sentiment = 0\n",
        "        if technicals['rsi'] > 70:\n",
        "            rsi_sentiment = -0.3  # Overbought\n",
        "        elif technicals['rsi'] < 30:\n",
        "            rsi_sentiment = 0.3   # Oversold\n",
        "\n",
        "        ma_sentiment = 0\n",
        "        if current_price > technicals['sma_20'] > technicals['sma_50']:\n",
        "            ma_sentiment = 0.4  # Strong uptrend\n",
        "        elif current_price < technicals['sma_20'] < technicals['sma_50']:\n",
        "            ma_sentiment = -0.4  # Strong downtrend\n",
        "\n",
        "        macd_sentiment = 0.2 if technicals['macd'] > technicals['signal'] else -0.2\n",
        "\n",
        "        # Combined sentiment\n",
        "        price_sentiment = (\n",
        "            (min(1, daily_change_pct / 5) * 0.4) +\n",
        "            (min(1, prev_change_pct / 3) * 0.3) +\n",
        "            rsi_sentiment +\n",
        "            ma_sentiment +\n",
        "            macd_sentiment\n",
        "        )\n",
        "\n",
        "        return max(-1, min(1, price_sentiment))\n",
        "\n",
        "    def get_market_news(self, symbol):\n",
        "        \"\"\"Fetch market news with multiple sources\"\"\"\n",
        "        try:\n",
        "            # Remove .L suffix for LSE stocks for news search\n",
        "            search_symbol = symbol.replace('.L', '') if '.L' in symbol else symbol\n",
        "\n",
        "            # Get company name\n",
        "            if symbol in LSE_TICKERS:\n",
        "                company_name = LSE_TICKERS[symbol]\n",
        "            elif symbol in PENNY_STOCKS:\n",
        "                company_name = PENNY_STOCKS[symbol]\n",
        "            else:\n",
        "                company_name = search_symbol\n",
        "\n",
        "            # Fetch actual news using the existing function\n",
        "            articles = fetch_news_for_ticker(symbol, company_name, \"regular\")\n",
        "\n",
        "            if not articles:\n",
        "                # Fallback to simulated news\n",
        "                news_sources = [\n",
        "                    f\"Financial Times: {search_symbol} shows strong momentum\",\n",
        "                    f\"Bloomberg: Analysts positive on {search_symbol} outlook\",\n",
        "                    f\"Reuters: {search_symbol} earnings exceed expectations\",\n",
        "                    f\"MarketWatch: {search_symbol} trading volume spikes\",\n",
        "                    f\"CNBC: Institutional investors accumulating {search_symbol}\",\n",
        "                    f\"WSJ: {search_symbol} announces strategic initiatives\",\n",
        "                    f\"Investing.com: Technical breakout for {search_symbol}\",\n",
        "                    f\"SeekingAlpha: {search_symbol} undervalued relative to peers\"\n",
        "                ]\n",
        "\n",
        "                articles = [{'title': np.random.choice(news_sources),\n",
        "                            'source': 'Financial News',\n",
        "                            'publishedAt': datetime.now().isoformat()}]\n",
        "\n",
        "            return articles\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"News error for {symbol}: {e}\")\n",
        "            return []\n",
        "\n",
        "    def analyze_news_sentiment(self, articles):\n",
        "        \"\"\"Analyze sentiment from news headlines\"\"\"\n",
        "        if not articles:\n",
        "            return 0\n",
        "\n",
        "        sentiments = []\n",
        "        for article in articles:\n",
        "            title = article.get('title', '')\n",
        "            if title:\n",
        "                try:\n",
        "                    analysis = TextBlob(title)\n",
        "                    sentiment = analysis.sentiment.polarity\n",
        "                    # Weight by source credibility\n",
        "                    source = article.get('source', '').lower()\n",
        "                    if 'bloomberg' in source or 'financial times' in source:\n",
        "                        sentiment *= 1.2\n",
        "                    elif 'reuters' in source:\n",
        "                        sentiment *= 1.1\n",
        "                    sentiments.append(sentiment)\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        return np.mean(sentiments) if sentiments else 0\n",
        "\n",
        "    def get_social_sentiment(self, symbol):\n",
        "        \"\"\"Enhanced social media sentiment analysis\"\"\"\n",
        "        # Simulate social media data from multiple platforms\n",
        "        platforms = {\n",
        "            'twitter': np.random.uniform(-0.4, 0.4),\n",
        "            'reddit': np.random.uniform(-0.3, 0.3),\n",
        "            'stocktwits': np.random.uniform(-0.5, 0.5),\n",
        "            'forums': np.random.uniform(-0.2, 0.2)\n",
        "        }\n",
        "\n",
        "        # Volume simulation\n",
        "        volumes = {\n",
        "            'twitter': np.random.randint(100, 1000),\n",
        "            'reddit': np.random.randint(50, 500),\n",
        "            'stocktwits': np.random.randint(200, 800),\n",
        "            'forums': np.random.randint(30, 300)\n",
        "        }\n",
        "\n",
        "        # Weighted average by platform volume\n",
        "        total_volume = sum(volumes.values())\n",
        "        weighted_sentiment = sum(platforms[p] * (volumes[p] / total_volume) for p in platforms)\n",
        "\n",
        "        return weighted_sentiment\n",
        "\n",
        "    def get_options_flow(self, symbol):\n",
        "        \"\"\"Simulate options flow data\"\"\"\n",
        "        # More realistic options simulation\n",
        "        if '.L' in symbol:  # LSE stocks have different options activity\n",
        "            call_volume = np.random.randint(50, 300)\n",
        "            put_volume = np.random.randint(40, 250)\n",
        "        else:  # US stocks\n",
        "            call_volume = np.random.randint(100, 2000)\n",
        "            put_volume = np.random.randint(80, 1500)\n",
        "\n",
        "        put_call_ratio = put_volume / (call_volume + 1e-6)\n",
        "\n",
        "        if put_call_ratio < 0.7:\n",
        "            return 0.4  # Very bullish\n",
        "        elif put_call_ratio < 0.9:\n",
        "            return 0.2  # Bullish\n",
        "        elif put_call_ratio > 1.3:\n",
        "            return -0.4  # Very bearish\n",
        "        elif put_call_ratio > 1.1:\n",
        "            return -0.2  # Bearish\n",
        "        else:\n",
        "            return 0  # Neutral\n",
        "\n",
        "    def calculate_market_cap_sentiment(self, symbol, current_price):\n",
        "        \"\"\"Consider market cap in sentiment analysis\"\"\"\n",
        "        try:\n",
        "            stock = yf.Ticker(symbol)\n",
        "            info = stock.info\n",
        "            market_cap = info.get('marketCap', 0)\n",
        "\n",
        "            if market_cap > 200e9:  # Large cap\n",
        "                return -0.1  # More stable, less volatile sentiment\n",
        "            elif market_cap > 10e9:  # Mid cap\n",
        "                return 0.0\n",
        "            else:  # Small cap\n",
        "                return 0.2  # More volatile, higher potential sentiment\n",
        "\n",
        "        except:\n",
        "            return 0.0\n",
        "\n",
        "    def calculate_composite_sentiment(self, symbol):\n",
        "        \"\"\"Calculate comprehensive market sentiment\"\"\"\n",
        "        stock_data = self.get_stock_data(symbol)\n",
        "\n",
        "        if stock_data is None or len(stock_data) < 5:\n",
        "            return None\n",
        "\n",
        "        technicals = self.calculate_technical_indicators(stock_data)\n",
        "        current_price = stock_data['Close'].iloc[-1]\n",
        "\n",
        "        # Get all sentiment components\n",
        "        volume_sentiment = self.calculate_volume_sentiment(stock_data)\n",
        "        price_sentiment = self.calculate_price_sentiment(stock_data, technicals)\n",
        "\n",
        "        news = self.get_market_news(symbol)\n",
        "        news_sentiment = self.analyze_news_sentiment(news)\n",
        "\n",
        "        social_sentiment = self.get_social_sentiment(symbol)\n",
        "        options_sentiment = self.get_options_flow(symbol)\n",
        "        market_cap_sentiment = self.calculate_market_cap_sentiment(symbol, current_price)\n",
        "\n",
        "        # Weighted composite sentiment (adjusted for different markets)\n",
        "        weights = {\n",
        "            'price': 0.30,\n",
        "            'volume': 0.20,\n",
        "            'news': 0.15,\n",
        "            'social': 0.10,\n",
        "            'options': 0.10,\n",
        "            'market_c': 0.05,\n",
        "            'technicals': 0.10\n",
        "        }\n",
        "\n",
        "        # Add technical sentiment from RSI and moving averages\n",
        "        technical_sentiment = (\n",
        "            (0.3 if technicals['rsi'] < 40 else -0.3 if technicals['rsi'] > 60 else 0) +\n",
        "            (0.2 if current_price > technicals['sma_20'] else -0.2)\n",
        "        )\n",
        "\n",
        "        composite = (\n",
        "            price_sentiment * weights['price'] +\n",
        "            volume_sentiment * weights['volume'] +\n",
        "            news_sentiment * weights['news'] +\n",
        "            social_sentiment * weights['social'] +\n",
        "            options_sentiment * weights['options'] +\n",
        "            market_cap_sentiment * weights['market_c'] +\n",
        "            # market_cap_sentiment * weights['market_cap'] +\n",
        "            technical_sentiment * weights['technicals']\n",
        "        )\n",
        "\n",
        "        # Determine signal strength\n",
        "        if composite > 0.3:\n",
        "            signal = \"STRONG BUY\"\n",
        "            confidence = \"HIGH\"\n",
        "        elif composite > 0.15:\n",
        "            signal = \"BUY\"\n",
        "            confidence = \"MEDIUM\"\n",
        "        elif composite > 0.05:\n",
        "            signal = \"MILD BUY\"\n",
        "            confidence = \"LOW\"\n",
        "        elif composite < -0.3:\n",
        "            signal = \"STRONG SELL\"\n",
        "            confidence = \"HIGH\"\n",
        "        elif composite < -0.15:\n",
        "            signal = \"SELL\"\n",
        "            confidence = \"MEDIUM\"\n",
        "        elif composite < -0.05:\n",
        "            signal = \"MILD SELL\"\n",
        "            confidence = \"LOW\"\n",
        "        else:\n",
        "            signal = \"NEUTRAL\"\n",
        "            confidence = \"LOW\"\n",
        "\n",
        "        result = {\n",
        "            'symbol': symbol,\n",
        "            'market': 'LSE' if '.L' in symbol else 'LSE Penny',\n",
        "            'timestamp': datetime.now(),\n",
        "            'composite_sentiment': composite,\n",
        "            'signal': signal,\n",
        "            'confidence': confidence,\n",
        "            'current_price': current_price,\n",
        "            'price_change': ((current_price - stock_data['Open'].iloc[-1]) / stock_data['Open'].iloc[-1]) * 100,\n",
        "            'volume': stock_data['Volume'].iloc[-1],\n",
        "            'rsi': technicals['rsi'],\n",
        "            'components': {\n",
        "                'price': price_sentiment,\n",
        "                'volume': volume_sentiment,\n",
        "                'news': news_sentiment,\n",
        "                'social': social_sentiment,\n",
        "                'options': options_sentiment\n",
        "            }\n",
        "        }\n",
        "\n",
        "        self.sentiment_data.append(result)\n",
        "        return result\n",
        "\n",
        "    def analyze_top_stocks(self, top_n=15):\n",
        "        \"\"\"Analyze and recommend top stocks\"\"\"\n",
        "        print(\"🔍 Analyzing market sentiment across all stocks...\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        results = []\n",
        "        total_stocks = len(self.all_stocks)\n",
        "\n",
        "        for i, symbol in enumerate(self.all_stocks, 1):\n",
        "            print(f\"Processing {i}/{total_stocks}: {symbol}\", end='\\r')\n",
        "            result = self.calculate_composite_sentiment(symbol)\n",
        "            if result:\n",
        "                results.append(result)\n",
        "            time.sleep(0.1)  # Be polite to API\n",
        "\n",
        "        print(\"\\n\" + \"=\" * 100)\n",
        "\n",
        "        # Sort by composite sentiment (highest first)\n",
        "        results.sort(key=lambda x: x['composite_sentiment'], reverse=True)\n",
        "\n",
        "        # Get top recommendations\n",
        "        top_stocks = results[:top_n]\n",
        "\n",
        "        return top_stocks, results\n",
        "\n",
        "    def display_recommendations(self, top_stocks, all_results):\n",
        "        \"\"\"Display top stock recommendations\"\"\"\n",
        "        print(f\"\\n🎯 TOP {len(top_stocks)} STOCK RECOMMENDATIONS\")\n",
        "        print(\"=\" * 120)\n",
        "        print(f\"{'Rank':<5} {'Symbol':<8} {'Market':<8} {'Price':<10} {'Change%':<8} {'Signal':<12} {'Confidence':<10} {'Sentiment':<10} {'RSI':<6}\")\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "        for i, stock in enumerate(top_stocks, 1):\n",
        "            change_color = '\\033[92m' if stock['price_change'] >= 0 else '\\033[91m'\n",
        "            signal_color = '\\033[92m' if 'BUY' in stock['signal'] else '\\033[91m' if 'SELL' in stock['signal'] else '\\033[93m'\n",
        "            rsi_color = '\\033[91m' if stock['rsi'] > 70 else '\\033[92m' if stock['rsi'] < 30 else '\\033[93m'\n",
        "\n",
        "            print(f\"{i:<5} {stock['symbol']:<8} {stock['market']:<8} \"\n",
        "                  f\"${stock['current_price']:<9.2f} \"\n",
        "                  f\"{change_color}{stock['price_change']:>+6.1f}%\\033[0m \"\n",
        "                  f\"{signal_color}{stock['signal']:<12}\\033[0m \"\n",
        "                  f\"{stock['confidence']:<10} \"\n",
        "                  f\"{stock['composite_sentiment']:>+8.3f} \"\n",
        "                  f\"{rsi_color}{stock['rsi']:>5.1f}\\033[0m\")\n",
        "\n",
        "        print(\"-\" * 120)\n",
        "\n",
        "        # Market distribution\n",
        "        market_counts = pd.Series([s['market'] for s in top_stocks]).value_counts()\n",
        "        print(f\"\\n📊 Market Distribution in Top {len(top_stocks)}:\")\n",
        "        for market, count in market_counts.items():\n",
        "            print(f\"  {market}: {count} stocks\")\n",
        "\n",
        "        # Signal distribution\n",
        "        signal_counts = pd.Series([s['signal'] for s in top_stocks]).value_counts()\n",
        "        print(f\"\\n🎯 Signal Distribution:\")\n",
        "        for signal, count in signal_counts.items():\n",
        "            print(f\"  {signal}: {count} stocks\")\n",
        "\n",
        "        return top_stocks\n",
        "\n",
        "    def generate_detailed_report(self, top_stocks):\n",
        "        \"\"\"Generate detailed analysis report\"\"\"\n",
        "        print(f\"\\n📈 DETAILED ANALYSIS REPORT\")\n",
        "        print(\"=\" * 100)\n",
        "\n",
        "        for i, stock in enumerate(top_stocks[:5], 1):  # Top 5 detailed analysis\n",
        "            print(f\"\\n{i}. {stock['symbol']} ({stock['market']}) - {stock['signal']}\")\n",
        "            print(f\"   Current Price: ${stock['current_price']:.2f}\")\n",
        "            print(f\"   Today's Change: {stock['price_change']:+.2f}%\")\n",
        "            print(f\"   Composite Sentiment: {stock['composite_sentiment']:+.3f}\")\n",
        "            print(f\"   RSI: {stock['rsi']:.1f}\")\n",
        "            print(f\"   Confidence: {stock['confidence']}\")\n",
        "            print(\"   Component Breakdown:\")\n",
        "            for comp, value in stock['components'].items():\n",
        "                print(f\"     - {comp.capitalize()}: {value:+.3f}\")\n",
        "\n",
        "    def create_visualizations(self, all_results):\n",
        "        \"\"\"Create comprehensive visualizations\"\"\"\n",
        "        if not all_results:\n",
        "            return\n",
        "\n",
        "        df = pd.DataFrame(all_results)\n",
        "\n",
        "        plt.figure(figsize=(20, 15))\n",
        "\n",
        "        # 1. Sentiment Distribution by Market\n",
        "        plt.subplot(2, 3, 1)\n",
        "        market_data = df.groupby('market')['composite_sentiment'].mean()\n",
        "        market_data.plot(kind='bar', color=['blue', 'green', 'orange', 'red'])\n",
        "        plt.title('Average Sentiment by Market')\n",
        "        plt.xticks(rotation=45)\n",
        "\n",
        "        # 2. Top 10 Stocks by Sentiment\n",
        "        plt.subplot(2, 3, 2)\n",
        "        top_10 = df.nlargest(10, 'composite_sentiment')\n",
        "        plt.barh(top_10['symbol'], top_10['composite_sentiment'],\n",
        "                color=['green' if x > 0 else 'red' for x in top_10['composite_sentiment']])\n",
        "        plt.title('Top 10 Stocks by Sentiment')\n",
        "        plt.xlabel('Sentiment Score')\n",
        "\n",
        "        # 3. RSI Distribution\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.hist(df['rsi'], bins=20, alpha=0.7, color='purple')\n",
        "        plt.axvline(30, color='green', linestyle='--', label='Oversold (30)')\n",
        "        plt.axvline(70, color='red', linestyle='--', label='Overbought (70)')\n",
        "        plt.title('RSI Distribution')\n",
        "        plt.legend()\n",
        "\n",
        "        # 4. Signal Distribution\n",
        "        plt.subplot(2, 3, 4)\n",
        "        signal_counts = df['signal'].value_counts()\n",
        "        colors = ['green' if 'BUY' in s else 'red' if 'SELL' in s else 'gray' for s in signal_counts.index]\n",
        "        signal_counts.plot(kind='pie', autopct='%1.1f%%', colors=colors)\n",
        "        plt.title('Overall Signal Distribution')\n",
        "\n",
        "        # 5. Price vs Sentiment Scatter\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.scatter(df['current_price'], df['composite_sentiment'], alpha=0.6)\n",
        "        plt.xlabel('Price ($)')\n",
        "        plt.ylabel('Sentiment Score')\n",
        "        plt.title('Price vs Sentiment')\n",
        "\n",
        "        # 6. Volume vs Sentiment\n",
        "        plt.subplot(2, 3, 6)\n",
        "        plt.scatter(np.log(df['volume'] + 1), df['composite_sentiment'], alpha=0.6)\n",
        "        plt.xlabel('Log Volume')\n",
        "        plt.ylabel('Sentiment Score')\n",
        "        plt.title('Trading Volume vs Sentiment')\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "    def save_recommendations(self, top_stocks, filename='stock_recommendations.csv'):\n",
        "        \"\"\"Save recommendations to CSV file\"\"\"\n",
        "        df = pd.DataFrame(top_stocks)\n",
        "\n",
        "        # Flatten components dictionary\n",
        "        components_df = pd.json_normalize(df['components'])\n",
        "        df = pd.concat([df.drop(['components', 'timestamp'], axis=1), components_df], axis=1)\n",
        "\n",
        "        df.to_csv(filename, index=False)\n",
        "        print(f\"\\n💾 Recommendations saved to {filename}\")\n",
        "\n",
        "def safe_divide(a, b):\n",
        "    \"\"\"Safe division function to handle division by zero\"\"\"\n",
        "    try:\n",
        "        return float(a) / float(b) if float(b) != 0 else 0\n",
        "    except (ValueError, TypeError):\n",
        "        return 0\n",
        "\n",
        "def convert_to_float(value):\n",
        "    \"\"\"Convert value to float safely\"\"\"\n",
        "    try:\n",
        "        return float(value)\n",
        "    except (ValueError, TypeError):\n",
        "        return np.nan\n",
        "\n",
        "def get_price_data(ticker, days=90):\n",
        "    \"\"\"Get historical price data for technical analysis\"\"\"\n",
        "    end_date = datetime.now()\n",
        "    start_date = end_date - timedelta(days=days)\n",
        "    try:\n",
        "        data = yf.download(ticker, start=start_date, end=end_date, progress=False)\n",
        "        return data\n",
        "    except Exception as e:\n",
        "        print(f\"Error downloading price data for {ticker}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "def add_technical_indicators(data, indicators, stock_type=\"regular\"):\n",
        "    \"\"\"Add selected technical indicators to price data\"\"\"\n",
        "    if data.empty:\n",
        "        return data\n",
        "\n",
        "    # Set window sizes based on stock type\n",
        "    sma_window = 10 if stock_type == \"penny\" else 20\n",
        "    ema_window = 10 if stock_type == \"penny\" else 20\n",
        "\n",
        "    for indicator in indicators:\n",
        "        indicator = indicator.strip().lower()\n",
        "        if indicator == \"sma\":\n",
        "            data[f'SMA_{sma_window}'] = data['Close'].rolling(window=sma_window).mean()\n",
        "        elif indicator == \"ema\":\n",
        "            data[f'EMA_{ema_window}'] = data['Close'].ewm(span=ema_window).mean()\n",
        "        elif indicator == \"bollinger\":\n",
        "            window = 10 if stock_type == \"penny\" else 20\n",
        "            data[f'SMA_{window}'] = data['Close'].rolling(window=window).mean()\n",
        "            data[f'STD_{window}'] = data['Close'].rolling(window=window).std()\n",
        "            data['BB_Upper'] = data[f'SMA_{window}'] + 2 * data[f'STD_{window}']\n",
        "            data['BB_Lower'] = data[f'SMA_{window}'] - 2 * data[f'STD_{window}']\n",
        "        elif indicator == \"vwap\":\n",
        "            if 'Volume' in data and data['Volume'].sum() > 0:\n",
        "                data['VWAP'] = (data['Close'] * data['Volume']).cumsum() / data['Volume'].cumsum()\n",
        "        elif indicator == \"rsi\":\n",
        "            delta = data['Close'].diff()\n",
        "            gain = delta.where(delta > 0, 0)\n",
        "            loss = -delta.where(delta < 0, 0)\n",
        "            avg_gain = gain.rolling(window=14).mean()\n",
        "            avg_loss = loss.rolling(window=14).mean()\n",
        "            rs = avg_gain / avg_loss\n",
        "            data['RSI'] = 100 - (100 / (1 + rs))\n",
        "    return data\n",
        "\n",
        "def plot_price_chart(ticker, company, indicators, stock_type=\"regular\"):\n",
        "    \"\"\"Create interactive price chart with selected indicators\"\"\"\n",
        "    days = 90 if stock_type == \"penny\" else 180\n",
        "    data = get_price_data(ticker, days)\n",
        "    if data.empty:\n",
        "        print(f\"\\nNo price data available for {ticker}\")\n",
        "        return\n",
        "\n",
        "    data = add_technical_indicators(data, indicators, stock_type)\n",
        "\n",
        "    fig = go.Figure()\n",
        "\n",
        "    # Candlestick chart\n",
        "    fig.add_trace(go.Candlestick(\n",
        "        x=data.index,\n",
        "        open=data['Open'],\n",
        "        high=data['High'],\n",
        "        low=data['Low'],\n",
        "        close=data['Close'],\n",
        "        name='Price'\n",
        "    ))\n",
        "\n",
        "    # Add selected indicators\n",
        "    for indicator in indicators:\n",
        "        indicator = indicator.strip().lower()\n",
        "        window = 10 if stock_type == \"penny\" else 20\n",
        "\n",
        "        if indicator == \"sma\" and f'SMA_{window}' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data[f'SMA_{window}'],\n",
        "                mode='lines',\n",
        "                name=f'SMA ({window})',\n",
        "                line=dict(color='blue', width=2)\n",
        "            ))\n",
        "        elif indicator == \"ema\" and f'EMA_{window}' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data[f'EMA_{window}'],\n",
        "                mode='lines',\n",
        "                name=f'EMA ({window})',\n",
        "                line=dict(color='green', width=2)\n",
        "            ))\n",
        "        elif indicator == \"bollinger\" and all(col in data for col in ['BB_Upper', 'BB_Lower']):\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['BB_Upper'],\n",
        "                mode='lines',\n",
        "                name='BB Upper',\n",
        "                line=dict(color='red', width=1)\n",
        "            ))\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['BB_Lower'],\n",
        "                mode='lines',\n",
        "                name='BB Lower',\n",
        "                line=dict(color='blue', width=2),\n",
        "                fill='tonexty',\n",
        "                fillcolor='rgba(255,0,0,0.1)'\n",
        "            ))\n",
        "        elif indicator == \"vwap\" and 'VWAP' in data:\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['VWAP'],\n",
        "                mode='lines',\n",
        "                name='VWAP',\n",
        "                line=dict(color='purple', width=2)\n",
        "            ))\n",
        "        elif indicator == \"rsi\" and 'RSI' in data:\n",
        "            # Create RSI subplot\n",
        "            fig.add_trace(go.Scatter(\n",
        "                x=data.index,\n",
        "                y=data['RSI'],\n",
        "                mode='lines',\n",
        "                name='RSI',\n",
        "                line=dict(color='orange', width=2),\n",
        "                secondary_y=True\n",
        "            ))\n",
        "            # Add RSI reference lines\n",
        "            fig.add_hline(y=70, line_dash=\"dot\", line_color=\"red\",\n",
        "                         annotation_text=\"Overbought\", annotation_position=\"top right\",\n",
        "                         secondary_y=True)\n",
        "            fig.add_hline(y=30, line_dash=\"dot\", line_color=\"green\",\n",
        "                         annotation_text=\"Oversold\", annotation_position=\"bottom right\",\n",
        "                         secondary_y=True)\n",
        "\n",
        "    title_suffix = \"Penny Stock\" if stock_type == \"penny\" else \"Stock\"\n",
        "    fig.update_layout(\n",
        "        title=f\"{company} ({ticker}) - {title_suffix} Price Chart\",\n",
        "        xaxis_title=\"Date\",\n",
        "        yaxis_title=\"Price (GBp)\" if \".L\" in ticker else \"Price ($)\",\n",
        "        xaxis_rangeslider_visible=False,\n",
        "        height=600,\n",
        "        template=\"plotly_white\"\n",
        "    )\n",
        "\n",
        "    if 'rsi' in [i.strip().lower() for i in indicators]:\n",
        "        fig.update_layout(\n",
        "            yaxis2=dict(\n",
        "                title=\"RSI\",\n",
        "                overlaying=\"y\",\n",
        "                side=\"right\",\n",
        "                range=[0, 100]\n",
        "            )\n",
        "        )\n",
        "\n",
        "    fig.show()\n",
        "\n",
        "def get_insider_transactions(ticker):\n",
        "    \"\"\"Scrape insider transactions data from MarketWatch\"\"\"\n",
        "    try:\n",
        "        base_ticker = ticker.replace('.L', '') if '.L' in ticker else ticker\n",
        "        url = f\"https://www.marketwatch.com/investing/stock/{base_ticker}/insideractions\"\n",
        "        headers = {'User-Agent': ua.random}\n",
        "        response = requests.get(url, headers=headers, timeout=10)\n",
        "        soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "        transactions = []\n",
        "        for row in soup.select('table.insider-actions tr')[1:6]:  # Get last 5 transactions\n",
        "            cells = row.find_all('td')\n",
        "            if len(cells) >= 5:\n",
        "                transactions.append({\n",
        "                    'date': cells[0].text.strip(),\n",
        "                    'insider': cells[1].text.strip(),\n",
        "                    'position': cells[2].text.strip(),\n",
        "                    'transaction': cells[3].text.strip(),\n",
        "                    'shares': cells[4].text.strip()\n",
        "                })\n",
        "\n",
        "        buy_count = sum(1 for t in transactions if 'Buy' in t['transaction'])\n",
        "        sell_count = sum(1 for t in transactions if 'Sell' in t['transaction'])\n",
        "        sentiment = (buy_count - sell_count) / len(transactions) if transactions else 0\n",
        "\n",
        "        return {\n",
        "            'insider_transactions': transactions[:3],  # Return latest 3\n",
        "            'insider_sentiment': sentiment\n",
        "        }\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting insider transactions for {ticker}: {e}\")\n",
        "        return {\n",
        "            'insider_transactions': 'N/A',\n",
        "            'insider_sentiment': 0\n",
        "        }\n",
        "\n",
        "def calculate_metric_score(value, metric, stock_type=\"regular\"):\n",
        "    \"\"\"Calculate score for each metric (0-1) with safer division\"\"\"\n",
        "    try:\n",
        "        float_value = convert_to_float(value)\n",
        "        if pd.isna(float_value):\n",
        "            return 0\n",
        "\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        ideal = thresholds[metric]['ideal']\n",
        "        acceptable = thresholds[metric]['acceptable']\n",
        "\n",
        "        if metric in ['debt_to_equity', 'price_to_book', 'pe_ratio', 'peg_ratio']:\n",
        "            # For metrics where lower is better\n",
        "            if float_value <= ideal:\n",
        "                return 1\n",
        "            elif float_value <= acceptable:\n",
        "                return 0.5 * (1 + safe_divide((acceptable - float_value), (acceptable - ideal)))\n",
        "            else:\n",
        "                return max(0, 0.5 * safe_divide(acceptable, float_value))\n",
        "        else:\n",
        "            # For metrics where higher is better (operating margin)\n",
        "            if float_value >= ideal:\n",
        "                return 1\n",
        "            elif float_value >= acceptable:\n",
        "                return 0.5 * (1 + safe_divide((float_value - acceptable), (ideal - acceptable)))\n",
        "            else:\n",
        "                return max(0, 0.5 * safe_divide(float_value, acceptable))\n",
        "    except Exception as e:\n",
        "        print(f\"Error calculating score for {metric}: {e}\")\n",
        "        return 0\n",
        "\n",
        "def get_fundamentals(ticker, stock_type=\"regular\"):\n",
        "    \"\"\"Get fundamental data with improved error handling\"\"\"\n",
        "    try:\n",
        "        stock = yf.Ticker(ticker)\n",
        "        info = stock.info\n",
        "\n",
        "        # Get current price with fallbacks\n",
        "        current_price = convert_to_float(\n",
        "            info.get('currentPrice',\n",
        "                   info.get('regularMarketPrice',\n",
        "                           info.get('ask', np.nan)))\n",
        "        )\n",
        "\n",
        "        if pd.isna(current_price):\n",
        "            print(f\"{ticker}: No price data available\")\n",
        "            return None\n",
        "\n",
        "        # Get volume data with error handling\n",
        "        try:\n",
        "            hist = stock.history(period=\"1mo\")\n",
        "            avg_volume = hist['Volume'].mean() if not hist.empty else 0\n",
        "        except Exception as e:\n",
        "            print(f\"{ticker}: Error getting volume data - {e}\")\n",
        "            avg_volume = 0\n",
        "\n",
        "        # Skip if not matching stock type criteria\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        if (current_price > thresholds['price']['max'] or\n",
        "            avg_volume < thresholds['volume']['min']):\n",
        "            return None\n",
        "\n",
        "        # Get company name from appropriate database\n",
        "        if stock_type == \"penny\":\n",
        "            company_name = PENNY_STOCKS.get(ticker, info.get('shortName', ticker))\n",
        "        else:\n",
        "            company_name = LSE_TICKERS.get(ticker, info.get('shortName', ticker))\n",
        "\n",
        "        fundamentals = {\n",
        "            'ticker': ticker,\n",
        "            'company': company_name,\n",
        "            'price': current_price,\n",
        "            'volume': avg_volume,\n",
        "            'operating_margin': convert_to_float(info.get('operatingMargins', np.nan)),\n",
        "            'debt_to_equity': convert_to_float(info.get('debtToEquity', np.nan)),\n",
        "            'price_to_book': convert_to_float(info.get('priceToBook', np.nan)),\n",
        "            'pe_ratio': convert_to_float(info.get('trailingPE', np.nan)),\n",
        "            'peg_ratio': convert_to_float(info.get('pegRatio', np.nan)),\n",
        "            'market_cap': convert_to_float(info.get('marketCap', np.nan)),\n",
        "            'sector': info.get('sector', 'N/A'),\n",
        "            'industry': info.get('industry', 'N/A'),\n",
        "            'beta': convert_to_float(info.get('beta', np.nan)),\n",
        "            'stock_type': stock_type\n",
        "        }\n",
        "\n",
        "        # Get insider transactions with error handling\n",
        "        try:\n",
        "            insider_data = get_insider_transactions(ticker)\n",
        "            fundamentals.update(insider_data)\n",
        "        except Exception as e:\n",
        "            print(f\"Error getting insider data for {ticker}: {e}\")\n",
        "            fundamentals.update({\n",
        "                'insider_transactions': 'N/A',\n",
        "                'insider_sentiment': 0\n",
        "            })\n",
        "\n",
        "        return fundamentals\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting fundamentals for {ticker}: {e}\")\n",
        "        return None\n",
        "\n",
        "def fetch_news_for_ticker(ticker, company_name, stock_type=\"regular\"):\n",
        "    \"\"\"Fetch news articles for a specific ticker from all sources\"\"\"\n",
        "    all_articles = []\n",
        "\n",
        "    for source in NEWS_SOURCES:\n",
        "        try:\n",
        "            if source[\"parser\"] == \"google\":\n",
        "                query = f\"{ticker.replace('.L','')}+{company_name.replace(' ','+')}\"\n",
        "                if stock_type == \"penny\":\n",
        "                    query += \"+penny+stock\"\n",
        "                url = source[\"url\"].format(query=query)\n",
        "            else:\n",
        "                url = source[\"url\"]\n",
        "\n",
        "            headers = {'User-Agent': ua.random}\n",
        "\n",
        "            if source[\"parser\"] == \"google\":\n",
        "                feed = feedparser.parse(url)\n",
        "                for entry in feed.entries[:10]:\n",
        "                    all_articles.append({\n",
        "                        'title': entry.title,\n",
        "                        'link': entry.link,\n",
        "                        'source': 'Google News',\n",
        "                        'time': entry.published\n",
        "                    })\n",
        "                continue\n",
        "\n",
        "            response = requests.get(url, headers=headers, timeout=15)\n",
        "            soup = BeautifulSoup(response.text, 'html.parser')\n",
        "\n",
        "            if source[\"parser\"] == \"vox\":\n",
        "                for item in soup.select('div.post-content-wrap')[:10]:\n",
        "                    title_elem = item.select_one('h2.entry-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.entry-date').text if item.select_one('time.entry-date') else \"\"\n",
        "                        })\n",
        "\n",
        "            elif source[\"parser\"] == \"yahoo\":\n",
        "                for item in soup.select('h3[class*=\"Mb(5px)\"]')[:10]:\n",
        "                    link = item.find('a')['href']\n",
        "                    if not link.startswith('http'):\n",
        "                        link = f\"https://uk.finance.yahoo.com{link}\"\n",
        "                    all_articles.append({\n",
        "                        'title': item.text.strip(),\n",
        "                        'link': link,\n",
        "                        'source': source['name']\n",
        "                    })\n",
        "\n",
        "            elif source[\"parser\"] == \"pennystocks\":\n",
        "                for item in soup.select('div.td-module-container')[:10]:\n",
        "                    title_elem = item.select_one('h3.entry-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.entry-date')['datetime'] if item.select_one('time.entry-date') else \"\"\n",
        "                        })\n",
        "\n",
        "            elif source[\"parser\"] == \"investorplace\":\n",
        "                for item in soup.select('div.article-content')[:10]:\n",
        "                    title_elem = item.select_one('h4.article-title a')\n",
        "                    if title_elem:\n",
        "                        all_articles.append({\n",
        "                            'title': title_elem.text.strip(),\n",
        "                            'link': title_elem['href'],\n",
        "                            'source': source['name'],\n",
        "                            'time': item.select_one('time.article-time')['datetime'] if item.select_one('time.article-time') else \"\"\n",
        "                        })\n",
        "\n",
        "            time.sleep(1)  # Be polite with requests\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error fetching from {source['name']}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Filter to only articles mentioning the ticker or company\n",
        "    filtered_articles = []\n",
        "    for article in all_articles:\n",
        "        if (ticker.replace('.L','') in article['title'] or\n",
        "            company_name.lower() in article['title'].lower()):\n",
        "            filtered_articles.append(article)\n",
        "\n",
        "    return filtered_articles\n",
        "\n",
        "def analyze_news_sentiment(articles):\n",
        "    \"\"\"Analyze sentiment for news articles\"\"\"\n",
        "    if not articles:\n",
        "        return 0, []\n",
        "\n",
        "    sentiments = []\n",
        "    analyzed_articles = []\n",
        "\n",
        "    for article in articles:\n",
        "        try:\n",
        "            sentiment = sia.polarity_scores(article['title'])\n",
        "            compound = sentiment['compound']\n",
        "            sentiments.append(compound)\n",
        "            analyzed_articles.append({\n",
        "                **article,\n",
        "                'sentiment': compound,\n",
        "                'sentiment_label': 'Positive' if compound >= 0.05 else 'Negative' if compound <= -0.05 else 'Neutral'\n",
        "            })\n",
        "        except Exception as e:\n",
        "            print(f\"Error analyzing article: {e}\")\n",
        "            continue\n",
        "\n",
        "    avg_sentiment = sum(sentiments)/len(sentiments) if sentiments else 0\n",
        "    return avg_sentiment, analyzed_articles\n",
        "\n",
        "def create_interactive_visualizations(top_stocks, stock_type=\"regular\"):\n",
        "    \"\"\"Create Plotly visualizations for the top stocks\"\"\"\n",
        "    # Prepare data for visualizations\n",
        "    tickers = [stock['ticker'] for stock in top_stocks]\n",
        "    companies = [stock['company'] for stock in top_stocks]\n",
        "    prices = [stock['price'] for stock in top_stocks]\n",
        "    volumes = [stock['volume']/1000000 for stock in top_stocks]  # In millions\n",
        "\n",
        "    if stock_type == \"regular\":\n",
        "        op_margins = [stock['operating_margin']*100 if not pd.isna(stock['operating_margin']) else 0 for stock in top_stocks]\n",
        "        debt_equity = [stock['debt_to_equity'] if not pd.isna(stock['debt_to_equity']) else 0 for stock in top_stocks]\n",
        "        pb_ratios = [stock['price_to_book'] if not pd.isna(stock['price_to_book']) else 0 for stock in top_stocks]\n",
        "        pe_ratios = [stock['pe_ratio'] if not pd.isna(stock['pe_ratio']) else 0 for stock in top_stocks]\n",
        "        peg_ratios = [stock['peg_ratio'] if not pd.isna(stock['peg_ratio']) else 0 for stock in top_stocks]\n",
        "\n",
        "    comp_scores = [stock['composite_score'] for stock in top_stocks]\n",
        "\n",
        "    # Get news sentiment for each stock\n",
        "    news_sentiments = []\n",
        "    for stock in top_stocks:\n",
        "        articles = fetch_news_for_ticker(stock['ticker'], stock['company'], stock_type)\n",
        "        avg_sentiment, _ = analyze_news_sentiment(articles)\n",
        "        news_sentiments.append(avg_sentiment)\n",
        "\n",
        "    # Create subplots based on stock type\n",
        "    if stock_type == \"regular\":\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Fundamental Metrics Comparison\",\n",
        "                \"Composite Scores vs News Sentiment\",\n",
        "                \"Valuation Ratios\",\n",
        "                \"Profitability & Financial Health\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"bar\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.15,\n",
        "            horizontal_spacing=0.15\n",
        "        )\n",
        "\n",
        "        # Fundamental Metrics Comparison (Bar Chart)\n",
        "        metrics = ['Operating Margin', 'Debt-to-Equity', 'Price-to-Book', 'P/E Ratio', 'PEG Ratio']\n",
        "        for i, company in enumerate(companies[:2]):  # Compare top 2 companies\n",
        "            fig.add_trace(\n",
        "                go.Bar(\n",
        "                    x=metrics,\n",
        "                    y=[op_margins[i], debt_equity[i], pb_ratios[i], pe_ratios[i], peg_ratios[i]],\n",
        "                    name=company,\n",
        "                    marker_color='blue' if i == 0 else 'green'\n",
        "                ),\n",
        "                row=1, col=1\n",
        "            )\n",
        "\n",
        "        # Valuation Ratios (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=pb_ratios,\n",
        "                name='Price-to-Book',\n",
        "                marker_color='indianred'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=pe_ratios,\n",
        "                name='P/E Ratio',\n",
        "                marker_color='lightsalmon'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=peg_ratios,\n",
        "                name='PEG Ratio',\n",
        "                marker_color='crimson'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Profitability & Financial Health (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=op_margins,\n",
        "                name='Operating Margin (%)',\n",
        "                marker_color='darkgreen'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=debt_equity,\n",
        "                name='Debt-to-Equity',\n",
        "                marker_color='darkblue'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update axes for regular stocks\n",
        "        fig.update_yaxes(title_text=\"Value\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Ratio Value\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Percentage/Value\", row=2, col=2)\n",
        "\n",
        "    else:  # Penny stocks\n",
        "        fig = make_subplots(\n",
        "            rows=2, cols=2,\n",
        "            subplot_titles=(\n",
        "                \"Price & Volume\",\n",
        "                \"Composite Scores vs News Sentiment\",\n",
        "                \"Financial Health\",\n",
        "                \"Operating Performance\"\n",
        "            ),\n",
        "            specs=[\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"scatter\"}],\n",
        "                [{\"type\": \"bar\"}, {\"type\": \"bar\"}]\n",
        "            ],\n",
        "            vertical_spacing=0.15,\n",
        "            horizontal_spacing=0.15\n",
        "        )\n",
        "\n",
        "        # Price & Volume (Dual Axis Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=prices,\n",
        "                name='Price ($)',\n",
        "                marker_color='blue'\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=volumes,\n",
        "                name='Volume (M)',\n",
        "                marker_color='lightblue',\n",
        "                opacity=0.6\n",
        "            ),\n",
        "            row=1, col=1\n",
        "        )\n",
        "\n",
        "        # Financial Health (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=debt_equity,\n",
        "                name='Debt-to-Equity',\n",
        "                marker_color='darkred'\n",
        "            ),\n",
        "            row=2, col=1\n",
        "        )\n",
        "\n",
        "        # Operating Performance (Bar Chart)\n",
        "        fig.add_trace(\n",
        "            go.Bar(\n",
        "                x=companies,\n",
        "                y=op_margins,\n",
        "                name='Operating Margin (%)',\n",
        "                marker_color='darkgreen'\n",
        "            ),\n",
        "            row=2, col=2\n",
        "        )\n",
        "\n",
        "        # Update axes for penny stocks\n",
        "        fig.update_yaxes(title_text=\"Price / Volume\", row=1, col=1)\n",
        "        fig.update_yaxes(title_text=\"Debt-to-Equity\", row=2, col=1)\n",
        "        fig.update_yaxes(title_text=\"Operating Margin (%)\", row=2, col=2)\n",
        "\n",
        "    # Composite Scores vs News Sentiment (Scatter Plot) - common for both\n",
        "    fig.add_trace(\n",
        "        go.Scatter(\n",
        "            x=comp_scores,\n",
        "            y=news_sentiments,\n",
        "            text=companies,\n",
        "            mode='markers+text',\n",
        "            marker=dict(\n",
        "                size=12,\n",
        "                color=comp_scores,\n",
        "                colorscale='Viridis',\n",
        "                showscale=True,\n",
        "                colorbar=dict(title=\"Composite Score\")\n",
        "            ),\n",
        "            name='Score vs Sentiment',\n",
        "            textposition='top center'\n",
        "        ),\n",
        "        row=1, col=2\n",
        "    )\n",
        "\n",
        "    # Update layout\n",
        "    title_suffix = \"Penny Stocks\" if stock_type == \"penny\" else \"LSE Stocks\"\n",
        "    fig.update_layout(\n",
        "        title_text=f\"Top {title_suffix} Analysis Dashboard\",\n",
        "        height=900,\n",
        "        showlegend=True,\n",
        "        hovermode=\"closest\",\n",
        "        template=\"plotly_white\",\n",
        "        barmode='group'\n",
        "    )\n",
        "\n",
        "    # Update subplot titles\n",
        "    fig.update_annotations(font_size=12)\n",
        "\n",
        "    # Update common axes\n",
        "    fig.update_yaxes(title_text=\"News Sentiment\", row=1, col=2)\n",
        "    fig.update_xaxes(title_text=\"Composite Score\", row=1, col=2)\n",
        "\n",
        "    # Show the figure\n",
        "    fig.show()\n",
        "\n",
        "def screen_stocks(stock_type=\"regular\"):\n",
        "    \"\"\"Screen stocks based on fundamental metrics\"\"\"\n",
        "    screened_stocks = []\n",
        "\n",
        "    # Select appropriate stock database\n",
        "    stock_db = PENNY_STOCKS if stock_type == \"penny\" else LSE_TICKERS\n",
        "\n",
        "    for i, ticker in enumerate(stock_db.keys(), 1):\n",
        "        print(f\"Processing {i}/{len(stock_db)}: {ticker}\", end='\\r')\n",
        "\n",
        "        fundamentals = get_fundamentals(ticker, stock_type)\n",
        "        if not fundamentals:\n",
        "            continue\n",
        "\n",
        "        # Calculate scores for each metric\n",
        "        metric_scores = {\n",
        "            'operating_margin': calculate_metric_score(\n",
        "                fundamentals['operating_margin'], 'operating_margin', stock_type),\n",
        "            'debt_to_equity': calculate_metric_score(\n",
        "                fundamentals['debt_to_equity'], 'debt_to_equity', stock_type),\n",
        "            'price_to_book': calculate_metric_score(\n",
        "                fundamentals['price_to_book'], 'price_to_book', stock_type),\n",
        "            'pe_ratio': calculate_metric_score(\n",
        "                fundamentals['pe_ratio'], 'pe_ratio', stock_type),\n",
        "            'peg_ratio': calculate_metric_score(\n",
        "                fundamentals['peg_ratio'], 'peg_ratio', stock_type),\n",
        "            'insider_sentiment': max(0, fundamentals['insider_sentiment']),\n",
        "        }\n",
        "\n",
        "        # Add volume score for penny stocks\n",
        "        if stock_type == \"penny\":\n",
        "            metric_scores['volume'] = min(1, fundamentals['volume'] / 1000000)  # Normalize volume\n",
        "\n",
        "        # Calculate composite score with appropriate weights\n",
        "        if stock_type == \"penny\":\n",
        "            weights = {\n",
        "                'operating_margin': 0.15,\n",
        "                'debt_to_equity': 0.15,\n",
        "                'price_to_book': 0.15,\n",
        "                'pe_ratio': 0.1,\n",
        "                'peg_ratio': 0.1,\n",
        "                'insider_sentiment': 0.2,\n",
        "                'volume': 0.15\n",
        "            }\n",
        "        else:\n",
        "            weights = {\n",
        "                'operating_margin': 0.25,\n",
        "                'debt_to_equity': 0.2,\n",
        "                'price_to_book': 0.15,\n",
        "                'pe_ratio': 0.15,\n",
        "                'peg_ratio': 0.15,\n",
        "                'insider_sentiment': 0.1\n",
        "            }\n",
        "\n",
        "        composite_score = sum(\n",
        "            metric_scores[metric] * weights[metric]\n",
        "            for metric in metric_scores\n",
        "        )\n",
        "\n",
        "        # Count how many \"ideal\" criteria are met\n",
        "        thresholds = METRIC_THRESHOLDS[stock_type]\n",
        "        ideal_met = sum(\n",
        "            1 for metric in ['operating_margin', 'debt_to_equity',\n",
        "                           'price_to_book', 'pe_ratio', 'peg_ratio']\n",
        "            if (not pd.isna(fundamentals[metric])) and\n",
        "               ((metric == 'operating_margin' and fundamentals[metric] >= thresholds[metric]['ideal']) or\n",
        "                (metric != 'operating_margin' and fundamentals[metric] <= thresholds[metric]['ideal']))\n",
        "        )\n",
        "\n",
        "        screened_stocks.append({\n",
        "            **fundamentals,\n",
        "            **metric_scores,\n",
        "            'composite_score': composite_score,\n",
        "            'ideal_met': ideal_met,\n",
        "            'data_quality': sum(0 if pd.isna(fundamentals[m]) else 1\n",
        "                          for m in ['operating_margin', 'debt_to_equity',\n",
        "                                  'price_to_book', 'pe_ratio', 'peg_ratio'])\n",
        "        })\n",
        "\n",
        "    # Filter stocks with at least 3 metrics worth of data\n",
        "    screened_stocks = [s for s in screened_stocks if s['data_quality'] >= 3]\n",
        "\n",
        "    # Sort by composite score then by number of ideal criteria met\n",
        "    screened_stocks.sort(key=lambda x: (-x['composite_score'], -x['ideal_met']))\n",
        "\n",
        "    return screened_stocks\n",
        "\n",
        "def display_top_candidates_with_news(stocks, stock_type=\"regular\"):\n",
        "    \"\"\"Display the top stocks with news sentiment analysis and interactive visualizations\"\"\"\n",
        "    if len(stocks) == 0:\n",
        "        print(f\"No {stock_type} stocks found matching the criteria\")\n",
        "        print(\"Possible reasons:\")\n",
        "        print(\"- Market may be closed now\")\n",
        "        print(\"- Some data sources may be temporarily unavailable\")\n",
        "        print(\"- The screening criteria may be too strict\")\n",
        "        return\n",
        "\n",
        "    top_5 = stocks[:5]\n",
        "\n",
        "    # Create main summary table\n",
        "    summary_data = []\n",
        "    for stock in top_5:\n",
        "        articles = fetch_news_for_ticker(stock['ticker'], stock['company'], stock_type)\n",
        "        avg_sentiment, _ = analyze_news_sentiment(articles)\n",
        "\n",
        "        if stock_type == \"penny\":\n",
        "            summary_data.append([\n",
        "                stock['ticker'],\n",
        "                stock['company'][:15] + '...' if len(stock['company']) > 15 else stock['company'],\n",
        "                f\"${stock['price']:.2f}\",\n",
        "                f\"{stock['volume']/1000:.1f}K\",\n",
        "                f\"{stock['composite_score']:.2f}\",\n",
        "                f\"{avg_sentiment:.2f}\",\n",
        "                f\"{stock['debt_to_equity']:.2f}\" if not pd.isna(stock['debt_to_equity']) else 'N/A',\n",
        "                len(articles)\n",
        "            ])\n",
        "        else:\n",
        "            summary_data.append([\n",
        "                stock['ticker'],\n",
        "                stock['company'][:15] + '...' if len(stock['company']) > 15 else stock['company'],\n",
        "                f\"{stock['composite_score']:.2f}\",\n",
        "                f\"{avg_sentiment:.2f}\",\n",
        "                f\"{stock['operating_margin']*100:.1f}%\" if not pd.isna(stock['operating_margin']) else 'N/A',\n",
        "                f\"{stock['debt_to_equity']:.2f}\" if not pd.isna(stock['debt_to_equity']) else 'N/A',\n",
        "                len(articles)\n",
        "            ])\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        headers = [\n",
        "            \"Ticker\", \"Company\", \"Price\", \"Volume\", \"Score\",\n",
        "            \"News Sent\", \"D/E\", \"News Count\"\n",
        "        ]\n",
        "    else:\n",
        "        headers = [\n",
        "            \"Ticker\", \"Company\", \"Fundamental\", \"News\", \"OpM\",\n",
        "            \"D/E\", \"News Count\"\n",
        "        ]\n",
        "\n",
        "    print(tabulate(summary_data, headers=headers, tablefmt='grid'))\n",
        "\n",
        "    # Detailed analysis of top candidate\n",
        "    best = top_5[0]\n",
        "    articles = fetch_news_for_ticker(best['ticker'], best['company'], stock_type)\n",
        "    avg_sentiment, analyzed_articles = analyze_news_sentiment(articles)\n",
        "\n",
        "    stock_type_label = \"Penny Stock\" if stock_type == \"penny\" else \"Stock\"\n",
        "    print(f\"\\nTop {stock_type_label}: {best['company']} ({best['ticker']})\")\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        print(f\"Current Price: ${best['price']:.2f}\")\n",
        "        print(f\"Average Volume: {best['volume']/1000:.1f}K\")\n",
        "\n",
        "    print(f\"Composite Score: {best['composite_score']:.2f}\")\n",
        "    print(f\"Average News Sentiment: {avg_sentiment:.2f}\")\n",
        "\n",
        "    print(\"\\nRecent News Headlines:\")\n",
        "    for article in analyzed_articles[:5]:\n",
        "        print(f\"- [{article['sentiment_label']}] {article['title']} ({article['source']})\")\n",
        "\n",
        "    print(\"\\nKey Metrics:\")\n",
        "    print(f\"- Operating Margin: {best['operating_margin']*100:.1f}%\" if not pd.isna(best['operating_margin']) else \"- Operating Margin: N/A\")\n",
        "    print(f\"- Debt-to-Equity: {best['debt_to_equity']:.2f}\" if not pd.isna(best['debt_to_equity']) else \"- Debt-to-Equity: N/A\")\n",
        "    print(f\"- Price-to-Book: {best['price_to_book']:.2f}\" if not pd.isna(best['price_to_book']) else \"- Price-to-Book: N/A\")\n",
        "\n",
        "    if stock_type == \"penny\":\n",
        "        print(f\"- Beta (Volatility): {best['beta']:.2f}\" if not pd.isna(best['beta']) else \"- Beta: N/A\")\n",
        "\n",
        "    print(f\"- P/E Ratio: {best['pe_ratio']:.1f}\" if not pd.isna(best['pe_ratio']) else \"- P/E Ratio: N/A\")\n",
        "    print(f\"- PEG Ratio: {best['peg_ratio']:.1f}\" if not pd.isna(best['peg_ratio']) else \"- PEG Ratio: N/A\")\n",
        "    print(f\"- Insider Sentiment: {'Positive' if best['insider_sentiment'] > 0 else 'Neutral'}\")\n",
        "\n",
        "    # Create interactive visualizations\n",
        "    create_interactive_visualizations(top_5, stock_type)\n",
        "\n",
        "    # Ask user for technical indicators and plot price chart\n",
        "    print(f\"\\nTechnical Analysis for Top {stock_type_label}:\")\n",
        "    indicator_options = \"SMA, EMA, Bollinger, VWAP\" + (\", RSI\" if stock_type == \"penny\" else \"\")\n",
        "    indicators = input(f\"Select Indicators (comma-separated: {indicator_options}): \").split(\",\")\n",
        "    plot_price_chart(best['ticker'], best['company'], indicators, stock_type)\n",
        "\n",
        "def run_sentiment_analysis():\n",
        "    \"\"\"Run the comprehensive market sentiment analysis\"\"\"\n",
        "    print(\"🚀 Comprehensive Market Sentiment Analyzer\")\n",
        "    print(\"📊 Analyzing LSE and LSE Penny Stocks\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    analyzer = ComprehensiveMarketSentimentAnalyzer()\n",
        "\n",
        "    # Analyze all stocks and get top 15 recommendations\n",
        "    top_stocks, all_results = analyzer.analyze_top_stocks(top_n=15)\n",
        "\n",
        "    # Display recommendations\n",
        "    analyzer.display_recommendations(top_stocks, all_results)\n",
        "\n",
        "    # Generate detailed report\n",
        "    analyzer.generate_detailed_report(top_stocks)\n",
        "\n",
        "    # Create visualizations\n",
        "    analyzer.create_visualizations(all_results)\n",
        "\n",
        "    # Save to CSV\n",
        "    analyzer.save_recommendations(top_stocks)\n",
        "\n",
        "    print(f\"\\n✅ Sentiment analysis complete! {len(top_stocks)} top recommendations generated.\")\n",
        "    print(\"💡 Remember: This is sentiment analysis - always do your own research before investing!\")\n",
        "\n",
        "def main():\n",
        "    print(\"Comprehensive Stock Screener with Technical & Sentiment Analysis\\n\")\n",
        "\n",
        "    # Let user choose analysis type\n",
        "    print(\"Select analysis type:\")\n",
        "    print(\"1. Fundamental Stock Screening\")\n",
        "    print(\"2. Comprehensive Market Sentiment Analysis\")\n",
        "    print(\"3. LSE Fundamental Data Collection\")\n",
        "    print(\"4. All Analyses (Full Suite)\")\n",
        "\n",
        "    choice = input(\"Enter your choice (1, 2, 3, or 4): \").strip()\n",
        "\n",
        "    if choice == \"1\":\n",
        "        # Let user choose stock type\n",
        "        print(\"\\nSelect stock type to screen:\")\n",
        "        print(\"1. Regular LSE Stocks\")\n",
        "        print(\"2. Penny Stocks\")\n",
        "\n",
        "        stock_choice = input(\"Enter your choice (1 or 2): \").strip()\n",
        "\n",
        "        if stock_choice == \"1\":\n",
        "            stock_type = \"regular\"\n",
        "            print(\"\\nScreening regular LSE stocks based on:\")\n",
        "            print(\"- Fundamental metrics (Profitability, Valuation, etc.)\")\n",
        "            print(\"- News sentiment analysis\")\n",
        "            print(\"- Technical indicators\")\n",
        "        elif stock_choice == \"2\":\n",
        "            stock_type = \"penny\"\n",
        "            print(\"\\nScreening penny stocks based on:\")\n",
        "            print(\"- Price (<$5) and Volume (>500K)\")\n",
        "            print(\"- Fundamental metrics (adjusted for penny stocks)\")\n",
        "            print(\"- News sentiment from penny stock sources\")\n",
        "            print(\"- Technical indicators\")\n",
        "        else:\n",
        "            print(\"Invalid choice. Defaulting to regular LSE stocks.\")\n",
        "            stock_type = \"regular\"\n",
        "\n",
        "        try:\n",
        "            # Screen stocks based on fundamentals\n",
        "            screened_stocks = screen_stocks(stock_type)\n",
        "\n",
        "            # Display results with news sentiment and technical analysis\n",
        "            display_top_candidates_with_news(screened_stocks, stock_type)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during screening: {e}\")\n",
        "            print(\"This might be due to temporary Yahoo Finance API issues or data limitations.\")\n",
        "\n",
        "        print(\"\\nAnalysis Complete\")\n",
        "\n",
        "        if stock_type == \"penny\":\n",
        "            print(\"Note: Penny stocks are highly volatile - always conduct thorough research before investing\")\n",
        "        else:\n",
        "            print(\"Note: Combined fundamental, sentiment, and technical analysis provides comprehensive insights\")\n",
        "\n",
        "    elif choice == \"2\":\n",
        "        run_sentiment_analysis()\n",
        "\n",
        "    elif choice == \"3\":\n",
        "        # Run LSE fundamental data collection\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LSE FUNDAMENTAL DATA COLLECTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        run_lse_fundamental_collection()\n",
        "\n",
        "    elif choice == \"4\":\n",
        "        # Run all analyses\n",
        "        print(\"\\nRunning comprehensive analysis suite...\")\n",
        "\n",
        "        # First run LSE fundamental data collection\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"LSE FUNDAMENTAL DATA COLLECTION\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        lse_data, value_candidates = run_lse_fundamental_collection()\n",
        "\n",
        "        # Then run fundamental screening\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"FUNDAMENTAL STOCK SCREENING\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        for stock_type in [\"regular\", \"penny\"]:\n",
        "            print(f\"\\nScreening {stock_type} stocks...\")\n",
        "            try:\n",
        "                screened_stocks = screen_stocks(stock_type)\n",
        "                if screened_stocks:\n",
        "                    display_top_candidates_with_news(screened_stocks[:3], stock_type)  # Show top 3 only\n",
        "                else:\n",
        "                    print(f\"No {stock_type} stocks found matching criteria.\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error screening {stock_type} stocks: {e}\")\n",
        "\n",
        "        # Finally run sentiment analysis\n",
        "        print(\"\\n\" + \"=\"*80)\n",
        "        print(\"COMPREHENSIVE MARKET SENTIMENT ANALYSIS\")\n",
        "        print(\"=\"*80)\n",
        "\n",
        "        run_sentiment_analysis()\n",
        "\n",
        "    else:\n",
        "        print(\"Invalid choice. Defaulting to fundamental stock screening.\")\n",
        "        # Run fundamental screening with regular stocks\n",
        "        stock_type = \"regular\"\n",
        "        try:\n",
        "            screened_stocks = screen_stocks(stock_type)\n",
        "            display_top_candidates_with_news(screened_stocks, stock_type)\n",
        "        except Exception as e:\n",
        "            print(f\"\\nAn error occurred during screening: {e}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Install required packages if not already installed\n",
        "    required_packages = {\n",
        "        'yfinance': 'yfinance',\n",
        "        'feedparser': 'feedparser',\n",
        "        'nltk': 'nltk',\n",
        "        'fake_useragent': 'fake-useragent',\n",
        "        'plotly': 'plotly',\n",
        "        'textblob': 'textblob',\n",
        "        'seaborn': 'seaborn',\n",
        "        'tqdm': 'tqdm'\n",
        "    }\n",
        "\n",
        "    for package, install_name in required_packages.items():\n",
        "        try:\n",
        "            __import__(package)\n",
        "        except ImportError:\n",
        "            print(f\"Installing {package}...\")\n",
        "            import subprocess\n",
        "            subprocess.run(['pip', 'install', install_name], check=True)\n",
        "\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 383
        },
        "outputId": "58b3a513-39c1-40e8-9240-2f098ca62a76",
        "id": "6FvD-3kVzpuT"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'fake_useragent'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2082254888.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mbs4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBeautifulSoup\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mfake_useragent\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mUserAgent\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnltk\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msentiment\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSentimentIntensityAnalyzer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'fake_useragent'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ],
      "id": "6FvD-3kVzpuT"
    }
  ],
  "metadata": {
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}